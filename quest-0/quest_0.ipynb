{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "0.14.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "print(torch.__version__)\n",
    "print(torchtext.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set device as GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    dev = 'mps'\n",
    "else:\n",
    "    dev = 'cpu'\n",
    "device = torch.device(dev)    \n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch warmup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use torch.randn to create two tensors of size (29, 30, 32) and (32, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of Size (29, 30, 32): \n",
      " tensor([[[ 0.3367, -0.0778,  0.0546,  ...,  0.8098, -0.2722,  0.0287],\n",
      "         [-0.2116, -0.6549,  1.6391,  ..., -0.4656, -0.7392, -0.2576],\n",
      "         [ 0.5508,  0.4069,  1.3977,  ...,  0.3417, -0.3126, -1.4643],\n",
      "         ...,\n",
      "         [ 0.0874,  0.4584,  0.2170,  ..., -1.2799,  0.0577,  0.3215],\n",
      "         [-0.6971,  1.4693,  0.2514,  ..., -0.1359, -0.6667, -0.1972],\n",
      "         [ 1.1785, -0.2125,  0.2029,  ...,  0.2271,  1.0148, -0.5254]],\n",
      "\n",
      "        [[-1.1548, -0.0106,  0.3458,  ...,  1.6871,  0.0092, -0.1228],\n",
      "         [ 0.6263,  0.4055, -0.6232,  ..., -0.1954, -0.6879, -1.0670],\n",
      "         [ 1.0371,  0.0730, -0.2725,  ...,  0.5270, -0.3998,  1.4140],\n",
      "         ...,\n",
      "         [ 0.7513,  1.6772,  0.2534,  ..., -0.1046, -0.6020,  1.3206],\n",
      "         [-1.1698,  0.9286, -1.1145,  ...,  0.6316,  0.8283,  1.4860],\n",
      "         [-1.1292, -1.1039, -0.5689,  ..., -0.9585, -0.6074,  0.7523]],\n",
      "\n",
      "        [[-0.7420, -1.1133,  0.5136,  ..., -1.4099, -1.0709,  0.5335],\n",
      "         [-0.1390,  1.9167, -1.5279,  ..., -0.3420,  0.2342, -1.3319],\n",
      "         [ 2.4054, -0.4871, -1.4744,  ...,  1.1217, -1.2886,  1.9451],\n",
      "         ...,\n",
      "         [ 0.5756,  0.1151,  0.3564,  ...,  2.4789,  0.8274, -0.1626],\n",
      "         [ 2.0302,  1.4883,  0.0219,  ..., -1.7676, -0.6667, -0.4725],\n",
      "         [ 0.3070,  0.8545, -0.3977,  ..., -0.9944,  0.2126, -1.6179]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.8715, -0.1713, -0.7429,  ...,  0.0374,  0.7981,  0.7306],\n",
      "         [ 0.1014,  0.9076, -0.9018,  ..., -1.4812,  0.1580, -0.0863],\n",
      "         [ 0.4568,  1.7752,  1.3981,  ...,  0.4364,  0.4201,  0.0163],\n",
      "         ...,\n",
      "         [-1.0283,  0.1710, -0.2310,  ...,  1.3980, -0.2782,  0.6715],\n",
      "         [-0.9316,  0.5630, -0.7227,  ..., -0.1650,  0.1392,  0.3698],\n",
      "         [ 0.3132,  0.2629, -0.5914,  ...,  0.2540, -0.3627,  0.4986]],\n",
      "\n",
      "        [[ 1.6288,  1.2345,  0.6359,  ..., -1.3273, -0.2075,  0.9425],\n",
      "         [ 0.7015,  1.4149, -0.2438,  ..., -1.1645, -1.0642,  1.5170],\n",
      "         [-0.6500,  1.1289,  1.5883,  ..., -0.8596, -1.0551,  0.8315],\n",
      "         ...,\n",
      "         [ 0.3895,  1.2867,  0.5481,  ...,  0.3723,  0.4901, -0.5173],\n",
      "         [-1.3404, -0.6740,  0.3437,  ...,  2.2310,  1.3877, -0.5979],\n",
      "         [ 1.8522,  0.6471,  1.2493,  ...,  0.8040,  0.1740, -0.6520]],\n",
      "\n",
      "        [[-1.5747,  0.2687, -0.2336,  ..., -0.6227,  0.4539, -0.2525],\n",
      "         [-0.1298, -0.2109,  0.2169,  ..., -0.8877, -0.2957,  0.9865],\n",
      "         [ 0.1370,  1.4688,  0.6492,  ..., -0.5996, -0.3028,  1.3314],\n",
      "         ...,\n",
      "         [ 0.4515,  0.3579, -1.2672,  ..., -0.0345, -0.2696,  0.0737],\n",
      "         [ 0.3081, -0.2207,  0.7414,  ...,  0.4882,  0.8300,  0.1306],\n",
      "         [-1.7145,  0.6796,  0.7579,  ..., -0.0912, -1.0347, -2.2738]]])\n",
      "Tensor of Size (32, 100): \n",
      " tensor([[-0.6425, -0.6737,  0.4286,  ...,  0.5754, -0.8126,  0.5078],\n",
      "        [ 0.1319,  1.1485,  0.3919,  ...,  1.3685,  0.1326, -1.4896],\n",
      "        [ 0.4790, -0.6332, -1.5139,  ..., -0.9994, -1.3019, -1.2019],\n",
      "        ...,\n",
      "        [-2.4454,  0.1052, -0.5891,  ...,  0.9626, -1.0811,  1.1320],\n",
      "        [ 1.6853,  1.2465, -0.2137,  ...,  1.6271,  0.9113, -0.9855],\n",
      "        [ 1.6930, -0.3652, -0.4201,  ...,  0.9507,  0.4820, -0.9715]])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.randn(29, 30, 32)\n",
    "print('Tensor of Size (29, 30, 32): \\n' , tensor_a)\n",
    "\n",
    "tensor_b = torch.randn(32, 100)\n",
    "print('Tensor of Size (32, 100): \\n' , tensor_b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use  torch.matmul  to matrix multiply the two tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product of tensor_a and tensor_b: \n",
      " tensor([[[-5.9818e+00, -1.7661e+00, -1.6034e+00,  ..., -3.0387e+00,\n",
      "          -3.7300e-01, -1.1283e+00],\n",
      "         [ 4.9403e+00, -2.2627e-01,  1.4862e+00,  ..., -1.2954e+01,\n",
      "           1.8034e+00,  2.4297e+00],\n",
      "         [-1.8042e+00, -1.0814e+01,  1.9619e+00,  ...,  2.5432e-01,\n",
      "          -2.8195e-01, -1.5294e+00],\n",
      "         ...,\n",
      "         [-1.9089e-01,  8.1660e+00,  3.3491e+00,  ...,  9.2679e+00,\n",
      "           5.1710e+00, -5.0976e+00],\n",
      "         [-3.1682e-01, -9.1286e-01, -4.4653e+00,  ...,  3.5076e+00,\n",
      "          -5.4038e+00,  8.5440e-01],\n",
      "         [-7.3638e+00,  2.7562e+00,  1.4914e+00,  ..., -2.3446e+00,\n",
      "          -5.7287e+00,  7.7993e-01]],\n",
      "\n",
      "        [[-4.8814e+00, -4.5118e+00,  5.8500e+00,  ..., -1.7505e+00,\n",
      "          -9.7674e+00,  6.5590e+00],\n",
      "         [-1.1605e+01, -8.1115e-01, -2.7348e+00,  ...,  3.6612e+00,\n",
      "          -1.6930e+00,  2.9778e+00],\n",
      "         [ 1.3404e+00,  5.8532e+00, -1.3133e+01,  ...,  2.7410e+00,\n",
      "          -3.1024e+00, -2.4988e+00],\n",
      "         ...,\n",
      "         [ 5.7065e+00, -3.4710e+00,  2.2102e+00,  ...,  1.6726e+00,\n",
      "          -1.9818e-01, -2.3744e+00],\n",
      "         [ 9.0111e+00,  7.6259e-01, -7.9463e-01,  ...,  3.0037e+00,\n",
      "           5.7595e+00,  6.1629e-01],\n",
      "         [ 6.3312e+00,  8.9499e+00, -4.7430e-01,  ..., -5.9395e+00,\n",
      "           4.1113e+00, -9.1330e-01]],\n",
      "\n",
      "        [[ 9.3840e+00,  4.8832e-01, -3.3650e+00,  ..., -4.5400e+00,\n",
      "          -7.1142e-01, -7.8463e-01],\n",
      "         [-5.2652e+00,  6.5980e+00,  5.4461e+00,  ...,  5.3439e+00,\n",
      "           7.1507e+00, -3.1384e+00],\n",
      "         [-7.6232e+00, -9.1756e-01, -2.4831e+00,  ..., -6.1729e+00,\n",
      "          -4.1371e-02,  1.0506e+01],\n",
      "         ...,\n",
      "         [-9.3179e+00, -4.4575e+00,  5.9407e+00,  ...,  9.2306e+00,\n",
      "          -4.2039e+00,  1.1471e+01],\n",
      "         [ 9.2039e+00,  4.2572e+00,  1.2229e+01,  ...,  4.4297e+00,\n",
      "           4.0283e+00,  4.2574e+00],\n",
      "         [ 7.9123e-03, -4.2531e+00,  8.1606e+00,  ...,  2.4967e+00,\n",
      "           6.3144e+00, -1.6940e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.1648e+00, -8.4531e+00,  1.0180e+01,  ..., -4.0630e+00,\n",
      "          -8.5387e+00,  1.6104e+00],\n",
      "         [-1.3111e+00,  7.3909e-01, -7.3345e+00,  ...,  3.0954e+00,\n",
      "          -5.0717e+00,  5.9212e+00],\n",
      "         [ 7.2651e-01,  2.4438e+00,  3.4895e-01,  ..., -3.7833e+00,\n",
      "          -5.1596e+00, -1.2746e+00],\n",
      "         ...,\n",
      "         [-9.1673e+00,  8.4125e+00, -7.1363e+00,  ..., -6.3062e-01,\n",
      "           1.4020e+01, -3.9658e+00],\n",
      "         [ 1.5060e+00,  2.6391e+00, -4.1815e-01,  ..., -1.8989e+00,\n",
      "           6.6191e-01,  1.8884e+00],\n",
      "         [ 7.4493e+00,  6.3730e-01,  1.1929e+00,  ...,  3.1907e-01,\n",
      "           8.1735e+00, -2.4934e+00]],\n",
      "\n",
      "        [[-4.0229e+00,  3.5699e-01,  2.9880e+00,  ..., -2.6103e-01,\n",
      "          -7.5534e+00, -9.5645e+00],\n",
      "         [ 1.0906e+01, -9.5006e+00, -2.8851e+00,  ..., -2.4495e+00,\n",
      "          -1.0234e+00, -8.1322e+00],\n",
      "         [-2.4637e+00,  3.0555e+00,  1.6118e+01,  ...,  5.4542e+00,\n",
      "           8.2009e+00,  3.8112e+00],\n",
      "         ...,\n",
      "         [-2.9776e+00,  1.4443e+00, -1.3679e+00,  ...,  1.1193e+00,\n",
      "           3.8217e+00, -3.7400e-01],\n",
      "         [-4.2947e+00,  6.0622e+00, -2.0172e+00,  ..., -8.4275e+00,\n",
      "          -9.2995e+00,  3.4134e+00],\n",
      "         [ 1.6167e+00, -7.0122e+00, -2.8927e-01,  ...,  2.8121e+00,\n",
      "          -1.1998e+01,  5.4115e+00]],\n",
      "\n",
      "        [[ 7.5365e+00, -6.5178e+00,  4.7000e+00,  ..., -6.3404e+00,\n",
      "           6.9859e+00, -2.8562e+00],\n",
      "         [ 6.1621e+00, -1.6796e+00,  5.2082e+00,  ...,  7.8601e+00,\n",
      "          -8.0286e-01,  4.0619e+00],\n",
      "         [ 7.9669e+00,  6.9972e+00,  2.4254e+00,  ...,  4.4881e+00,\n",
      "           4.3400e+00, -8.3085e+00],\n",
      "         ...,\n",
      "         [-8.8918e+00, -2.1446e+00,  6.3355e+00,  ...,  1.4202e+01,\n",
      "          -2.1995e+00,  8.8337e+00],\n",
      "         [ 3.6025e+00,  8.5873e+00, -6.7997e+00,  ...,  8.5181e+00,\n",
      "           8.4043e+00, -8.4843e+00],\n",
      "         [-7.8089e+00, -5.9742e+00,  7.8201e+00,  ...,  9.4723e-01,\n",
      "           1.2701e+01, -7.8987e-01]]])\n",
      "Shape of Tensor:  torch.Size([29, 30, 100])\n"
     ]
    }
   ],
   "source": [
    "product = torch.matmul(tensor_a, \n",
    "                       tensor_b)\n",
    "print('Product of tensor_a and tensor_b: \\n' , product)\n",
    "\n",
    "print('Shape of Tensor: ' , product.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What is the difference between torch.matmul , torch.mm , torch.bmm , and torch.einsum , and the @ operator?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. torch.matmul() ->\n",
    "2. torch.mm() ->\n",
    "3. torch.bmm() ->\n",
    "4. torch.einsum() ->\n",
    "5. @ operator -> It is a shorthand for the torch.matmul() function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use torch.sum on the resulting tensor, passing the optional argument of dim=1 to sum across the 1st dimension. Before you run this, can you predict the size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Tensor across 1st Dimension: \n",
      " tensor([[ -0.3894, -43.6030,  -4.2514,  ...,   0.7059,   3.6391,  20.6740],\n",
      "        [-73.2093,  69.4370, -48.8484,  ...,  55.7981, -16.5465,  42.0442],\n",
      "        [ 29.6912,  45.8306,   9.9017,  ...,  28.8507,  19.8293, -26.6173],\n",
      "        ...,\n",
      "        [-23.6733, -24.7574, -25.7522,  ..., -11.3934, -32.3515,  54.4481],\n",
      "        [-33.7505,  31.8131,  45.4374,  ...,  18.3431,   1.0394,  32.7575],\n",
      "        [ -0.6389, -63.0511,  38.4923,  ...,  -2.8087,  13.5516,  45.7638]])\n",
      "Shape of Tensor:  torch.Size([29, 100])\n"
     ]
    }
   ],
   "source": [
    "tensor_sum = torch.sum(product, \n",
    "                       dim = 1)\n",
    "print('Sum of Tensor across 1st Dimension: \\n' , tensor_sum)\n",
    "\n",
    "print('Shape of Tensor: ' , tensor_sum.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create a new long tensor of size  (3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Tensor: \n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Updated Long Tensor: \n",
      " tensor([[2, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 4, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 6, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "long_tensor = torch.ones((3, 10), \n",
    "                         dtype = torch.long)\n",
    "print('Long Tensor: \\n' , long_tensor)\n",
    "\n",
    "long_tensor[0, 0] = 2\n",
    "long_tensor[1, 2] = 4\n",
    "long_tensor[2, 4] = 6\n",
    "\n",
    "print('Updated Long Tensor: \\n' , long_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Use this new long tensor to index into the tensor from step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed Tensor: \n",
      " tensor([[[[ 9.3840e+00,  4.8832e-01, -3.3650e+00,  ..., -4.5400e+00,\n",
      "           -7.1142e-01, -7.8463e-01],\n",
      "          [-5.2652e+00,  6.5980e+00,  5.4461e+00,  ...,  5.3439e+00,\n",
      "            7.1507e+00, -3.1384e+00],\n",
      "          [-7.6232e+00, -9.1756e-01, -2.4831e+00,  ..., -6.1729e+00,\n",
      "           -4.1371e-02,  1.0506e+01],\n",
      "          ...,\n",
      "          [-9.3179e+00, -4.4575e+00,  5.9407e+00,  ...,  9.2306e+00,\n",
      "           -4.2039e+00,  1.1471e+01],\n",
      "          [ 9.2039e+00,  4.2572e+00,  1.2229e+01,  ...,  4.4297e+00,\n",
      "            4.0283e+00,  4.2574e+00],\n",
      "          [ 7.9123e-03, -4.2531e+00,  8.1606e+00,  ...,  2.4967e+00,\n",
      "            6.3144e+00, -1.6940e+00]],\n",
      "\n",
      "         [[-4.8814e+00, -4.5118e+00,  5.8500e+00,  ..., -1.7505e+00,\n",
      "           -9.7674e+00,  6.5590e+00],\n",
      "          [-1.1605e+01, -8.1115e-01, -2.7348e+00,  ...,  3.6612e+00,\n",
      "           -1.6930e+00,  2.9778e+00],\n",
      "          [ 1.3404e+00,  5.8532e+00, -1.3133e+01,  ...,  2.7410e+00,\n",
      "           -3.1024e+00, -2.4988e+00],\n",
      "          ...,\n",
      "          [ 5.7065e+00, -3.4710e+00,  2.2102e+00,  ...,  1.6726e+00,\n",
      "           -1.9818e-01, -2.3744e+00],\n",
      "          [ 9.0111e+00,  7.6259e-01, -7.9463e-01,  ...,  3.0037e+00,\n",
      "            5.7595e+00,  6.1629e-01],\n",
      "          [ 6.3312e+00,  8.9499e+00, -4.7430e-01,  ..., -5.9395e+00,\n",
      "            4.1113e+00, -9.1330e-01]],\n",
      "\n",
      "         [[-4.8814e+00, -4.5118e+00,  5.8500e+00,  ..., -1.7505e+00,\n",
      "           -9.7674e+00,  6.5590e+00],\n",
      "          [-1.1605e+01, -8.1115e-01, -2.7348e+00,  ...,  3.6612e+00,\n",
      "           -1.6930e+00,  2.9778e+00],\n",
      "          [ 1.3404e+00,  5.8532e+00, -1.3133e+01,  ...,  2.7410e+00,\n",
      "           -3.1024e+00, -2.4988e+00],\n",
      "          ...,\n",
      "          [ 5.7065e+00, -3.4710e+00,  2.2102e+00,  ...,  1.6726e+00,\n",
      "           -1.9818e-01, -2.3744e+00],\n",
      "          [ 9.0111e+00,  7.6259e-01, -7.9463e-01,  ...,  3.0037e+00,\n",
      "            5.7595e+00,  6.1629e-01],\n",
      "          [ 6.3312e+00,  8.9499e+00, -4.7430e-01,  ..., -5.9395e+00,\n",
      "            4.1113e+00, -9.1330e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.8814e+00, -4.5118e+00,  5.8500e+00,  ..., -1.7505e+00,\n",
      "           -9.7674e+00,  6.5590e+00],\n",
      "          [-1.1605e+01, -8.1115e-01, -2.7348e+00,  ...,  3.6612e+00,\n",
      "           -1.6930e+00,  2.9778e+00],\n",
      "          [ 1.3404e+00,  5.8532e+00, -1.3133e+01,  ...,  2.7410e+00,\n",
      "           -3.1024e+00, -2.4988e+00],\n",
      "          ...,\n",
      "          [ 5.7065e+00, -3.4710e+00,  2.2102e+00,  ...,  1.6726e+00,\n",
      "           -1.9818e-01, -2.3744e+00],\n",
      "          [ 9.0111e+00,  7.6259e-01, -7.9463e-01,  ...,  3.0037e+00,\n",
      "            5.7595e+00,  6.1629e-01],\n",
      "          [ 6.3312e+00,  8.9499e+00, -4.7430e-01,  ..., -5.9395e+00,\n",
      "            4.1113e+00, -9.1330e-01]],\n",
      "\n",
      "         [[-4.8814e+00, -4.5118e+00,  5.8500e+00,  ..., -1.7505e+00,\n",
      "           -9.7674e+00,  6.5590e+00],\n",
      "          [-1.1605e+01, -8.1115e-01, -2.7348e+00,  ...,  3.6612e+00,\n",
      "           -1.6930e+00,  2.9778e+00],\n",
      "          [ 1.3404e+00,  5.8532e+00, -1.3133e+01,  ...,  2.7410e+00,\n",
      "           -3.1024e+00, -2.4988e+00],\n",
      "          ...,\n",
      "          [ 5.7065e+00, -3.4710e+00,  2.2102e+00,  ...,  1.6726e+00,\n",
      "           -1.9818e-01, -2.3744e+00],\n",
      "          [ 9.0111e+00,  7.6259e-01, -7.9463e-01,  ...,  3.0037e+00,\n",
      "            5.7595e+00,  6.1629e-01],\n",
      "          [ 6.3312e+00,  8.9499e+00, -4.7430e-01,  ..., -5.9395e+00,\n",
      "            4.1113e+00, -9.1330e-01]],\n",
      "\n",
      "         [[-4.8814e+00, -4.5118e+00,  5.8500e+00,  ..., -1.7505e+00,\n",
      "           -9.7674e+00,  6.5590e+00],\n",
      "          [-1.1605e+01, -8.1115e-01, -2.7348e+00,  ...,  3.6612e+00,\n",
      "           -1.6930e+00,  2.9778e+00],\n",
      "          [ 1.3404e+00,  5.8532e+00, -1.3133e+01,  ...,  2.7410e+00,\n",
      "           -3.1024e+00, -2.4988e+00],\n",
      "          ...,\n",
      "          [ 5.7065e+00, -3.4710e+00,  2.2102e+00,  ...,  1.6726e+00,\n",
      "           -1.9818e-01, -2.3744e+00],\n",
      "          [ 9.0111e+00,  7.6259e-01, -7.9463e-01,  ...,  3.0037e+00,\n",
      "            5.7595e+00,  6.1629e-01],\n",
      "          [ 6.3312e+00,  8.9499e+00, -4.7430e-01,  ..., -5.9395e+00,\n",
      "            4.1113e+00, -9.1330e-01]]],\n",
      "\n",
      "\n",
      "        [[[-4.8814e+00, -4.5118e+00,  5.8500e+00,  ..., -1.7505e+00,\n",
      "           -9.7674e+00,  6.5590e+00],\n",
      "          [-1.1605e+01, -8.1115e-01, -2.7348e+00,  ...,  3.6612e+00,\n",
      "           -1.6930e+00,  2.9778e+00],\n",
      "          [ 1.3404e+00,  5.8532e+00, -1.3133e+01,  ...,  2.7410e+00,\n",
      "           -3.1024e+00, -2.4988e+00],\n",
      "          ...,\n",
      "          [ 5.7065e+00, -3.4710e+00,  2.2102e+00,  ...,  1.6726e+00,\n",
      "           -1.9818e-01, -2.3744e+00],\n",
      "          [ 9.0111e+00,  7.6259e-01, -7.9463e-01,  ...,  3.0037e+00,\n",
      "            5.7595e+00,  6.1629e-01],\n",
      "          [ 6.3312e+00,  8.9499e+00, -4.7430e-01,  ..., -5.9395e+00,\n",
      "            4.1113e+00, -9.1330e-01]],\n",
      "\n",
      "         [[-4.8814e+00, -4.5118e+00,  5.8500e+00,  ..., -1.7505e+00,\n",
      "           -9.7674e+00,  6.5590e+00],\n",
      "          [-1.1605e+01, -8.1115e-01, -2.7348e+00,  ...,  3.6612e+00,\n",
      "           -1.6930e+00,  2.9778e+00],\n",
      "          [ 1.3404e+00,  5.8532e+00, -1.3133e+01,  ...,  2.7410e+00,\n",
      "           -3.1024e+00, -2.4988e+00],\n",
      "          ...,\n",
      "          [ 5.7065e+00, -3.4710e+00,  2.2102e+00,  ...,  1.6726e+00,\n",
      "           -1.9818e-01, -2.3744e+00],\n",
      "          [ 9.0111e+00,  7.6259e-01, -7.9463e-01,  ...,  3.0037e+00,\n",
      "            5.7595e+00,  6.1629e-01],\n",
      "          [ 6.3312e+00,  8.9499e+00, -4.7430e-01,  ..., -5.9395e+00,\n",
      "            4.1113e+00, -9.1330e-01]],\n",
      "\n",
      "         [[-8.8517e-01,  1.9193e+00,  1.7080e-01,  ...,  2.8658e+00,\n",
      "            6.5925e+00, -3.9878e+00],\n",
      "          [ 6.1497e-01,  6.1808e+00,  3.6890e+00,  ..., -9.3369e-01,\n",
      "            5.1559e-02, -4.7815e+00],\n",
      "          [-7.4013e+00,  1.3148e+00, -1.7819e+00,  ..., -5.4986e+00,\n",
      "           -1.5302e+00,  5.7347e+00],\n",
      "          ...,\n",
      "          [-1.2095e+01, -6.7823e+00, -8.4611e-02,  ...,  7.3173e+00,\n",
      "           -8.5650e+00,  2.1244e+00],\n",
      "          [ 6.2163e+00, -8.5033e+00, -2.4973e+00,  ...,  6.9891e+00,\n",
      "            5.2473e+00, -4.0982e+00],\n",
      "          [ 3.8534e-01, -4.8414e+00,  2.9262e+00,  ..., -5.2821e+00,\n",
      "           -1.7023e+00, -5.5599e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.8814e+00, -4.5118e+00,  5.8500e+00,  ..., -1.7505e+00,\n",
      "           -9.7674e+00,  6.5590e+00],\n",
      "          [-1.1605e+01, -8.1115e-01, -2.7348e+00,  ...,  3.6612e+00,\n",
      "           -1.6930e+00,  2.9778e+00],\n",
      "          [ 1.3404e+00,  5.8532e+00, -1.3133e+01,  ...,  2.7410e+00,\n",
      "           -3.1024e+00, -2.4988e+00],\n",
      "          ...,\n",
      "          [ 5.7065e+00, -3.4710e+00,  2.2102e+00,  ...,  1.6726e+00,\n",
      "           -1.9818e-01, -2.3744e+00],\n",
      "          [ 9.0111e+00,  7.6259e-01, -7.9463e-01,  ...,  3.0037e+00,\n",
      "            5.7595e+00,  6.1629e-01],\n",
      "          [ 6.3312e+00,  8.9499e+00, -4.7430e-01,  ..., -5.9395e+00,\n",
      "            4.1113e+00, -9.1330e-01]],\n",
      "\n",
      "         [[-4.8814e+00, -4.5118e+00,  5.8500e+00,  ..., -1.7505e+00,\n",
      "           -9.7674e+00,  6.5590e+00],\n",
      "          [-1.1605e+01, -8.1115e-01, -2.7348e+00,  ...,  3.6612e+00,\n",
      "           -1.6930e+00,  2.9778e+00],\n",
      "          [ 1.3404e+00,  5.8532e+00, -1.3133e+01,  ...,  2.7410e+00,\n",
      "           -3.1024e+00, -2.4988e+00],\n",
      "          ...,\n",
      "          [ 5.7065e+00, -3.4710e+00,  2.2102e+00,  ...,  1.6726e+00,\n",
      "           -1.9818e-01, -2.3744e+00],\n",
      "          [ 9.0111e+00,  7.6259e-01, -7.9463e-01,  ...,  3.0037e+00,\n",
      "            5.7595e+00,  6.1629e-01],\n",
      "          [ 6.3312e+00,  8.9499e+00, -4.7430e-01,  ..., -5.9395e+00,\n",
      "            4.1113e+00, -9.1330e-01]],\n",
      "\n",
      "         [[-4.8814e+00, -4.5118e+00,  5.8500e+00,  ..., -1.7505e+00,\n",
      "           -9.7674e+00,  6.5590e+00],\n",
      "          [-1.1605e+01, -8.1115e-01, -2.7348e+00,  ...,  3.6612e+00,\n",
      "           -1.6930e+00,  2.9778e+00],\n",
      "          [ 1.3404e+00,  5.8532e+00, -1.3133e+01,  ...,  2.7410e+00,\n",
      "           -3.1024e+00, -2.4988e+00],\n",
      "          ...,\n",
      "          [ 5.7065e+00, -3.4710e+00,  2.2102e+00,  ...,  1.6726e+00,\n",
      "           -1.9818e-01, -2.3744e+00],\n",
      "          [ 9.0111e+00,  7.6259e-01, -7.9463e-01,  ...,  3.0037e+00,\n",
      "            5.7595e+00,  6.1629e-01],\n",
      "          [ 6.3312e+00,  8.9499e+00, -4.7430e-01,  ..., -5.9395e+00,\n",
      "            4.1113e+00, -9.1330e-01]]],\n",
      "\n",
      "\n",
      "        [[[-4.8814e+00, -4.5118e+00,  5.8500e+00,  ..., -1.7505e+00,\n",
      "           -9.7674e+00,  6.5590e+00],\n",
      "          [-1.1605e+01, -8.1115e-01, -2.7348e+00,  ...,  3.6612e+00,\n",
      "           -1.6930e+00,  2.9778e+00],\n",
      "          [ 1.3404e+00,  5.8532e+00, -1.3133e+01,  ...,  2.7410e+00,\n",
      "           -3.1024e+00, -2.4988e+00],\n",
      "          ...,\n",
      "          [ 5.7065e+00, -3.4710e+00,  2.2102e+00,  ...,  1.6726e+00,\n",
      "           -1.9818e-01, -2.3744e+00],\n",
      "          [ 9.0111e+00,  7.6259e-01, -7.9463e-01,  ...,  3.0037e+00,\n",
      "            5.7595e+00,  6.1629e-01],\n",
      "          [ 6.3312e+00,  8.9499e+00, -4.7430e-01,  ..., -5.9395e+00,\n",
      "            4.1113e+00, -9.1330e-01]],\n",
      "\n",
      "         [[-4.8814e+00, -4.5118e+00,  5.8500e+00,  ..., -1.7505e+00,\n",
      "           -9.7674e+00,  6.5590e+00],\n",
      "          [-1.1605e+01, -8.1115e-01, -2.7348e+00,  ...,  3.6612e+00,\n",
      "           -1.6930e+00,  2.9778e+00],\n",
      "          [ 1.3404e+00,  5.8532e+00, -1.3133e+01,  ...,  2.7410e+00,\n",
      "           -3.1024e+00, -2.4988e+00],\n",
      "          ...,\n",
      "          [ 5.7065e+00, -3.4710e+00,  2.2102e+00,  ...,  1.6726e+00,\n",
      "           -1.9818e-01, -2.3744e+00],\n",
      "          [ 9.0111e+00,  7.6259e-01, -7.9463e-01,  ...,  3.0037e+00,\n",
      "            5.7595e+00,  6.1629e-01],\n",
      "          [ 6.3312e+00,  8.9499e+00, -4.7430e-01,  ..., -5.9395e+00,\n",
      "            4.1113e+00, -9.1330e-01]],\n",
      "\n",
      "         [[-4.8814e+00, -4.5118e+00,  5.8500e+00,  ..., -1.7505e+00,\n",
      "           -9.7674e+00,  6.5590e+00],\n",
      "          [-1.1605e+01, -8.1115e-01, -2.7348e+00,  ...,  3.6612e+00,\n",
      "           -1.6930e+00,  2.9778e+00],\n",
      "          [ 1.3404e+00,  5.8532e+00, -1.3133e+01,  ...,  2.7410e+00,\n",
      "           -3.1024e+00, -2.4988e+00],\n",
      "          ...,\n",
      "          [ 5.7065e+00, -3.4710e+00,  2.2102e+00,  ...,  1.6726e+00,\n",
      "           -1.9818e-01, -2.3744e+00],\n",
      "          [ 9.0111e+00,  7.6259e-01, -7.9463e-01,  ...,  3.0037e+00,\n",
      "            5.7595e+00,  6.1629e-01],\n",
      "          [ 6.3312e+00,  8.9499e+00, -4.7430e-01,  ..., -5.9395e+00,\n",
      "            4.1113e+00, -9.1330e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.8814e+00, -4.5118e+00,  5.8500e+00,  ..., -1.7505e+00,\n",
      "           -9.7674e+00,  6.5590e+00],\n",
      "          [-1.1605e+01, -8.1115e-01, -2.7348e+00,  ...,  3.6612e+00,\n",
      "           -1.6930e+00,  2.9778e+00],\n",
      "          [ 1.3404e+00,  5.8532e+00, -1.3133e+01,  ...,  2.7410e+00,\n",
      "           -3.1024e+00, -2.4988e+00],\n",
      "          ...,\n",
      "          [ 5.7065e+00, -3.4710e+00,  2.2102e+00,  ...,  1.6726e+00,\n",
      "           -1.9818e-01, -2.3744e+00],\n",
      "          [ 9.0111e+00,  7.6259e-01, -7.9463e-01,  ...,  3.0037e+00,\n",
      "            5.7595e+00,  6.1629e-01],\n",
      "          [ 6.3312e+00,  8.9499e+00, -4.7430e-01,  ..., -5.9395e+00,\n",
      "            4.1113e+00, -9.1330e-01]],\n",
      "\n",
      "         [[-4.8814e+00, -4.5118e+00,  5.8500e+00,  ..., -1.7505e+00,\n",
      "           -9.7674e+00,  6.5590e+00],\n",
      "          [-1.1605e+01, -8.1115e-01, -2.7348e+00,  ...,  3.6612e+00,\n",
      "           -1.6930e+00,  2.9778e+00],\n",
      "          [ 1.3404e+00,  5.8532e+00, -1.3133e+01,  ...,  2.7410e+00,\n",
      "           -3.1024e+00, -2.4988e+00],\n",
      "          ...,\n",
      "          [ 5.7065e+00, -3.4710e+00,  2.2102e+00,  ...,  1.6726e+00,\n",
      "           -1.9818e-01, -2.3744e+00],\n",
      "          [ 9.0111e+00,  7.6259e-01, -7.9463e-01,  ...,  3.0037e+00,\n",
      "            5.7595e+00,  6.1629e-01],\n",
      "          [ 6.3312e+00,  8.9499e+00, -4.7430e-01,  ..., -5.9395e+00,\n",
      "            4.1113e+00, -9.1330e-01]],\n",
      "\n",
      "         [[-4.8814e+00, -4.5118e+00,  5.8500e+00,  ..., -1.7505e+00,\n",
      "           -9.7674e+00,  6.5590e+00],\n",
      "          [-1.1605e+01, -8.1115e-01, -2.7348e+00,  ...,  3.6612e+00,\n",
      "           -1.6930e+00,  2.9778e+00],\n",
      "          [ 1.3404e+00,  5.8532e+00, -1.3133e+01,  ...,  2.7410e+00,\n",
      "           -3.1024e+00, -2.4988e+00],\n",
      "          ...,\n",
      "          [ 5.7065e+00, -3.4710e+00,  2.2102e+00,  ...,  1.6726e+00,\n",
      "           -1.9818e-01, -2.3744e+00],\n",
      "          [ 9.0111e+00,  7.6259e-01, -7.9463e-01,  ...,  3.0037e+00,\n",
      "            5.7595e+00,  6.1629e-01],\n",
      "          [ 6.3312e+00,  8.9499e+00, -4.7430e-01,  ..., -5.9395e+00,\n",
      "            4.1113e+00, -9.1330e-01]]]])\n",
      "Shape of Tensor:  torch.Size([3, 10, 30, 100])\n"
     ]
    }
   ],
   "source": [
    "indexed_tensor = product[long_tensor]\n",
    "print('Indexed Tensor: \\n' , indexed_tensor)\n",
    "\n",
    "print('Shape of Tensor: ' , indexed_tensor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Use  torch.mean  to average across the last dimension in the tensor from step 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2157,  1.0234, -0.8735,  0.1761,  0.2776, -0.4006,  1.1887,\n",
      "           0.4868,  0.2982, -0.5467,  0.3171,  0.6192,  0.1026,  0.5908,\n",
      "          -0.8421,  0.2589, -1.1169, -0.1902,  0.1689,  0.9301,  0.1618,\n",
      "           0.6100,  0.7669,  0.6138, -0.7244, -0.8433, -0.2402, -0.0565,\n",
      "           0.6954, -0.1287],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831]],\n",
      "\n",
      "        [[ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.0863,  0.0985,  0.4572,  1.3022,  0.4036,  0.7790, -0.1730,\n",
      "          -0.6420,  0.6789, -0.0820,  0.6861,  0.2028,  0.3143, -0.1696,\n",
      "           0.3218,  0.3981,  0.8887,  0.6937,  0.0251, -0.9999, -1.0070,\n",
      "          -0.8767,  0.4000, -0.0232,  0.4276,  0.0978,  0.8906,  0.0593,\n",
      "          -0.6233, -0.3309],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831]],\n",
      "\n",
      "        [[ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.1329, -0.0896,  0.2795,  0.8808,  0.4326, -0.1488, -0.5484,\n",
      "           0.5850,  0.9779, -0.5234,  0.2968,  0.6432, -0.4675,  1.0159,\n",
      "          -0.6173,  0.3706,  0.5775,  1.0739,  0.1478,  0.3320, -0.7403,\n",
      "           0.1528,  0.2367,  1.3933,  0.0887,  0.9617, -0.4070,  0.2133,\n",
      "           1.0300,  0.4709],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831],\n",
      "         [ 0.3585, -0.2706, -0.6280,  0.2025, -0.3965,  0.0791, -0.2661,\n",
      "           1.6628, -0.5922, -0.0110, -0.3937, -0.6176,  0.0131, -0.2943,\n",
      "          -0.2396,  0.0329, -0.5518,  0.6359, -0.0845,  0.1698,  0.1838,\n",
      "           0.1674,  0.1482, -0.1087, -0.1855,  0.4301, -0.6898,  0.0579,\n",
      "           0.3037,  0.9831]]])\n",
      "Shape of Tensor:  torch.Size([3, 10, 30])\n"
     ]
    }
   ],
   "source": [
    "mean_tensor = torch.mean(indexed_tensor, \n",
    "                         dim = 3)\n",
    "print(mean_tensor)\n",
    "\n",
    "print('Shape of Tensor: ' , mean_tensor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Redo step 2. on the GPU and compare results from step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product of tensor_a and tensor_b on GPU: \n",
      " tensor([[[-5.9818e+00, -1.7661e+00, -1.6034e+00,  ..., -3.0387e+00,\n",
      "          -3.7300e-01, -1.1283e+00],\n",
      "         [ 4.9403e+00, -2.2627e-01,  1.4862e+00,  ..., -1.2954e+01,\n",
      "           1.8034e+00,  2.4297e+00],\n",
      "         [-1.8042e+00, -1.0814e+01,  1.9619e+00,  ...,  2.5432e-01,\n",
      "          -2.8195e-01, -1.5294e+00],\n",
      "         ...,\n",
      "         [-1.9089e-01,  8.1660e+00,  3.3491e+00,  ...,  9.2679e+00,\n",
      "           5.1710e+00, -5.0976e+00],\n",
      "         [-3.1682e-01, -9.1286e-01, -4.4653e+00,  ...,  3.5076e+00,\n",
      "          -5.4038e+00,  8.5440e-01],\n",
      "         [-7.3638e+00,  2.7562e+00,  1.4914e+00,  ..., -2.3446e+00,\n",
      "          -5.7287e+00,  7.7993e-01]],\n",
      "\n",
      "        [[-4.8814e+00, -4.5118e+00,  5.8500e+00,  ..., -1.7505e+00,\n",
      "          -9.7674e+00,  6.5590e+00],\n",
      "         [-1.1605e+01, -8.1115e-01, -2.7348e+00,  ...,  3.6612e+00,\n",
      "          -1.6930e+00,  2.9778e+00],\n",
      "         [ 1.3404e+00,  5.8532e+00, -1.3133e+01,  ...,  2.7410e+00,\n",
      "          -3.1024e+00, -2.4988e+00],\n",
      "         ...,\n",
      "         [ 5.7065e+00, -3.4710e+00,  2.2102e+00,  ...,  1.6726e+00,\n",
      "          -1.9818e-01, -2.3744e+00],\n",
      "         [ 9.0111e+00,  7.6259e-01, -7.9463e-01,  ...,  3.0037e+00,\n",
      "           5.7595e+00,  6.1629e-01],\n",
      "         [ 6.3312e+00,  8.9499e+00, -4.7430e-01,  ..., -5.9395e+00,\n",
      "           4.1113e+00, -9.1330e-01]],\n",
      "\n",
      "        [[ 9.3840e+00,  4.8832e-01, -3.3650e+00,  ..., -4.5400e+00,\n",
      "          -7.1142e-01, -7.8463e-01],\n",
      "         [-5.2652e+00,  6.5980e+00,  5.4461e+00,  ...,  5.3439e+00,\n",
      "           7.1507e+00, -3.1384e+00],\n",
      "         [-7.6232e+00, -9.1756e-01, -2.4831e+00,  ..., -6.1729e+00,\n",
      "          -4.1371e-02,  1.0506e+01],\n",
      "         ...,\n",
      "         [-9.3179e+00, -4.4575e+00,  5.9407e+00,  ...,  9.2306e+00,\n",
      "          -4.2039e+00,  1.1471e+01],\n",
      "         [ 9.2039e+00,  4.2572e+00,  1.2229e+01,  ...,  4.4297e+00,\n",
      "           4.0283e+00,  4.2574e+00],\n",
      "         [ 7.9123e-03, -4.2531e+00,  8.1606e+00,  ...,  2.4967e+00,\n",
      "           6.3144e+00, -1.6940e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.1648e+00, -8.4531e+00,  1.0180e+01,  ..., -4.0630e+00,\n",
      "          -8.5387e+00,  1.6104e+00],\n",
      "         [-1.3111e+00,  7.3909e-01, -7.3345e+00,  ...,  3.0954e+00,\n",
      "          -5.0717e+00,  5.9212e+00],\n",
      "         [ 7.2651e-01,  2.4438e+00,  3.4895e-01,  ..., -3.7833e+00,\n",
      "          -5.1596e+00, -1.2746e+00],\n",
      "         ...,\n",
      "         [-9.1673e+00,  8.4125e+00, -7.1363e+00,  ..., -6.3062e-01,\n",
      "           1.4020e+01, -3.9658e+00],\n",
      "         [ 1.5060e+00,  2.6391e+00, -4.1815e-01,  ..., -1.8989e+00,\n",
      "           6.6191e-01,  1.8884e+00],\n",
      "         [ 7.4493e+00,  6.3730e-01,  1.1929e+00,  ...,  3.1907e-01,\n",
      "           8.1735e+00, -2.4934e+00]],\n",
      "\n",
      "        [[-4.0229e+00,  3.5699e-01,  2.9880e+00,  ..., -2.6103e-01,\n",
      "          -7.5534e+00, -9.5645e+00],\n",
      "         [ 1.0906e+01, -9.5006e+00, -2.8851e+00,  ..., -2.4495e+00,\n",
      "          -1.0234e+00, -8.1322e+00],\n",
      "         [-2.4637e+00,  3.0555e+00,  1.6118e+01,  ...,  5.4542e+00,\n",
      "           8.2009e+00,  3.8112e+00],\n",
      "         ...,\n",
      "         [-2.9776e+00,  1.4443e+00, -1.3679e+00,  ...,  1.1193e+00,\n",
      "           3.8217e+00, -3.7400e-01],\n",
      "         [-4.2947e+00,  6.0622e+00, -2.0172e+00,  ..., -8.4275e+00,\n",
      "          -9.2995e+00,  3.4134e+00],\n",
      "         [ 1.6167e+00, -7.0122e+00, -2.8927e-01,  ...,  2.8121e+00,\n",
      "          -1.1998e+01,  5.4115e+00]],\n",
      "\n",
      "        [[ 7.5365e+00, -6.5178e+00,  4.7000e+00,  ..., -6.3404e+00,\n",
      "           6.9859e+00, -2.8562e+00],\n",
      "         [ 6.1621e+00, -1.6796e+00,  5.2082e+00,  ...,  7.8601e+00,\n",
      "          -8.0286e-01,  4.0619e+00],\n",
      "         [ 7.9669e+00,  6.9972e+00,  2.4254e+00,  ...,  4.4881e+00,\n",
      "           4.3400e+00, -8.3085e+00],\n",
      "         ...,\n",
      "         [-8.8918e+00, -2.1446e+00,  6.3355e+00,  ...,  1.4202e+01,\n",
      "          -2.1995e+00,  8.8337e+00],\n",
      "         [ 3.6025e+00,  8.5873e+00, -6.7997e+00,  ...,  8.5181e+00,\n",
      "           8.4043e+00, -8.4843e+00],\n",
      "         [-7.8089e+00, -5.9742e+00,  7.8201e+00,  ...,  9.4723e-01,\n",
      "           1.2701e+01, -7.8987e-01]]])\n",
      "Shape of Tensor:  torch.Size([29, 30, 100])\n"
     ]
    }
   ],
   "source": [
    "tensor_a_cuda = tensor_a.to(device = device)\n",
    "tensor_b_cuda = tensor_b.to(device = device)\n",
    "product_gpu = torch.matmul(tensor_a, tensor_b)\n",
    "print('Product of tensor_a and tensor_b on GPU: \\n' , product_gpu)\n",
    "\n",
    "print('Shape of Tensor: ' , product_gpu.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Write a pure PyTorch program to compute the value of $\\sqrt{2}$ up to 4 decimal places without using the square root or other math functions from any of the libraries. \n",
    "### Hint: Notice that the answer is the (positive) root of the equation, $$𝑥^2 −2 = 0$$ \n",
    "### To find the root, you might want to use \"Newton's Method\": $$𝑥_{𝑛+1} = 𝑥_{𝑛} − \\frac{𝑓(𝑥)}{𝑓′(𝑥)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fail-fast prototyping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building neural networks, you want things to either work or fail fast. Long iteration loops are \n",
    "the worst enemy of a machine learning practitioner. \\\n",
    "For e.g., while writing code, you might want to incrementally test your code by doing something \n",
    "like this:\n",
    "\n",
    "batch_size = 32 \\\n",
    "num_features = 512 \\\n",
    "embedding_size = 16\n",
    "\n",
    "\\# construct a dummy input \\\n",
    "x = torch.randn(batch_size, num_features)\n",
    "\n",
    "\\# we want to project the input to embedding_size \\\n",
    "fc = torch.nn.Linear(num_features, embedding_size)\n",
    "\n",
    "\\# test if that works \\\n",
    "print(fc(x).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fail-fast exercises"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. [Glove](https://nlp.stanford.edu/projects/glove/) has 300 dimension embeddings. Design an nn.Module that takes a sentence of max_len words, tokenizes words by spaces, represents the sentence by averaging the glove embeddings of constituent words. What is the shape of the resulting sentence embedding? When you implement this, you will need to make some assumptions. What are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2196017 words present in GloVe\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe Embeddings\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "GLOVE_DIM = 300\n",
    "glove = GloVe(name = '840B', \n",
    "              dim = GLOVE_DIM)\n",
    "\n",
    "print(f'Loaded {len(glove.itos)} words present in GloVe')\n",
    "\n",
    "embeddings_tensor = glove.vectors\n",
    "embeddings_tensor = embeddings_tensor.to(device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "NUM_SENT = 512\n",
    "sents = list()\n",
    "for i in range(NUM_SENT):\n",
    "    sents.append('This is the quest zero and it has a deadline this Sunday March 29')\n",
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 300])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GloveEmbeddingAvg(nn.Module):\n",
    "    \n",
    "    def __init__(self, max_len):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings_tensor)\n",
    "        \n",
    "    def forward(self, \n",
    "                sent):\n",
    "        # Tokenize the sentence by spaces\n",
    "        tokens = sent.split(' ')[:self.max_len]\n",
    "        # Get idx of each token from the GloVe dictionary\n",
    "        glove_dict_indexes = [glove.stoi[token] for token in tokens]\n",
    "        # Convert it into Tensor\n",
    "        glove_dict_indexes = torch.tensor(glove_dict_indexes, \n",
    "                                          device = device)\n",
    "        # Get Word Embeddings for all tokens\n",
    "        word_embeds = self.embedding(glove_dict_indexes)\n",
    "        # Sentence Embedding = Average of Word Embeddings\n",
    "        sent_embeds = word_embeds.mean(dim = 0)\n",
    "        # Reshape Sentence Embedding as a 2D Tensor\n",
    "        return sent_embeds.view(1, -1)\n",
    "\n",
    "MAX_LEN = 10\n",
    "glove_embeds_avg = GloveEmbeddingAvg(MAX_LEN).to(device)\n",
    "\n",
    "print(glove_embeds_avg(sents[0]).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How will you modify step 1. so that the sentence embeddings are in $R^{50}$ ?\n",
    "BONUS: Can you think of more than one way to do this? What are the implications of each method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveEmbeddingAvg_50_Dim(nn.Module):\n",
    "    \n",
    "    def __init__(self, max_len):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings_tensor)\n",
    "        self.fc = nn.Linear(GLOVE_DIM, 50)\n",
    "        \n",
    "    def forward(self, \n",
    "                x):\n",
    "        # Slice each sentence to Max Length\n",
    "        x = x[:, :self.max_len]\n",
    "        # Get Word Embeddings for all tokens\n",
    "        word_embeds = self.embedding(x) # [BATCH_SIZE, MAX_LEN, GLOVE_DIM]\n",
    "        # Sentence Embedding = Average of Word Embeddings\n",
    "        sent_embeds = word_embeds.mean(dim = 0) # [MAX_LEN, GLOVE_DIM]\n",
    "        # Linear Layer to reduce Sentence Embedding Dimension to 50\n",
    "        return self.fc(sent_embeds) # [MAX_LEN, 50]\n",
    "\n",
    "MAX_LEN = 10\n",
    "glove_embeds_avg_50_dim = GloveEmbeddingAvg_50_Dim(MAX_LEN).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Quickly test your answer in step 2. with a batch of 512 sentences on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 50])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a Sentence\n",
    "def tokenize(sent):\n",
    "    # Tokenize the sentence by spaces\n",
    "    tokens = sent.split(' ')\n",
    "    # Get idx of each token from the GloVe dictionary\n",
    "    glove_dict_indexes = [glove.stoi[token] for token in tokens]\n",
    "    return glove_dict_indexes\n",
    "\n",
    "# Create Tokenized Sentence Corpus\n",
    "tokenized_sents = list()\n",
    "for sent in sents:\n",
    "    tokenized_sents.append(tokenize(sent))\n",
    "tokenized_sents = torch.tensor(tokenized_sents, \n",
    "                               device = device)\n",
    "\n",
    "# Run forward pass\n",
    "BATCH_SIZE = 512\n",
    "for i in range(0, len(tokenized_sents), BATCH_SIZE):\n",
    "    batch = tokenized_sents[i:i+BATCH_SIZE]\n",
    "    sentence_embeddings = glove_embeds_avg_50_dim(batch)\n",
    "    print(sentence_embeddings.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You almost implemented the model in the Deep Averaging Networks (DAN) paper!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Task: \n",
    "### Create a   MultiEmbedding  Module that can take two sets of indices, embed them, and concat the results. You might remember it from the previous lecture where we had to produce an embedding for \"green apple\" from embeddings of \"green\" and \"apple\". Your  MultiEmbedding class should work with the following test code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_emb, \n",
    "                 size_emb1, \n",
    "                 size_emb2):\n",
    "        super().__init__()\n",
    "        self.embedding_A = nn.Embedding(num_emb, size_emb1)\n",
    "        self.embedding_B = nn.Embedding(num_emb, size_emb2)\n",
    "        \n",
    "    def forward(self, \n",
    "                indices1, \n",
    "                indices2):\n",
    "        embed_A = self.embedding_A(indices1)\n",
    "        embed_B = self.embedding_B(indices2)\n",
    "        # Concatenate the Embeddings\n",
    "        return torch.cat((embed_A, embed_B), \n",
    "                         dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10, 500])\n"
     ]
    }
   ],
   "source": [
    "# Test code: instantiate a MultiEmbedding with the sizes for each embedding. \n",
    "# For this example, you can just randomly initialize each interior embedding. \n",
    "# In a practical setting, you might support methods for initializing with \n",
    "# combinations of embeddings, such as GloVe 300d vectors and word2vec 200d \n",
    "# vectors, yielding 500d embeddings. Both embeddings share a vocabulary/range \n",
    "# of supported indices indicated by `num_emb`\n",
    "\n",
    "NUM_EMB = 10000\n",
    "SIZE_EMB1 = 300\n",
    "SIZE_EMB2 = 200\n",
    "BATCH_SIZE = 64\n",
    "NUM_LENGTH = 10\n",
    "\n",
    "multiemb = MultiEmbedding(NUM_EMB, \n",
    "                          SIZE_EMB1, \n",
    "                          SIZE_EMB2).to(device)\n",
    "\n",
    "# You can then call this with a pair of indices where each value is in 0 <= i < num_emb\n",
    "indices1 =  torch.randint(0, \n",
    "                          NUM_EMB, \n",
    "                          (BATCH_SIZE, NUM_LENGTH), \n",
    "                          dtype = torch.long, \n",
    "                          device = device) # long tensor of shape (batch, num_length)\n",
    "indices2 =  torch.randint(0, \n",
    "                          NUM_EMB, \n",
    "                          (BATCH_SIZE, NUM_LENGTH), \n",
    "                          dtype = torch.long, \n",
    "                          device = device) # long tensor of shape (batch, num_length)\n",
    "output = multiemb(indices1, \n",
    "                  indices2)\n",
    "print(output.shape) # should be (batch, num_length, size_emb1 + size_emb2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Datasets and DataLoaders: \n",
    "### Read this short post on PyTorch Dataset and DataLoaders. Often in prototyping we need to generate dummy datasets to test our models. Implement a PyTorch Dataset class that generates up to  num_sentences  random sentences of length up to  max_len words. For each sentence, generate a binary label. You should be able to test your code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAveragingNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, max_len):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings_tensor)\n",
    "        self.fc = nn.Linear(GLOVE_DIM, 50)\n",
    "        \n",
    "    def forward(self, \n",
    "                sent):\n",
    "        # Tokenize the sentence by spaces\n",
    "        tokens = sent.split(' ')[:self.max_len]\n",
    "        # Get idx of each token from the GloVe dictionary\n",
    "        glove_dict_indexes = [glove.stoi[token] for token in tokens]\n",
    "        # Convert it into Tensor\n",
    "        glove_dict_indexes = torch.tensor(glove_dict_indexes, \n",
    "                                          device = device)\n",
    "        # Get Word Embeddings for all tokens\n",
    "        word_embeds = self.embedding(glove_dict_indexes)\n",
    "        # Sentence Embedding = Average of Word Embeddings\n",
    "        sent_embeds = word_embeds.mean(dim = 0)\n",
    "        return self.fc(sent_embeds.view(1, -1))\n",
    "\n",
    "model = DeepAveragingNetwork(MAX_LEN).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DummySentenceLabelDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_sentences, \n",
    "                 max_len):\n",
    "        self.num_sentences = num_sentences\n",
    "        self.max_len = max_len\n",
    "        self.sents = self.generate_sents()\n",
    "        self.labels = self.generate_labels()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_sentences\n",
    "        \n",
    "    def __getitem__(self, \n",
    "                    idx):\n",
    "        return self.sents[idx], self.labels[idx]\n",
    "    \n",
    "    def get_random_sent(self, word_list):\n",
    "        sent = ''\n",
    "        for i in range(self.max_len):\n",
    "            word = random.choice(word_list)\n",
    "            sent += word + ' '\n",
    "        return sent\n",
    "    \n",
    "    def generate_sents(self):\n",
    "        word_list = ['Hello', 'World', 'Python', 'Function', 'Random', 'Sentence', \n",
    "                     'List', 'Words', 'Generates', '20', 'Example', 'Simple', 'Program', \n",
    "                     'Easy', 'Understand', 'Learn', 'Code', 'Implement', 'Execute', 'Run', \n",
    "                     'Brazil', 'India', 'Chat', 'India', 'Golden', 'State', 'Warriors']\n",
    "        sents = list()\n",
    "        for i in range(self.num_sentences):\n",
    "            sent = self.get_random_sent(word_list)\n",
    "            sents.append(sent)\n",
    "        return sents\n",
    "    \n",
    "    def generate_labels(self):\n",
    "        labels = list()\n",
    "        for i in range(self.num_sentences):\n",
    "            labels.append(random.randint(0, 1))\n",
    "        return labels\n",
    "\n",
    "NUM_SENTENCES = 10\n",
    "MAX_LEN = 20\n",
    "dataset = DummySentenceLabelDataset(num_sentences = NUM_SENTENCES, \n",
    "                                    max_len = MAX_LEN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's measure the error rate for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: State India Hello Chat List Understand Python Warriors Sentence Learn Example Words State Run 20 Brazil India Chat Understand Chat ,\n",
      "Label: 1\n",
      "Sentence: India Sentence Understand Example Execute Easy Implement Simple Run Generates World India Function Chat Simple State India Program State Warriors ,\n",
      "Label: 1\n",
      "Sentence: Hello World Example Random Execute Python Example Random Example Function Hello Run Sentence Run Words Words Warriors List Python Program ,\n",
      "Label: 0\n",
      "Sentence: Learn Execute Chat Function Easy Run Brazil India Example Code Function Warriors Hello List Chat Warriors Chat Simple Random Generates ,\n",
      "Label: 0\n",
      "Sentence: Words Sentence Golden India 20 Generates Execute Sentence Brazil Words Easy Golden Program Simple 20 Function Simple Simple Simple List ,\n",
      "Label: 1\n",
      "Sentence: Warriors Understand Understand World Run Hello Learn Example Run Program 20 Understand Brazil Random Warriors Program Hello World Example Example ,\n",
      "Label: 1\n",
      "Sentence: Example Brazil Learn Understand Program Run Run Function Code Function Easy Implement Warriors Chat Golden Golden World State Easy Chat ,\n",
      "Label: 1\n",
      "Sentence: Words Words Simple Chat Understand List Hello Golden List Golden Easy Generates Program Generates Sentence India Program State Understand Warriors ,\n",
      "Label: 1\n",
      "Sentence: Simple Python Python Learn Code Hello Warriors Brazil Random List Code Function Simple Random List India Learn 20 State State ,\n",
      "Label: 0\n",
      "Sentence: Warriors Program List Python State Python Execute Program World Example Implement Easy Program Execute India World Generates Sentence Warriors Random ,\n",
      "Label: 1\n",
      "==================================================================\n",
      "Error rate: tensor([[0.7610, 0.6002, 0.8560, 0.7186, 0.7291, 0.7903, 0.7012, 0.8387, 0.7148,\n",
      "         0.7622, 0.6591, 0.7589, 0.8425, 0.7277, 0.7267, 0.6778, 0.8023, 0.7127,\n",
      "         0.6451, 0.6664, 0.6814, 0.7698, 0.7001, 0.7206, 0.7326, 0.5650, 0.6638,\n",
      "         0.6189, 0.6251, 0.7091, 0.7066, 0.7328, 0.7109, 0.6403, 0.8514, 0.8148,\n",
      "         0.8179, 0.6104, 0.6627, 0.7756, 0.7185, 0.6017, 0.7151, 0.6507, 0.7441,\n",
      "         0.6826, 0.6184, 0.8902, 0.8902, 0.8801]], device='mps:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "error = 0.0\n",
    "for sentence, label in dataset:\n",
    "  print(f'Sentence: {sentence},\\nLabel: {label}')\n",
    "  prediction = model(sentence)\n",
    "  error += abs(prediction - label)\n",
    "print('==================================================================')  \n",
    "print(f'Error rate: {error/len(dataset)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b37beb76676c8b0643f5764b5c5ae0ddf876ecbab29b433e279cae2d82963c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
