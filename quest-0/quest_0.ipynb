{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kushagra Seth -> Quest-0(NLP-244)\n",
    "### kuseth@ucsc.edu\n",
    "### Student ID: 2005986"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "0.14.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "print(torch.__version__)\n",
    "print(torchtext.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set device as GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    dev = 'mps'\n",
    "else:\n",
    "    dev = 'cpu'\n",
    "device = torch.device(dev)    \n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch warmup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use torch.randn to create two tensors of size (29, 30, 32) and (32, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of Size (29, 30, 32): \n",
      " tensor([[[-1.8250, -1.2043,  0.8640,  ...,  2.0039, -0.6876, -0.7435],\n",
      "         [ 1.5837, -0.4014, -0.5969,  ..., -1.8388, -1.8365, -0.1216],\n",
      "         [ 2.1305,  1.9082, -0.0703,  ...,  1.6481, -1.2472, -0.1516],\n",
      "         ...,\n",
      "         [-0.6248,  0.0737,  0.0201,  ..., -1.4963, -0.3888, -1.4361],\n",
      "         [ 0.9371,  0.7994,  1.6556,  ...,  0.1013, -0.0883, -0.4365],\n",
      "         [ 0.5516, -0.6175,  1.4019,  ..., -0.2200, -0.9271, -0.9267]],\n",
      "\n",
      "        [[-0.8407, -0.1999,  0.8184,  ..., -1.1076,  0.9687,  0.5709],\n",
      "         [ 0.3132,  0.7963, -1.1212,  ..., -0.0759,  1.9350, -1.2549],\n",
      "         [ 0.5318,  0.2876, -0.3385,  ...,  0.7122,  0.5005,  0.4711],\n",
      "         ...,\n",
      "         [-0.3604,  0.7390, -1.0716,  ...,  0.4013, -1.3561, -0.4087],\n",
      "         [-0.0552, -0.1154,  0.2454,  ..., -0.3374, -0.4628, -0.3023],\n",
      "         [-1.0101, -1.1009,  0.6721,  ...,  0.8856, -1.2659, -0.0539]],\n",
      "\n",
      "        [[-1.2904,  0.7627, -1.7949,  ..., -0.5664, -0.9933,  1.2508],\n",
      "         [-0.2317, -1.7380, -0.7585,  ...,  0.4299, -2.0802, -1.4434],\n",
      "         [ 0.9176, -0.0246,  0.4937,  ..., -1.0787, -0.8084,  0.4599],\n",
      "         ...,\n",
      "         [ 0.9526, -1.0634, -0.5359,  ..., -1.5248, -0.1127,  0.5602],\n",
      "         [-0.4751,  0.1560, -1.1334,  ..., -1.4280, -0.3554, -0.0372],\n",
      "         [ 1.0668, -0.0793, -1.1785,  ..., -1.5996,  1.7070,  1.8988]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0511,  0.8906, -0.3743,  ..., -1.1458,  1.3662, -1.4205],\n",
      "         [ 0.1139,  0.8832, -1.9760,  ..., -1.7872, -0.9600, -0.4772],\n",
      "         [-0.2770, -1.0871,  0.3012,  ...,  0.8767,  1.2413, -0.3932],\n",
      "         ...,\n",
      "         [-1.0982,  0.3452, -0.4938,  ...,  0.8813, -1.0067, -1.9080],\n",
      "         [-0.9138, -0.1418,  0.6013,  ..., -0.4973,  0.5563, -0.6659],\n",
      "         [-0.9323, -0.2606,  0.8968,  ...,  0.0844, -2.3929,  0.1457]],\n",
      "\n",
      "        [[-0.0588, -0.3725,  0.7667,  ...,  0.3988,  0.8634,  0.7472],\n",
      "         [-1.1639,  0.7883, -0.0538,  ...,  0.5076, -0.7165, -1.1489],\n",
      "         [-0.4244, -0.8602, -0.4462,  ..., -0.8192, -0.9054, -0.0745],\n",
      "         ...,\n",
      "         [ 0.3674, -1.5753,  0.5963,  ..., -0.8003, -1.1773,  0.3036],\n",
      "         [-0.4273, -0.1631, -0.3477,  ...,  0.4978, -0.4680,  0.7220],\n",
      "         [ 0.4940,  1.5842, -0.1551,  ...,  0.0588, -0.8648,  1.5983]],\n",
      "\n",
      "        [[ 0.6820, -0.8257,  0.0131,  ..., -1.6870,  1.1337, -1.1446],\n",
      "         [ 1.6668, -0.3122,  0.1787,  ..., -1.3015,  0.7672, -0.6105],\n",
      "         [ 0.7576, -2.1969, -1.9072,  ..., -1.1878,  0.0960,  1.6621],\n",
      "         ...,\n",
      "         [-0.0121, -1.6435,  0.9746,  ...,  2.3434, -1.3586, -0.4653],\n",
      "         [-1.5642, -0.5483,  1.4298,  ...,  1.4847, -1.1436, -0.2565],\n",
      "         [ 0.7596, -0.3850,  1.2369,  ..., -0.5796, -2.2145,  0.0968]]])\n",
      "Tensor of Size (32, 100): \n",
      " tensor([[-0.8523, -0.9750,  0.9362,  ..., -0.7210, -1.0805, -0.1403],\n",
      "        [ 0.6025,  0.7784,  2.1606,  ...,  0.3809, -1.0024,  0.6555],\n",
      "        [ 1.0061,  1.2352,  0.6137,  ...,  1.1551,  0.7639,  0.9634],\n",
      "        ...,\n",
      "        [ 0.3576, -0.9494,  1.7217,  ...,  0.8204, -1.4994, -0.6138],\n",
      "        [ 1.2968, -0.8315, -0.3596,  ...,  0.0938,  0.5309, -0.3404],\n",
      "        [-1.2555, -0.6128, -0.0643,  ...,  0.0730, -0.4241,  0.2217]])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.randn(29, 30, 32)\n",
    "print('Tensor of Size (29, 30, 32): \\n' , tensor_a)\n",
    "\n",
    "tensor_b = torch.randn(32, 100)\n",
    "print('Tensor of Size (32, 100): \\n' , tensor_b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use  torch.matmul  to matrix multiply the two tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product of tensor_a and tensor_b: \n",
      " tensor([[[  3.0082,   0.2774,   0.2860,  ...,   4.7896,  -4.8246,  -5.3570],\n",
      "         [ -6.2760,  -0.9145,  -2.4638,  ...,  -0.6401,  -8.7860,   8.2480],\n",
      "         [  4.8915,   3.4413,  16.0579,  ...,  -0.3568,  -9.6738,   5.6802],\n",
      "         ...,\n",
      "         [  7.1380,   4.3842,   3.6447,  ...,  -4.5751,  -2.8540,  -5.3435],\n",
      "         [  8.4457,   4.1616,   4.5761,  ...,  10.2699,  -0.7791,  -0.3597],\n",
      "         [ -3.4007,  11.1838,  -3.4072,  ...,   1.9385,  -0.8124,   3.1373]],\n",
      "\n",
      "        [[ -4.5026,  -2.4407,   7.5931,  ...,  -5.0318,  -0.3388,   0.2325],\n",
      "         [ 14.8979,   5.7587,  -6.2131,  ...,  11.3459,   1.8926,   6.5630],\n",
      "         [ -3.9810,  -3.0537,  12.3846,  ...,  -8.5466,  -1.7133,  -8.8943],\n",
      "         ...,\n",
      "         [  4.0924,   4.2022,   0.1867,  ...,   2.7929,  -1.0690,   9.1906],\n",
      "         [ -3.1723,   0.1007,  -0.5834,  ...,   4.3986,   5.0610,  -1.0318],\n",
      "         [ -4.3487,   0.7072,   4.0640,  ...,  -5.1994,  -1.9691,   1.4542]],\n",
      "\n",
      "        [[ -9.8683, -12.2725,  -0.3319,  ...,  -3.9293,   0.1924,  -5.8769],\n",
      "         [ -2.4308,   1.0324,  -5.1080,  ...,  -5.3305,  -6.8257,   0.3756],\n",
      "         [ -8.4826,  -1.3951,  10.1890,  ...,  -8.8583,   4.0310,  -3.9838],\n",
      "         ...,\n",
      "         [  1.0088,   1.5307,  -4.8676,  ...,  -1.0770,   3.1684,  -2.8112],\n",
      "         [  7.2108,   3.6587,   1.0987,  ...,  -4.0202,   2.4188,  11.0244],\n",
      "         [  6.7456,   0.7441,  -4.3096,  ...,  -5.4091,  -0.9578,  -5.2660]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 12.0306,   5.6884,  -3.4003,  ...,   3.4182,   2.7516,  -5.8331],\n",
      "         [  2.0178,   8.7115,  -5.6349,  ...,  -6.3676,  -2.5997,   5.6853],\n",
      "         [ 12.2046,   2.9315,  -5.4668,  ...,   2.1545,   9.3212,  -0.7755],\n",
      "         ...,\n",
      "         [ -3.9099,   8.8735,   9.6865,  ...,  -7.2345,   2.9411,  -1.3267],\n",
      "         [  4.0376,  -0.1899,  -3.1151,  ...,  -0.6456,   1.8689,   0.7360],\n",
      "         [ -3.0743,   2.2886,  11.6498,  ...,  -4.7197,   1.9555,  -0.0771]],\n",
      "\n",
      "        [[ -3.0832,   2.9233,  -1.5363,  ...,   8.0021,  -1.4690,  -0.3125],\n",
      "         [  5.5819,  -0.8205,   7.4464,  ...,  -7.0727,   5.1118,  -9.6153],\n",
      "         [  7.5457,   1.8199,  -3.0314,  ...,  -1.4455,  -0.2121,  -2.4782],\n",
      "         ...,\n",
      "         [ -6.9256, -10.6333,  -0.7512,  ...,  -1.0363,  10.3745,   2.4319],\n",
      "         [ -1.7239,  -2.7712,  -4.7344,  ...,   7.1690,   3.3304,  -3.4514],\n",
      "         [ -5.0591,  -3.5502,   4.9229,  ...,  -0.5592,  -6.3918,   7.3881]],\n",
      "\n",
      "        [[  7.8078,   5.0535,  -1.7631,  ...,  -5.2606,  -2.7049,  -7.5602],\n",
      "         [ -3.5394,  -1.3881,   7.2511,  ..., -11.9620,  -0.5190,  -6.7947],\n",
      "         [ -2.6846,  -2.4491,  -8.8204,  ...,  -2.3635,   2.2110,  -2.2485],\n",
      "         ...,\n",
      "         [  2.0544,  -0.3922,  10.7518,  ...,   4.3098,   3.9669,  -8.2762],\n",
      "         [  4.3249,   6.3635,   2.6976,  ...,  -1.9902,  -5.2601,   0.8030],\n",
      "         [ -2.0803,   6.7196,   7.0118,  ...,  -5.2563,  -8.2437,   2.0623]]])\n",
      "Shape of Tensor:  torch.Size([29, 30, 100])\n"
     ]
    }
   ],
   "source": [
    "product = torch.matmul(tensor_a, \n",
    "                       tensor_b)\n",
    "print('Product of tensor_a and tensor_b: \\n' , product)\n",
    "\n",
    "print('Shape of Tensor: ' , product.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What is the difference between torch.matmul , torch.mm , torch.bmm , and torch.einsum , and the @ operator?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. torch.matmul() -> It is a matrix multiplication function that can handle broadcasting, it can be used to perform matrix multiplication between two tensors of any shape. The function will automatically broadcast the smaller tensor to match the shape of the larger tensor, and then perform the matrix multiplication.\n",
    "2. torch.mm() -> It is also a matrix multiplication function, but it is a lower level function that does not handle broadcasting. It can only be used to perform matrix multiplication between two tensors with the same number of dimensions and the last two dimensions should have the same size. It is faster than torch.matmul since it avoids broadcasting.\n",
    "3. torch.bmm() -> It is used for batch matrix multiplication, it accepts three tensors of shapes (batch size, n, m), (batch size, m, p) and (batch size, n, p) to perform matrix multiplication on the last two dimensions of the input tensors.\n",
    "4. torch.einsum() -> It is a more flexible function than the others and allows you to specify the indices of the tensors that you want to contract, it can also be used for many other operations like dot product, outer product, and tensor transpose.\n",
    "5. @ operator -> It is a shorthand for the torch.matmul() function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use torch.sum on the resulting tensor, passing the optional argument of dim=1 to sum across the 1st dimension. Before you run this, can you predict the size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Tensor across 1st Dimension: \n",
      " tensor([[ 39.6074,  49.1218,  20.0786,  ...,  49.6437,  -9.8850,  46.0369],\n",
      "        [ 28.5889,   1.2142,  40.1818,  ..., -31.0937,  -4.3091,  17.3024],\n",
      "        [-21.1270,  14.7893,  13.6362,  ..., -23.0662,   3.7872,  -1.3205],\n",
      "        ...,\n",
      "        [ 16.8607,  15.3761,  18.5757,  ..., -38.7244,  24.7217, -23.5465],\n",
      "        [-19.4163,  22.7045, -68.4617,  ...,  42.1744,  32.6496,  25.6979],\n",
      "        [-42.0406, -28.6637,  23.7927,  ..., -68.8295, -53.4164, -12.5331]])\n",
      "Shape of Tensor:  torch.Size([29, 100])\n"
     ]
    }
   ],
   "source": [
    "tensor_sum = torch.sum(product, \n",
    "                       dim = 1)\n",
    "print('Sum of Tensor across 1st Dimension: \\n' , tensor_sum)\n",
    "\n",
    "print('Shape of Tensor: ' , tensor_sum.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create a new long tensor of size  (3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Tensor: \n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Updated Long Tensor: \n",
      " tensor([[2, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 4, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 6, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "long_tensor = torch.ones((3, 10), \n",
    "                         dtype = torch.long)\n",
    "print('Long Tensor: \\n' , long_tensor)\n",
    "\n",
    "long_tensor[0, 0] = 2\n",
    "long_tensor[1, 2] = 4\n",
    "long_tensor[2, 4] = 6\n",
    "\n",
    "print('Updated Long Tensor: \\n' , long_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Use this new long tensor to index into the tensor from step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed Tensor: \n",
      " tensor([[[[ -9.8683, -12.2725,  -0.3319,  ...,  -3.9293,   0.1924,  -5.8769],\n",
      "          [ -2.4308,   1.0324,  -5.1080,  ...,  -5.3305,  -6.8257,   0.3756],\n",
      "          [ -8.4826,  -1.3951,  10.1890,  ...,  -8.8583,   4.0310,  -3.9838],\n",
      "          ...,\n",
      "          [  1.0088,   1.5307,  -4.8676,  ...,  -1.0770,   3.1684,  -2.8112],\n",
      "          [  7.2108,   3.6587,   1.0987,  ...,  -4.0202,   2.4188,  11.0244],\n",
      "          [  6.7456,   0.7441,  -4.3096,  ...,  -5.4091,  -0.9578,  -5.2660]],\n",
      "\n",
      "         [[ -4.5026,  -2.4407,   7.5931,  ...,  -5.0318,  -0.3388,   0.2325],\n",
      "          [ 14.8979,   5.7587,  -6.2131,  ...,  11.3459,   1.8926,   6.5630],\n",
      "          [ -3.9810,  -3.0537,  12.3846,  ...,  -8.5466,  -1.7133,  -8.8943],\n",
      "          ...,\n",
      "          [  4.0924,   4.2022,   0.1867,  ...,   2.7929,  -1.0690,   9.1906],\n",
      "          [ -3.1723,   0.1007,  -0.5834,  ...,   4.3986,   5.0610,  -1.0318],\n",
      "          [ -4.3487,   0.7072,   4.0640,  ...,  -5.1994,  -1.9691,   1.4542]],\n",
      "\n",
      "         [[ -4.5026,  -2.4407,   7.5931,  ...,  -5.0318,  -0.3388,   0.2325],\n",
      "          [ 14.8979,   5.7587,  -6.2131,  ...,  11.3459,   1.8926,   6.5630],\n",
      "          [ -3.9810,  -3.0537,  12.3846,  ...,  -8.5466,  -1.7133,  -8.8943],\n",
      "          ...,\n",
      "          [  4.0924,   4.2022,   0.1867,  ...,   2.7929,  -1.0690,   9.1906],\n",
      "          [ -3.1723,   0.1007,  -0.5834,  ...,   4.3986,   5.0610,  -1.0318],\n",
      "          [ -4.3487,   0.7072,   4.0640,  ...,  -5.1994,  -1.9691,   1.4542]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ -4.5026,  -2.4407,   7.5931,  ...,  -5.0318,  -0.3388,   0.2325],\n",
      "          [ 14.8979,   5.7587,  -6.2131,  ...,  11.3459,   1.8926,   6.5630],\n",
      "          [ -3.9810,  -3.0537,  12.3846,  ...,  -8.5466,  -1.7133,  -8.8943],\n",
      "          ...,\n",
      "          [  4.0924,   4.2022,   0.1867,  ...,   2.7929,  -1.0690,   9.1906],\n",
      "          [ -3.1723,   0.1007,  -0.5834,  ...,   4.3986,   5.0610,  -1.0318],\n",
      "          [ -4.3487,   0.7072,   4.0640,  ...,  -5.1994,  -1.9691,   1.4542]],\n",
      "\n",
      "         [[ -4.5026,  -2.4407,   7.5931,  ...,  -5.0318,  -0.3388,   0.2325],\n",
      "          [ 14.8979,   5.7587,  -6.2131,  ...,  11.3459,   1.8926,   6.5630],\n",
      "          [ -3.9810,  -3.0537,  12.3846,  ...,  -8.5466,  -1.7133,  -8.8943],\n",
      "          ...,\n",
      "          [  4.0924,   4.2022,   0.1867,  ...,   2.7929,  -1.0690,   9.1906],\n",
      "          [ -3.1723,   0.1007,  -0.5834,  ...,   4.3986,   5.0610,  -1.0318],\n",
      "          [ -4.3487,   0.7072,   4.0640,  ...,  -5.1994,  -1.9691,   1.4542]],\n",
      "\n",
      "         [[ -4.5026,  -2.4407,   7.5931,  ...,  -5.0318,  -0.3388,   0.2325],\n",
      "          [ 14.8979,   5.7587,  -6.2131,  ...,  11.3459,   1.8926,   6.5630],\n",
      "          [ -3.9810,  -3.0537,  12.3846,  ...,  -8.5466,  -1.7133,  -8.8943],\n",
      "          ...,\n",
      "          [  4.0924,   4.2022,   0.1867,  ...,   2.7929,  -1.0690,   9.1906],\n",
      "          [ -3.1723,   0.1007,  -0.5834,  ...,   4.3986,   5.0610,  -1.0318],\n",
      "          [ -4.3487,   0.7072,   4.0640,  ...,  -5.1994,  -1.9691,   1.4542]]],\n",
      "\n",
      "\n",
      "        [[[ -4.5026,  -2.4407,   7.5931,  ...,  -5.0318,  -0.3388,   0.2325],\n",
      "          [ 14.8979,   5.7587,  -6.2131,  ...,  11.3459,   1.8926,   6.5630],\n",
      "          [ -3.9810,  -3.0537,  12.3846,  ...,  -8.5466,  -1.7133,  -8.8943],\n",
      "          ...,\n",
      "          [  4.0924,   4.2022,   0.1867,  ...,   2.7929,  -1.0690,   9.1906],\n",
      "          [ -3.1723,   0.1007,  -0.5834,  ...,   4.3986,   5.0610,  -1.0318],\n",
      "          [ -4.3487,   0.7072,   4.0640,  ...,  -5.1994,  -1.9691,   1.4542]],\n",
      "\n",
      "         [[ -4.5026,  -2.4407,   7.5931,  ...,  -5.0318,  -0.3388,   0.2325],\n",
      "          [ 14.8979,   5.7587,  -6.2131,  ...,  11.3459,   1.8926,   6.5630],\n",
      "          [ -3.9810,  -3.0537,  12.3846,  ...,  -8.5466,  -1.7133,  -8.8943],\n",
      "          ...,\n",
      "          [  4.0924,   4.2022,   0.1867,  ...,   2.7929,  -1.0690,   9.1906],\n",
      "          [ -3.1723,   0.1007,  -0.5834,  ...,   4.3986,   5.0610,  -1.0318],\n",
      "          [ -4.3487,   0.7072,   4.0640,  ...,  -5.1994,  -1.9691,   1.4542]],\n",
      "\n",
      "         [[ -3.5176,   3.5252,   5.8268,  ...,  -7.9886,   3.8127,  -7.6489],\n",
      "          [  2.2206,   3.6773,   2.7971,  ...,   0.7680,   2.2119,  -0.4053],\n",
      "          [ -4.7544,   4.1859,  -4.0969,  ...,  -4.5882,  13.9522,  -4.5911],\n",
      "          ...,\n",
      "          [  2.9914,   4.9546,  -0.1424,  ...,   6.1892,   1.2886,  -4.3132],\n",
      "          [ -0.0477,  -1.5909,   0.0684,  ...,  -0.8045,  -1.4614,  -2.6709],\n",
      "          [  1.6169,  -2.3655,  -4.1445,  ...,  -1.5364,   2.5530,   1.2840]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ -4.5026,  -2.4407,   7.5931,  ...,  -5.0318,  -0.3388,   0.2325],\n",
      "          [ 14.8979,   5.7587,  -6.2131,  ...,  11.3459,   1.8926,   6.5630],\n",
      "          [ -3.9810,  -3.0537,  12.3846,  ...,  -8.5466,  -1.7133,  -8.8943],\n",
      "          ...,\n",
      "          [  4.0924,   4.2022,   0.1867,  ...,   2.7929,  -1.0690,   9.1906],\n",
      "          [ -3.1723,   0.1007,  -0.5834,  ...,   4.3986,   5.0610,  -1.0318],\n",
      "          [ -4.3487,   0.7072,   4.0640,  ...,  -5.1994,  -1.9691,   1.4542]],\n",
      "\n",
      "         [[ -4.5026,  -2.4407,   7.5931,  ...,  -5.0318,  -0.3388,   0.2325],\n",
      "          [ 14.8979,   5.7587,  -6.2131,  ...,  11.3459,   1.8926,   6.5630],\n",
      "          [ -3.9810,  -3.0537,  12.3846,  ...,  -8.5466,  -1.7133,  -8.8943],\n",
      "          ...,\n",
      "          [  4.0924,   4.2022,   0.1867,  ...,   2.7929,  -1.0690,   9.1906],\n",
      "          [ -3.1723,   0.1007,  -0.5834,  ...,   4.3986,   5.0610,  -1.0318],\n",
      "          [ -4.3487,   0.7072,   4.0640,  ...,  -5.1994,  -1.9691,   1.4542]],\n",
      "\n",
      "         [[ -4.5026,  -2.4407,   7.5931,  ...,  -5.0318,  -0.3388,   0.2325],\n",
      "          [ 14.8979,   5.7587,  -6.2131,  ...,  11.3459,   1.8926,   6.5630],\n",
      "          [ -3.9810,  -3.0537,  12.3846,  ...,  -8.5466,  -1.7133,  -8.8943],\n",
      "          ...,\n",
      "          [  4.0924,   4.2022,   0.1867,  ...,   2.7929,  -1.0690,   9.1906],\n",
      "          [ -3.1723,   0.1007,  -0.5834,  ...,   4.3986,   5.0610,  -1.0318],\n",
      "          [ -4.3487,   0.7072,   4.0640,  ...,  -5.1994,  -1.9691,   1.4542]]],\n",
      "\n",
      "\n",
      "        [[[ -4.5026,  -2.4407,   7.5931,  ...,  -5.0318,  -0.3388,   0.2325],\n",
      "          [ 14.8979,   5.7587,  -6.2131,  ...,  11.3459,   1.8926,   6.5630],\n",
      "          [ -3.9810,  -3.0537,  12.3846,  ...,  -8.5466,  -1.7133,  -8.8943],\n",
      "          ...,\n",
      "          [  4.0924,   4.2022,   0.1867,  ...,   2.7929,  -1.0690,   9.1906],\n",
      "          [ -3.1723,   0.1007,  -0.5834,  ...,   4.3986,   5.0610,  -1.0318],\n",
      "          [ -4.3487,   0.7072,   4.0640,  ...,  -5.1994,  -1.9691,   1.4542]],\n",
      "\n",
      "         [[ -4.5026,  -2.4407,   7.5931,  ...,  -5.0318,  -0.3388,   0.2325],\n",
      "          [ 14.8979,   5.7587,  -6.2131,  ...,  11.3459,   1.8926,   6.5630],\n",
      "          [ -3.9810,  -3.0537,  12.3846,  ...,  -8.5466,  -1.7133,  -8.8943],\n",
      "          ...,\n",
      "          [  4.0924,   4.2022,   0.1867,  ...,   2.7929,  -1.0690,   9.1906],\n",
      "          [ -3.1723,   0.1007,  -0.5834,  ...,   4.3986,   5.0610,  -1.0318],\n",
      "          [ -4.3487,   0.7072,   4.0640,  ...,  -5.1994,  -1.9691,   1.4542]],\n",
      "\n",
      "         [[ -4.5026,  -2.4407,   7.5931,  ...,  -5.0318,  -0.3388,   0.2325],\n",
      "          [ 14.8979,   5.7587,  -6.2131,  ...,  11.3459,   1.8926,   6.5630],\n",
      "          [ -3.9810,  -3.0537,  12.3846,  ...,  -8.5466,  -1.7133,  -8.8943],\n",
      "          ...,\n",
      "          [  4.0924,   4.2022,   0.1867,  ...,   2.7929,  -1.0690,   9.1906],\n",
      "          [ -3.1723,   0.1007,  -0.5834,  ...,   4.3986,   5.0610,  -1.0318],\n",
      "          [ -4.3487,   0.7072,   4.0640,  ...,  -5.1994,  -1.9691,   1.4542]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ -4.5026,  -2.4407,   7.5931,  ...,  -5.0318,  -0.3388,   0.2325],\n",
      "          [ 14.8979,   5.7587,  -6.2131,  ...,  11.3459,   1.8926,   6.5630],\n",
      "          [ -3.9810,  -3.0537,  12.3846,  ...,  -8.5466,  -1.7133,  -8.8943],\n",
      "          ...,\n",
      "          [  4.0924,   4.2022,   0.1867,  ...,   2.7929,  -1.0690,   9.1906],\n",
      "          [ -3.1723,   0.1007,  -0.5834,  ...,   4.3986,   5.0610,  -1.0318],\n",
      "          [ -4.3487,   0.7072,   4.0640,  ...,  -5.1994,  -1.9691,   1.4542]],\n",
      "\n",
      "         [[ -4.5026,  -2.4407,   7.5931,  ...,  -5.0318,  -0.3388,   0.2325],\n",
      "          [ 14.8979,   5.7587,  -6.2131,  ...,  11.3459,   1.8926,   6.5630],\n",
      "          [ -3.9810,  -3.0537,  12.3846,  ...,  -8.5466,  -1.7133,  -8.8943],\n",
      "          ...,\n",
      "          [  4.0924,   4.2022,   0.1867,  ...,   2.7929,  -1.0690,   9.1906],\n",
      "          [ -3.1723,   0.1007,  -0.5834,  ...,   4.3986,   5.0610,  -1.0318],\n",
      "          [ -4.3487,   0.7072,   4.0640,  ...,  -5.1994,  -1.9691,   1.4542]],\n",
      "\n",
      "         [[ -4.5026,  -2.4407,   7.5931,  ...,  -5.0318,  -0.3388,   0.2325],\n",
      "          [ 14.8979,   5.7587,  -6.2131,  ...,  11.3459,   1.8926,   6.5630],\n",
      "          [ -3.9810,  -3.0537,  12.3846,  ...,  -8.5466,  -1.7133,  -8.8943],\n",
      "          ...,\n",
      "          [  4.0924,   4.2022,   0.1867,  ...,   2.7929,  -1.0690,   9.1906],\n",
      "          [ -3.1723,   0.1007,  -0.5834,  ...,   4.3986,   5.0610,  -1.0318],\n",
      "          [ -4.3487,   0.7072,   4.0640,  ...,  -5.1994,  -1.9691,   1.4542]]]])\n",
      "Shape of Tensor:  torch.Size([3, 10, 30, 100])\n"
     ]
    }
   ],
   "source": [
    "indexed_tensor = product[long_tensor]\n",
    "print('Indexed Tensor: \\n' , indexed_tensor)\n",
    "\n",
    "print('Shape of Tensor: ' , indexed_tensor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Use  torch.mean  to average across the last dimension in the tensor from step 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2624,  0.0920,  0.1998, -0.2572,  0.9898,  0.0511, -1.1804,\n",
      "          -0.1364, -0.0410, -0.4430,  0.3890, -0.1362, -0.3172, -0.6801,\n",
      "          -0.4552, -0.6581, -0.0715,  0.5099, -0.2592,  0.3645,  0.7615,\n",
      "           1.1511, -0.0833, -1.5810, -0.9481,  0.5141,  0.6794,  0.8316,\n",
      "           0.8277, -0.5746],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617]],\n",
      "\n",
      "        [[-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [ 0.8029, -0.4275,  0.7714,  0.8611, -0.2373, -1.1295, -0.8116,\n",
      "           0.3322,  0.5426,  0.2298,  0.6126, -0.0964,  0.0046,  0.8432,\n",
      "          -0.7496, -0.2534,  0.7591,  0.7683, -0.2090, -0.1525, -0.1817,\n",
      "          -0.0642, -0.2668,  0.6300, -0.6534, -0.6226,  0.3072, -0.8407,\n",
      "           0.3364,  0.8749],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617]],\n",
      "\n",
      "        [[-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [ 0.2816, -1.5698, -0.4147, -0.8703, -0.1686, -0.8328, -0.0914,\n",
      "           0.3477, -0.7395,  0.0654,  0.6852, -0.1203, -1.0406,  0.0664,\n",
      "          -0.1735, -0.3461, -0.1561, -0.4224,  0.0066, -0.1224, -0.7891,\n",
      "          -0.5740,  0.8439,  0.0503,  0.8396,  0.3229,  0.7196,  0.5155,\n",
      "           0.3770,  0.4233],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617],\n",
      "         [-0.9399, -0.5606,  0.3212,  0.7499,  0.6357,  0.1310,  1.4811,\n",
      "           0.0710, -0.0766, -0.5081, -0.3940,  0.1131,  0.7691, -1.2880,\n",
      "          -0.3516,  0.3515,  0.0965, -0.2948,  0.0537,  0.5397, -0.6451,\n",
      "          -0.9095, -0.8117,  0.1011,  0.4697, -0.4335, -0.4338,  0.6531,\n",
      "           0.6008,  0.1617]]])\n",
      "Shape of Tensor:  torch.Size([3, 10, 30])\n"
     ]
    }
   ],
   "source": [
    "mean_tensor = torch.mean(indexed_tensor, \n",
    "                         dim = -1)\n",
    "print(mean_tensor)\n",
    "\n",
    "print('Shape of Tensor: ' , mean_tensor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Redo step 2. on the GPU and compare results from step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product of tensor_a and tensor_b on GPU: \n",
      " tensor([[[  3.0082,   0.2774,   0.2860,  ...,   4.7896,  -4.8246,  -5.3570],\n",
      "         [ -6.2760,  -0.9145,  -2.4638,  ...,  -0.6401,  -8.7860,   8.2480],\n",
      "         [  4.8915,   3.4413,  16.0579,  ...,  -0.3568,  -9.6738,   5.6802],\n",
      "         ...,\n",
      "         [  7.1380,   4.3842,   3.6447,  ...,  -4.5751,  -2.8540,  -5.3435],\n",
      "         [  8.4457,   4.1616,   4.5761,  ...,  10.2699,  -0.7791,  -0.3597],\n",
      "         [ -3.4007,  11.1838,  -3.4072,  ...,   1.9385,  -0.8124,   3.1373]],\n",
      "\n",
      "        [[ -4.5026,  -2.4407,   7.5931,  ...,  -5.0318,  -0.3388,   0.2325],\n",
      "         [ 14.8979,   5.7587,  -6.2131,  ...,  11.3459,   1.8926,   6.5630],\n",
      "         [ -3.9810,  -3.0537,  12.3846,  ...,  -8.5466,  -1.7133,  -8.8943],\n",
      "         ...,\n",
      "         [  4.0924,   4.2022,   0.1867,  ...,   2.7929,  -1.0690,   9.1906],\n",
      "         [ -3.1723,   0.1007,  -0.5834,  ...,   4.3986,   5.0610,  -1.0318],\n",
      "         [ -4.3487,   0.7072,   4.0640,  ...,  -5.1994,  -1.9691,   1.4542]],\n",
      "\n",
      "        [[ -9.8683, -12.2725,  -0.3319,  ...,  -3.9293,   0.1924,  -5.8769],\n",
      "         [ -2.4308,   1.0324,  -5.1080,  ...,  -5.3305,  -6.8257,   0.3756],\n",
      "         [ -8.4826,  -1.3951,  10.1890,  ...,  -8.8583,   4.0310,  -3.9838],\n",
      "         ...,\n",
      "         [  1.0088,   1.5307,  -4.8676,  ...,  -1.0770,   3.1684,  -2.8112],\n",
      "         [  7.2108,   3.6587,   1.0987,  ...,  -4.0202,   2.4188,  11.0244],\n",
      "         [  6.7456,   0.7441,  -4.3096,  ...,  -5.4091,  -0.9578,  -5.2660]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 12.0306,   5.6884,  -3.4003,  ...,   3.4182,   2.7516,  -5.8331],\n",
      "         [  2.0178,   8.7115,  -5.6349,  ...,  -6.3676,  -2.5997,   5.6853],\n",
      "         [ 12.2046,   2.9315,  -5.4668,  ...,   2.1545,   9.3212,  -0.7755],\n",
      "         ...,\n",
      "         [ -3.9099,   8.8735,   9.6865,  ...,  -7.2345,   2.9411,  -1.3267],\n",
      "         [  4.0376,  -0.1899,  -3.1151,  ...,  -0.6456,   1.8689,   0.7360],\n",
      "         [ -3.0743,   2.2886,  11.6498,  ...,  -4.7197,   1.9555,  -0.0771]],\n",
      "\n",
      "        [[ -3.0832,   2.9233,  -1.5363,  ...,   8.0021,  -1.4690,  -0.3125],\n",
      "         [  5.5819,  -0.8205,   7.4464,  ...,  -7.0727,   5.1118,  -9.6153],\n",
      "         [  7.5457,   1.8199,  -3.0314,  ...,  -1.4455,  -0.2121,  -2.4782],\n",
      "         ...,\n",
      "         [ -6.9256, -10.6333,  -0.7512,  ...,  -1.0363,  10.3745,   2.4319],\n",
      "         [ -1.7239,  -2.7712,  -4.7344,  ...,   7.1690,   3.3304,  -3.4514],\n",
      "         [ -5.0591,  -3.5502,   4.9229,  ...,  -0.5592,  -6.3918,   7.3881]],\n",
      "\n",
      "        [[  7.8078,   5.0535,  -1.7631,  ...,  -5.2606,  -2.7049,  -7.5602],\n",
      "         [ -3.5394,  -1.3881,   7.2511,  ..., -11.9620,  -0.5190,  -6.7947],\n",
      "         [ -2.6846,  -2.4491,  -8.8204,  ...,  -2.3635,   2.2110,  -2.2485],\n",
      "         ...,\n",
      "         [  2.0544,  -0.3922,  10.7518,  ...,   4.3098,   3.9669,  -8.2762],\n",
      "         [  4.3249,   6.3635,   2.6976,  ...,  -1.9902,  -5.2601,   0.8030],\n",
      "         [ -2.0803,   6.7196,   7.0118,  ...,  -5.2563,  -8.2437,   2.0623]]])\n",
      "Shape of Tensor:  torch.Size([29, 30, 100])\n"
     ]
    }
   ],
   "source": [
    "tensor_a_cuda = tensor_a.to(device = device)\n",
    "tensor_b_cuda = tensor_b.to(device = device)\n",
    "product_gpu = torch.matmul(tensor_a, \n",
    "                           tensor_b)\n",
    "print('Product of tensor_a and tensor_b on GPU: \\n' , product_gpu)\n",
    "\n",
    "print('Shape of Tensor: ' , product_gpu.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Write a pure PyTorch program to compute the value of $\\sqrt{2}$ up to 4 decimal places without using the square root or other math functions from any of the libraries. \n",
    "### Hint: Notice that the answer is the (positive) root of the equation, $$𝑥^2 −2 = 0$$ \n",
    "### To find the root, you might want to use 'Newton's Method': $$𝑥_{𝑛+1} = 𝑥_{𝑛} − \\frac{𝑓(𝑥)}{𝑓′(𝑥)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4142\n"
     ]
    }
   ],
   "source": [
    "# Function f(x)\n",
    "def f(x):\n",
    "    return x**2 - 2\n",
    "\n",
    "# Derivative of f(x)\n",
    "def f_prime(x):\n",
    "    return 2*x\n",
    "\n",
    "# Initial Guess = 1.0\n",
    "x = torch.tensor([1.0], \n",
    "                 requires_grad = True)\n",
    "\n",
    "# Number of Iterations\n",
    "n = 10\n",
    "\n",
    "# Newton's Method\n",
    "for i in range(n):\n",
    "    x.data = x - f(x) / f_prime(x)\n",
    "\n",
    "# Print the final approximation to 4 decimal places\n",
    "print(round(x.item(), 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fail-fast prototyping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building neural networks, you want things to either work or fail fast. Long iteration loops are \n",
    "the worst enemy of a machine learning practitioner. \\\n",
    "For e.g., while writing code, you might want to incrementally test your code by doing something \n",
    "like this:\n",
    "\n",
    "batch_size = 32 \\\n",
    "num_features = 512 \\\n",
    "embedding_size = 16\n",
    "\n",
    "\\# construct a dummy input \\\n",
    "x = torch.randn(batch_size, num_features)\n",
    "\n",
    "\\# we want to project the input to embedding_size \\\n",
    "fc = torch.nn.Linear(num_features, embedding_size)\n",
    "\n",
    "\\# test if that works \\\n",
    "print(fc(x).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fail-fast exercises"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. [Glove](https://nlp.stanford.edu/projects/glove/) has 300 dimension embeddings. Design an nn.Module that takes a sentence of max_len words, tokenizes words by spaces, represents the sentence by averaging the glove embeddings of constituent words. What is the shape of the resulting sentence embedding? When you implement this, you will need to make some assumptions. What are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2196017 words present in GloVe\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe Embeddings\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "GLOVE_DIM = 300\n",
    "glove = GloVe(name = '840B', \n",
    "              dim = GLOVE_DIM)\n",
    "\n",
    "print(f'Loaded {len(glove.itos)} words present in GloVe')\n",
    "\n",
    "embeddings_tensor = glove.vectors\n",
    "embeddings_tensor = embeddings_tensor.to(device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "# Generate 512 sentences\n",
    "NUM_SENT = 512\n",
    "sents = list()\n",
    "for i in range(NUM_SENT):\n",
    "    sents.append('This is the quest zero and it has a deadline this Sunday March 29')\n",
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Resulting Sentence: torch.Size([1, 300])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GloveEmbeddingAvg(nn.Module):\n",
    "    \n",
    "    def __init__(self, max_len):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings_tensor)\n",
    "        \n",
    "    def forward(self, \n",
    "                sent):\n",
    "        # Tokenize the sentence by spaces\n",
    "        tokens = sent.split(' ')[:self.max_len]\n",
    "        # Get idx of each token from the GloVe dictionary\n",
    "        glove_dict_indexes = [glove.stoi[token] for token in tokens]\n",
    "        # Convert it into Tensor\n",
    "        glove_dict_indexes = torch.tensor(glove_dict_indexes, \n",
    "                                          device = device)\n",
    "        # Get Word Embeddings for all tokens\n",
    "        word_embeds = self.embedding(glove_dict_indexes) # [MAX_LEN, GLOVE_DIM]\n",
    "        # Sentence Embedding = Average of Word Embeddings\n",
    "        sent_embeds = word_embeds.mean(dim = 0) # [GLOVE_DIM]\n",
    "        # Reshape Sentence Embedding as a 2D Tensor\n",
    "        return sent_embeds.view(1, -1) # [1, GLOVE_DIM]\n",
    "\n",
    "MAX_LEN = 10\n",
    "glove_embeds_avg = GloveEmbeddingAvg(MAX_LEN).to(device)\n",
    "\n",
    "print(f'Shape of Resulting Sentence: {glove_embeds_avg(sents[0]).shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions:\n",
    "1. The forward function tokenizes the input sentence by splitting it on spaces and only considers words that are in the GloVe vocab. \n",
    "2. GloVe vocab is in the form of a dictionary(key, value pairs).\n",
    "3. The words are represented by their average embeddings, i.e., equivalent to sentence embedding.\n",
    "4. The maximum length of the sentence is pre-determined and fixed. \n",
    "5. The input sentence is in the form of a string."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How will you modify step 1. so that the sentence embeddings are in $R^{50}$ ?\n",
    "BONUS: Can you think of more than one way to do this? What are the implications of each method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveEmbeddingAvg_50_Dim(nn.Module):\n",
    "    \n",
    "    def __init__(self, max_len):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings_tensor)\n",
    "        self.fc = nn.Linear(GLOVE_DIM, 50)\n",
    "        \n",
    "    def forward(self, \n",
    "                x):\n",
    "        # Slice each sentence to Max Length\n",
    "        x = x[:, :self.max_len]\n",
    "        # Get Word Embeddings for all tokens\n",
    "        word_embeds = self.embedding(x) # [BATCH_SIZE, MAX_LEN, GLOVE_DIM]\n",
    "        # Sentence Embedding = Average of Word Embeddings\n",
    "        sent_embeds = word_embeds.mean(dim = 0) # [MAX_LEN, GLOVE_DIM]\n",
    "        # Linear Layer to reduce Sentence Embedding Dimension to 50\n",
    "        return self.fc(sent_embeds) # [MAX_LEN, 50]\n",
    "\n",
    "MAX_LEN = 10\n",
    "glove_embeds_avg_50_dim = GloveEmbeddingAvg_50_Dim(MAX_LEN).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To modify the step 1. so that the sentence embeddings are in 50 dimensions:\n",
    "1. One way to do this is to use a linear layer with 50 output units after averaging the GloVe embeddings of the constituent words. \n",
    "2. Another way to do this would be to use PCA(Principal Component Analysis) to reduce the dimensionality of the averaged embeddings to 50.\n",
    "\n",
    "The implications of using a linear layer is that it is a simple and fast method but it may not capture all the information present in the original 300-dimensional embeddings.\\\n",
    "On the other hand, PCA is a more complex method but it captures the most important variations present in the original embeddings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Quickly test your answer in step 2. with a batch of 512 sentences on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 50])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a Sentence\n",
    "def tokenize(sent):\n",
    "    # Tokenize the sentence by spaces\n",
    "    tokens = sent.split(' ')\n",
    "    # Get idx of each token from the GloVe dictionary\n",
    "    glove_dict_indexes = [glove.stoi[token] for token in tokens]\n",
    "    return glove_dict_indexes\n",
    "\n",
    "# Create Tokenized Sentence Corpus\n",
    "tokenized_sents = list()\n",
    "for sent in sents:\n",
    "    tokenized_sents.append(tokenize(sent))\n",
    "tokenized_sents = torch.tensor(tokenized_sents, \n",
    "                               device = device)\n",
    "\n",
    "# Run forward pass\n",
    "BATCH_SIZE = 512\n",
    "for i in range(0, len(tokenized_sents), BATCH_SIZE):\n",
    "    batch = tokenized_sents[i:i+BATCH_SIZE]\n",
    "    sentence_embeddings = glove_embeds_avg_50_dim(batch)\n",
    "    print(sentence_embeddings.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You almost implemented the model in the Deep Averaging Networks (DAN) paper!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Task: \n",
    "### Create a   MultiEmbedding  Module that can take two sets of indices, embed them, and concat the results. You might remember it from the previous lecture where we had to produce an embedding for \"green apple\" from embeddings of \"green\" and \"apple\". Your  MultiEmbedding class should work with the following test code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_emb, \n",
    "                 size_emb1, \n",
    "                 size_emb2):\n",
    "        super().__init__()\n",
    "        self.embedding_A = nn.Embedding(num_emb, size_emb1)\n",
    "        self.embedding_B = nn.Embedding(num_emb, size_emb2)\n",
    "        \n",
    "    def forward(self, \n",
    "                indices1, \n",
    "                indices2):\n",
    "        embed_A = self.embedding_A(indices1)\n",
    "        embed_B = self.embedding_B(indices2)\n",
    "        # Concatenate the Embeddings\n",
    "        return torch.cat((embed_A, embed_B), \n",
    "                         dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10, 500])\n"
     ]
    }
   ],
   "source": [
    "# Test code: instantiate a MultiEmbedding with the sizes for each embedding. \n",
    "# For this example, you can just randomly initialize each interior embedding. \n",
    "# In a practical setting, you might support methods for initializing with \n",
    "# combinations of embeddings, such as GloVe 300d vectors and word2vec 200d \n",
    "# vectors, yielding 500d embeddings. Both embeddings share a vocabulary/range \n",
    "# of supported indices indicated by `num_emb`\n",
    "\n",
    "NUM_EMB = 10000\n",
    "SIZE_EMB1 = 300\n",
    "SIZE_EMB2 = 200\n",
    "BATCH_SIZE = 64\n",
    "NUM_LENGTH = 10\n",
    "\n",
    "multiemb = MultiEmbedding(NUM_EMB, \n",
    "                          SIZE_EMB1, \n",
    "                          SIZE_EMB2).to(device)\n",
    "\n",
    "# You can then call this with a pair of indices where each value is in 0 <= i < num_emb\n",
    "indices1 =  torch.randint(0, \n",
    "                          NUM_EMB, \n",
    "                          (BATCH_SIZE, NUM_LENGTH), \n",
    "                          dtype = torch.long, \n",
    "                          device = device) # long tensor of shape (batch, num_length)\n",
    "indices2 =  torch.randint(0, \n",
    "                          NUM_EMB, \n",
    "                          (BATCH_SIZE, NUM_LENGTH), \n",
    "                          dtype = torch.long, \n",
    "                          device = device) # long tensor of shape (batch, num_length)\n",
    "output = multiemb(indices1, \n",
    "                  indices2)\n",
    "print(output.shape) # should be (batch, num_length, size_emb1 + size_emb2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Datasets and DataLoaders: \n",
    "### Read this short post on PyTorch Dataset and DataLoaders. Often in prototyping we need to generate dummy datasets to test our models. Implement a PyTorch Dataset class that generates up to  num_sentences  random sentences of length up to  max_len words. For each sentence, generate a binary label. You should be able to test your code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAveragingNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, max_len):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings_tensor)\n",
    "        self.fc = nn.Linear(GLOVE_DIM, 50)\n",
    "        \n",
    "    def forward(self, \n",
    "                sent):\n",
    "        # Tokenize the sentence by spaces\n",
    "        tokens = sent.split(' ')[:self.max_len]\n",
    "        # Get idx of each token from the GloVe dictionary\n",
    "        glove_dict_indexes = [glove.stoi[token] for token in tokens]\n",
    "        # Convert it into Tensor\n",
    "        glove_dict_indexes = torch.tensor(glove_dict_indexes, \n",
    "                                          device = device)\n",
    "        # Get Word Embeddings for all tokens\n",
    "        word_embeds = self.embedding(glove_dict_indexes) # [MAX_LEN, GLOVE_DIM]\n",
    "        # Sentence Embedding = Average of Word Embeddings\n",
    "        sent_embeds = word_embeds.mean(dim = 0) # [GLOVE_DIM]\n",
    "        # Linear Layer to reduce Sentence Embedding Dimension to 50\n",
    "        return self.fc(sent_embeds.view(1, -1)) # [1, 50]\n",
    "\n",
    "model = DeepAveragingNetwork(MAX_LEN).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DummySentenceLabelDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A simple dataset that generates random sentences and labels for each sentence.\n",
    "    Args:\n",
    "        num_sentences (int): The number of sentences to generate.\n",
    "        max_len (int): The maximum length of each sentence.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_sentences, \n",
    "                 max_len):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with the given number of sentences and maximum sentence length.\n",
    "        Generates the sentences and labels using the generate_sents() and generate_labels() methods.\n",
    "        \"\"\"\n",
    "        self.num_sentences = num_sentences\n",
    "        self.max_len = max_len\n",
    "        self.sents = self.generate_sents()\n",
    "        self.labels = self.generate_labels()\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of sentences in the dataset.\n",
    "        \"\"\"\n",
    "        return self.num_sentences\n",
    "        \n",
    "    def __getitem__(self, \n",
    "                    idx):\n",
    "        \"\"\"\n",
    "        Returns the sentence and label at the given index.\n",
    "        \"\"\"\n",
    "        return self.sents[idx], self.labels[idx]\n",
    "    \n",
    "    def get_random_sent(self, word_list):\n",
    "        \"\"\"\n",
    "        Generates a random sentence using the given list of words.\n",
    "        Args:\n",
    "            word_list (list): A list of words to choose from when generating the sentence.\n",
    "        Returns:\n",
    "            str: A sentence composed of randomly chosen words from the given list.\n",
    "        \"\"\"\n",
    "        sent = ''\n",
    "        for i in range(self.max_len):\n",
    "            word = random.choice(word_list)\n",
    "            sent += word + ' '\n",
    "        return sent\n",
    "    \n",
    "    def generate_sents(self):\n",
    "        \"\"\"\n",
    "        Generates a list of random sentences.\n",
    "        Uses the get_random_sent() method to generate each sentence.\n",
    "        Returns:\n",
    "            list: A list of randomly generated sentences.\n",
    "        \"\"\"\n",
    "        word_list = ['Hello', 'World', 'Python', 'Function', 'Random', 'Sentence', \n",
    "                     'List', 'Words', 'Generates', '20', 'Example', 'Simple', 'Program', \n",
    "                     'Easy', 'Understand', 'Learn', 'Code', 'Implement', 'Execute', 'Run', \n",
    "                     'Brazil', 'India', 'Chat', 'India', 'Golden', 'State', 'Warriors']\n",
    "        sents = list()\n",
    "        for i in range(self.num_sentences):\n",
    "            sent = self.get_random_sent(word_list)\n",
    "            sents.append(sent)\n",
    "        return sents\n",
    "    \n",
    "    def generate_labels(self):\n",
    "        \"\"\"\n",
    "        Generates a list of random labels (0 or 1) for each sentence.\n",
    "        Returns:\n",
    "            list: A list of randomly generated labels.\n",
    "        \"\"\"\n",
    "        labels = list()\n",
    "        for i in range(self.num_sentences):\n",
    "            labels.append(random.randint(0, 1))\n",
    "        return labels\n",
    "\n",
    "NUM_SENTENCES = 10\n",
    "MAX_LEN = 20\n",
    "dataset = DummySentenceLabelDataset(num_sentences = NUM_SENTENCES, \n",
    "                                    max_len = MAX_LEN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's measure the error rate for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Generates 20 Program 20 Simple Understand Function Easy Sentence Sentence State Code Generates Run Python Learn Python Understand Run Words ,\n",
      "Label: 1\n",
      "Sentence: Warriors Understand Random Simple India Warriors Hello Code Brazil Sentence Function Easy Generates Easy Sentence Warriors Execute Hello Golden Chat ,\n",
      "Label: 0\n",
      "Sentence: Implement World India Hello Implement World Chat Simple Execute Generates Warriors World Simple Hello List List Example Random Simple Code ,\n",
      "Label: 1\n",
      "Sentence: List Words State State List India Generates World State Warriors Hello Function 20 Chat State Golden India India Golden Understand ,\n",
      "Label: 1\n",
      "Sentence: Hello Random Hello Code State Random India Hello Easy Words Python Code Learn Code Implement Example Run India Function Hello ,\n",
      "Label: 1\n",
      "Sentence: Brazil Chat Sentence Chat Run Run Function Golden Hello Chat Implement Warriors Execute Implement Learn Example Sentence Run Easy Easy ,\n",
      "Label: 0\n",
      "Sentence: Simple 20 Function Understand Simple Words Example Program Execute Easy Function Random 20 Random Implement Simple Brazil Hello Easy Brazil ,\n",
      "Label: 1\n",
      "Sentence: Hello Simple India India Golden India Learn India Program Hello Hello Golden Function Hello Simple Execute India India Random Code ,\n",
      "Label: 0\n",
      "Sentence: Example Example Simple Python Function Understand Chat Example Code Code Easy Code Sentence List Example Learn Words Hello State Brazil ,\n",
      "Label: 0\n",
      "Sentence: World Run Run Python Golden Easy Random Implement Implement Python Simple India Generates Simple Code Warriors Understand State Hello Golden ,\n",
      "Label: 0\n",
      "==================================================================\n",
      "Error rate: tensor([[0.4945, 0.7165, 0.5714, 0.6038, 0.5429, 0.5385, 0.5251, 0.5156, 0.5617,\n",
      "         0.5000, 0.5239, 0.7601, 0.5883, 0.5632, 0.7614, 0.6934, 0.5179, 0.5382,\n",
      "         0.6012, 0.5080, 0.5182, 0.5355, 0.6402, 0.5000, 0.6426, 0.4972, 0.5385,\n",
      "         0.5105, 0.5953, 0.4812, 0.7212, 0.4730, 0.5192, 0.5921, 0.6563, 0.5193,\n",
      "         0.5646, 0.4826, 0.5046, 0.7020, 0.4856, 0.5127, 0.5141, 0.5116, 0.5571,\n",
      "         0.4902, 0.5442, 0.5286, 0.5708, 0.6229]], device='mps:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "error = 0.0\n",
    "for sentence, label in dataset:\n",
    "  print(f'Sentence: {sentence},\\nLabel: {label}')\n",
    "  prediction = model(sentence)\n",
    "  error += abs(prediction - label)\n",
    "print('==================================================================')  \n",
    "print(f'Error rate: {error/len(dataset)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b37beb76676c8b0643f5764b5c5ae0ddf876ecbab29b433e279cae2d82963c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
