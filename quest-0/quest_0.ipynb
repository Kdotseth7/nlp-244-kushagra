{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "0.14.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "print(torch.__version__)\n",
    "print(torchtext.__version__)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set device as GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    dev = 'mps'\n",
    "else:\n",
    "    dev = 'cpu'\n",
    "device = torch.device(dev)    \n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch warmup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use torch.randn to create two tensors of size (29, 30, 32) and (32, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of Size (29, 30, 32): \n",
      " tensor([[[-1.2669e+00, -5.6959e-01, -1.7969e+00,  ...,  1.8188e+00,\n",
      "           9.7032e-01,  1.5106e+00],\n",
      "         [ 6.6902e-01, -4.5929e-02, -5.6820e-01,  ..., -3.7463e-02,\n",
      "          -8.2540e-01, -4.4991e-01],\n",
      "         [-7.3153e-01,  3.6122e-02,  9.2934e-01,  ...,  1.5369e+00,\n",
      "          -3.8756e-01, -1.2843e+00],\n",
      "         ...,\n",
      "         [ 1.0068e+00, -2.0352e+00, -1.0178e-01,  ...,  1.3100e+00,\n",
      "          -1.0394e+00,  2.6688e-01],\n",
      "         [ 2.2937e+00, -9.4478e-01,  9.5343e-01,  ...,  3.3338e-01,\n",
      "           1.0328e+00, -9.9259e-01],\n",
      "         [ 1.2945e+00,  2.1403e+00, -6.1383e-01,  ...,  7.5230e-01,\n",
      "           7.0326e-01,  3.5825e-01]],\n",
      "\n",
      "        [[-1.2154e+00, -1.6357e+00,  4.5557e-02,  ...,  1.7509e-01,\n",
      "           1.1140e+00, -4.1342e-01],\n",
      "         [-1.6959e-01,  7.1065e-01, -1.8802e+00,  ..., -1.2577e-01,\n",
      "          -5.9885e-01, -9.0522e-01],\n",
      "         [ 1.9752e-01, -4.7546e-01,  3.2681e-01,  ..., -5.9618e-01,\n",
      "          -1.2522e+00,  2.1489e-01],\n",
      "         ...,\n",
      "         [ 4.4328e-01,  1.6350e+00, -1.1680e+00,  ..., -3.6258e-01,\n",
      "          -2.1581e-03, -9.0492e-01],\n",
      "         [-8.0744e-02, -6.8820e-02, -5.4561e-01,  ...,  8.6700e-01,\n",
      "          -8.2507e-01,  9.7306e-01],\n",
      "         [ 2.3055e-01,  1.6687e-01,  9.0607e-02,  ..., -1.1617e+00,\n",
      "          -6.8536e-01,  2.3498e+00]],\n",
      "\n",
      "        [[ 5.8260e-01, -1.8366e+00, -1.8544e+00,  ..., -6.4979e-01,\n",
      "           4.9818e-04,  1.1419e+00],\n",
      "         [ 1.0948e-01, -1.1808e+00,  3.4918e-01,  ...,  1.0612e-01,\n",
      "          -1.7155e+00, -7.6530e-01],\n",
      "         [ 6.4591e-01, -8.9741e-02, -1.1147e+00,  ...,  1.3890e+00,\n",
      "           1.6094e-02, -1.1621e-01],\n",
      "         ...,\n",
      "         [-2.4948e+00,  2.8466e-01, -9.5072e-01,  ..., -5.2520e-01,\n",
      "          -6.8602e-01,  4.8500e-03],\n",
      "         [-1.4140e+00,  2.4996e-01,  6.2007e-01,  ..., -1.4285e+00,\n",
      "           4.8679e-01, -1.4375e+00],\n",
      "         [ 2.9714e-01, -7.7073e-01, -8.2971e-01,  ..., -6.2551e-01,\n",
      "           5.2720e-01,  8.7156e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.6451e+00,  3.2702e-01, -1.2829e+00,  ..., -4.5496e-01,\n",
      "          -1.6985e+00,  4.0987e-01],\n",
      "         [ 1.0943e-01,  1.0152e+00, -1.2322e+00,  ..., -1.2139e+00,\n",
      "           2.8013e-02,  1.0615e+00],\n",
      "         [-1.1468e+00,  7.1571e-01,  5.3016e-01,  ..., -5.3707e-01,\n",
      "          -9.4195e-01, -3.6108e-01],\n",
      "         ...,\n",
      "         [ 2.3800e+00, -2.2150e-01,  4.0454e-02,  ..., -3.1758e-02,\n",
      "           4.3526e-02, -1.5719e+00],\n",
      "         [ 3.3540e-01, -1.8762e+00,  2.4170e+00,  ...,  2.3836e+00,\n",
      "           1.5774e+00,  6.6241e-02],\n",
      "         [ 7.7642e-01,  9.2436e-02, -1.0086e+00,  ...,  1.9943e+00,\n",
      "          -1.1058e+00, -6.1793e-01]],\n",
      "\n",
      "        [[ 1.0863e+00, -1.2007e+00, -2.0622e-01,  ..., -5.6975e-01,\n",
      "          -1.0503e+00,  2.9315e-01],\n",
      "         [-1.1388e+00,  3.5256e-02, -1.6893e+00,  ...,  5.1443e-02,\n",
      "          -4.2778e-01, -6.1374e-01],\n",
      "         [ 2.8497e-01,  5.8803e-01, -9.2786e-02,  ...,  2.8092e-01,\n",
      "           1.4671e-02, -6.2091e-02],\n",
      "         ...,\n",
      "         [-9.2294e-01, -5.1285e-01,  1.2545e+00,  ...,  3.2897e-01,\n",
      "          -6.0599e-01, -5.9299e-01],\n",
      "         [ 3.8524e-01, -4.1677e-02,  1.8327e+00,  ..., -3.0269e-02,\n",
      "          -3.4341e-01,  7.8273e-01],\n",
      "         [ 2.4806e-01,  5.8937e-01,  5.7992e-01,  ..., -1.3365e+00,\n",
      "          -5.9887e-01, -1.1740e+00]],\n",
      "\n",
      "        [[-1.0096e+00, -1.7437e+00,  1.1182e+00,  ...,  1.6651e+00,\n",
      "          -2.9688e-01,  5.5041e-01],\n",
      "         [-9.0394e-01, -1.1727e+00, -7.2741e-01,  ..., -1.2636e+00,\n",
      "          -1.5657e+00,  1.3259e-01],\n",
      "         [ 4.8345e-01, -5.2021e-01,  1.3675e+00,  ...,  3.0236e-01,\n",
      "          -3.9575e-01,  7.0636e-01],\n",
      "         ...,\n",
      "         [ 7.8190e-01,  1.2814e+00, -5.8384e-01,  ..., -1.7986e+00,\n",
      "          -1.6438e-01,  5.1922e-01],\n",
      "         [-1.6111e+00, -1.7499e-01,  1.8021e+00,  ...,  1.6175e+00,\n",
      "          -1.3373e+00, -3.3339e-01],\n",
      "         [ 7.8267e-01, -4.1390e-01, -1.2134e+00,  ...,  7.5502e-01,\n",
      "           1.5106e+00, -4.2634e-01]]])\n",
      "Tensor of Size (32, 100): \n",
      " tensor([[ 0.2002, -0.5868, -0.3084,  ..., -0.1979, -1.6597, -0.4545],\n",
      "        [-0.0947, -1.6835, -0.5105,  ..., -0.8109, -0.5513, -0.5857],\n",
      "        [ 0.2444, -0.3112,  2.0493,  ..., -0.0750, -2.3925,  0.1044],\n",
      "        ...,\n",
      "        [-0.7312, -0.0408,  1.7846,  ..., -0.5983, -1.8071, -1.1416],\n",
      "        [-1.6394, -0.2307, -1.7622,  ...,  0.9503,  0.8702,  0.1461],\n",
      "        [-1.8573,  2.0542, -0.3147,  ..., -1.9244,  1.1356, -2.1025]])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.randn(29, 30, 32)\n",
    "print('Tensor of Size (29, 30, 32): \\n' , tensor_a)\n",
    "\n",
    "tensor_b = torch.randn(32, 100)\n",
    "print('Tensor of Size (32, 100): \\n' , tensor_b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use  torch.matmul  to matrix multiply the two tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product of tensor_a and tensor_b: \n",
      " tensor([[[-13.2621,   9.3731, -11.7893,  ...,  -7.3152,  11.6473,  -6.2425],\n",
      "         [  1.6365,  -3.0511,   2.1437,  ...,   1.7488,  -6.4826,  -3.2745],\n",
      "         [ -5.7323,   2.4026,   5.0012,  ...,   2.1264,   1.2310,   7.0712],\n",
      "         ...,\n",
      "         [  1.7914,  -0.2842,  -0.6901,  ...,   7.1259,  -8.9405,  -0.2058],\n",
      "         [ -7.0371,   2.5656,   5.3010,  ...,   2.4419, -14.7984,   0.6097],\n",
      "         [ -6.4793,  -5.5452,   6.9085,  ...,  -0.6199,  -7.2145, -11.2881]],\n",
      "\n",
      "        [[  1.3306,   3.9205,   0.2360,  ...,   6.2160,   4.7269,   2.5456],\n",
      "         [ -2.1774,  -3.2473,  -2.5274,  ...,   3.5847,  -2.9077,   2.0045],\n",
      "         [  3.4768,   1.0987,  -4.0413,  ...,   2.4075,  -7.9049,  -3.2112],\n",
      "         ...,\n",
      "         [  2.3400,  -6.5402,  -4.7628,  ...,   8.0365,  -5.0185,   2.6872],\n",
      "         [  7.7205,  -5.6621,  -4.2838,  ...,  -2.6443,  -5.1610,   4.8454],\n",
      "         [ -3.1295,   5.9888,   2.1850,  ...,  -1.1024,   4.8072,  -3.6004]],\n",
      "\n",
      "        [[ -7.4345,  14.9439,  -2.7215,  ...,  -0.5436,  14.2772,   5.2867],\n",
      "         [ -7.5322,   2.3207,   2.3941,  ...,   3.0348,  -5.0813,   2.7684],\n",
      "         [ -6.3394,  -1.6578, -15.2204,  ...,  -2.6148,  -5.7177,  -5.8000],\n",
      "         ...,\n",
      "         [  0.3471,   0.5292,  -0.0286,  ...,  -2.2514,  10.0249,  -1.1148],\n",
      "         [ -1.6198,   1.6737,  -5.8284,  ...,   5.0135,   5.6666,   5.0532],\n",
      "         [ -4.8812,   3.6147,  -4.2578,  ...,  -1.2021,   6.5086,  -3.4150]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -2.7385,   4.9662,  -9.2138,  ...,  -3.9418,   9.0622,   5.7966],\n",
      "         [  0.2036,  -2.6212,   9.1101,  ...,   2.3419,   0.1791,  -3.0513],\n",
      "         [  2.2207,   6.2934,  -3.2702,  ...,  -4.9527,   1.3526,   1.8392],\n",
      "         ...,\n",
      "         [ -2.9905,  -2.7910,  -0.6549,  ...,   1.0246,  -2.0625,   4.3803],\n",
      "         [  3.4376,  -1.1878,   8.1888,  ...,  -7.7703, -13.7241,  -4.7959],\n",
      "         [  2.5718,   1.2749,   1.5723,  ...,   2.4818,  -7.6246,  -1.5750]],\n",
      "\n",
      "        [[ -3.4492,   8.9613,   8.8209,  ...,   0.3223,  -4.2518,  -0.8447],\n",
      "         [  2.6030,   1.3937,  -3.2947,  ...,  -2.0370,   0.7166,  -1.9964],\n",
      "         [  1.3181,   0.9509,  -0.7915,  ...,  -5.9073,   1.7024,   0.3803],\n",
      "         ...,\n",
      "         [ -0.9745,   9.3337,  -3.9029,  ...,  -0.6372,  -8.6131,   4.0391],\n",
      "         [ -9.5980,   9.0772,   3.4929,  ...,  -4.9908,   2.9416,  -2.1731],\n",
      "         [  5.7465,  11.4815,   1.3964,  ...,   1.5567,   1.8218,  -2.8327]],\n",
      "\n",
      "        [[  9.5255,  -3.4434,  -1.5909,  ...,   1.7131,   4.0838,   1.2086],\n",
      "         [ -6.5496,  -0.7251,  -2.5107,  ...,   2.6548,   0.7187,   2.6837],\n",
      "         [ -8.2246,  -4.2586,  -0.1263,  ...,  -1.6266,  -8.5336,  -0.6474],\n",
      "         ...,\n",
      "         [  1.2935,  -3.2013,  -2.8900,  ...,  -6.9646,   1.5004,  -0.2604],\n",
      "         [ -4.0238,  -3.3850,  -0.6968,  ...,  -2.2987,   2.7812,   7.5491],\n",
      "         [-14.1996,   1.4640,  -9.2722,  ...,   0.8289,  -5.4982,   8.6846]]])\n",
      "Shape of Tensor:  torch.Size([29, 30, 100])\n"
     ]
    }
   ],
   "source": [
    "product = torch.matmul(tensor_a, \n",
    "                       tensor_b)\n",
    "print('Product of tensor_a and tensor_b: \\n' , product)\n",
    "\n",
    "print('Shape of Tensor: ' , product.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What is the difference between torch.matmul , torch.mm , torch.bmm , and torch.einsum , and the @ operator?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. torch.matmul() ->\n",
    "2. torch.mm() ->\n",
    "3. torch.bmm() ->\n",
    "4. torch.einsum() ->\n",
    "5. @ operator -> It is a shorthand for the torch.matmul() function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use torch.sum on the resulting tensor, passing the optional argument of dim=1 to sum across the 1st dimension. Before you run this, can you predict the size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Tensor across 1st Dimension: \n",
      " tensor([[   4.2655,  -76.7089,  -49.9342,  ...,   14.1479,  -32.9342,\n",
      "           -1.1706],\n",
      "        [  28.3876,    1.3230,   10.2586,  ...,   34.6930,   12.5124,\n",
      "           41.9824],\n",
      "        [ -91.3736,   67.1420, -106.7556,  ...,  -32.7357,   62.9556,\n",
      "          -32.1673],\n",
      "        ...,\n",
      "        [ -15.1692,   33.4527,   27.5558,  ...,  -13.5718,   -8.8082,\n",
      "          -24.9549],\n",
      "        [  22.2288,   38.7153,  -16.8282,  ...,  -21.3191,    2.6734,\n",
      "           -9.2732],\n",
      "        [ -15.7508,   28.8513,   16.5797,  ...,   -8.1515,   31.8660,\n",
      "           34.4739]])\n",
      "Shape of Tensor:  torch.Size([29, 100])\n"
     ]
    }
   ],
   "source": [
    "tensor_sum = torch.sum(product, \n",
    "                       dim = 1)\n",
    "print('Sum of Tensor across 1st Dimension: \\n' , tensor_sum)\n",
    "\n",
    "print('Shape of Tensor: ' , tensor_sum.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create a new long tensor of size  (3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Tensor: \n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Updated Long Tensor: \n",
      " tensor([[2, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 4, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 6, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "long_tensor = torch.ones((3, 10), \n",
    "                         dtype = torch.long)\n",
    "print('Long Tensor: \\n' , long_tensor)\n",
    "\n",
    "long_tensor[0, 0] = 2\n",
    "long_tensor[1, 2] = 4\n",
    "long_tensor[2, 4] = 6\n",
    "\n",
    "print('Updated Long Tensor: \\n' , long_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Use this new long tensor to index into the tensor from step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed Tensor: \n",
      " tensor([[[[ -7.4345,  14.9439,  -2.7215,  ...,  -0.5436,  14.2772,   5.2867],\n",
      "          [ -7.5322,   2.3207,   2.3941,  ...,   3.0348,  -5.0813,   2.7684],\n",
      "          [ -6.3394,  -1.6578, -15.2204,  ...,  -2.6148,  -5.7177,  -5.8000],\n",
      "          ...,\n",
      "          [  0.3471,   0.5292,  -0.0286,  ...,  -2.2514,  10.0249,  -1.1148],\n",
      "          [ -1.6198,   1.6737,  -5.8284,  ...,   5.0135,   5.6666,   5.0532],\n",
      "          [ -4.8812,   3.6147,  -4.2578,  ...,  -1.2021,   6.5086,  -3.4150]],\n",
      "\n",
      "         [[  1.3306,   3.9205,   0.2360,  ...,   6.2160,   4.7269,   2.5456],\n",
      "          [ -2.1774,  -3.2473,  -2.5274,  ...,   3.5847,  -2.9077,   2.0045],\n",
      "          [  3.4768,   1.0987,  -4.0413,  ...,   2.4075,  -7.9049,  -3.2112],\n",
      "          ...,\n",
      "          [  2.3400,  -6.5402,  -4.7628,  ...,   8.0365,  -5.0185,   2.6872],\n",
      "          [  7.7205,  -5.6621,  -4.2838,  ...,  -2.6443,  -5.1610,   4.8454],\n",
      "          [ -3.1295,   5.9888,   2.1850,  ...,  -1.1024,   4.8072,  -3.6004]],\n",
      "\n",
      "         [[  1.3306,   3.9205,   0.2360,  ...,   6.2160,   4.7269,   2.5456],\n",
      "          [ -2.1774,  -3.2473,  -2.5274,  ...,   3.5847,  -2.9077,   2.0045],\n",
      "          [  3.4768,   1.0987,  -4.0413,  ...,   2.4075,  -7.9049,  -3.2112],\n",
      "          ...,\n",
      "          [  2.3400,  -6.5402,  -4.7628,  ...,   8.0365,  -5.0185,   2.6872],\n",
      "          [  7.7205,  -5.6621,  -4.2838,  ...,  -2.6443,  -5.1610,   4.8454],\n",
      "          [ -3.1295,   5.9888,   2.1850,  ...,  -1.1024,   4.8072,  -3.6004]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  1.3306,   3.9205,   0.2360,  ...,   6.2160,   4.7269,   2.5456],\n",
      "          [ -2.1774,  -3.2473,  -2.5274,  ...,   3.5847,  -2.9077,   2.0045],\n",
      "          [  3.4768,   1.0987,  -4.0413,  ...,   2.4075,  -7.9049,  -3.2112],\n",
      "          ...,\n",
      "          [  2.3400,  -6.5402,  -4.7628,  ...,   8.0365,  -5.0185,   2.6872],\n",
      "          [  7.7205,  -5.6621,  -4.2838,  ...,  -2.6443,  -5.1610,   4.8454],\n",
      "          [ -3.1295,   5.9888,   2.1850,  ...,  -1.1024,   4.8072,  -3.6004]],\n",
      "\n",
      "         [[  1.3306,   3.9205,   0.2360,  ...,   6.2160,   4.7269,   2.5456],\n",
      "          [ -2.1774,  -3.2473,  -2.5274,  ...,   3.5847,  -2.9077,   2.0045],\n",
      "          [  3.4768,   1.0987,  -4.0413,  ...,   2.4075,  -7.9049,  -3.2112],\n",
      "          ...,\n",
      "          [  2.3400,  -6.5402,  -4.7628,  ...,   8.0365,  -5.0185,   2.6872],\n",
      "          [  7.7205,  -5.6621,  -4.2838,  ...,  -2.6443,  -5.1610,   4.8454],\n",
      "          [ -3.1295,   5.9888,   2.1850,  ...,  -1.1024,   4.8072,  -3.6004]],\n",
      "\n",
      "         [[  1.3306,   3.9205,   0.2360,  ...,   6.2160,   4.7269,   2.5456],\n",
      "          [ -2.1774,  -3.2473,  -2.5274,  ...,   3.5847,  -2.9077,   2.0045],\n",
      "          [  3.4768,   1.0987,  -4.0413,  ...,   2.4075,  -7.9049,  -3.2112],\n",
      "          ...,\n",
      "          [  2.3400,  -6.5402,  -4.7628,  ...,   8.0365,  -5.0185,   2.6872],\n",
      "          [  7.7205,  -5.6621,  -4.2838,  ...,  -2.6443,  -5.1610,   4.8454],\n",
      "          [ -3.1295,   5.9888,   2.1850,  ...,  -1.1024,   4.8072,  -3.6004]]],\n",
      "\n",
      "\n",
      "        [[[  1.3306,   3.9205,   0.2360,  ...,   6.2160,   4.7269,   2.5456],\n",
      "          [ -2.1774,  -3.2473,  -2.5274,  ...,   3.5847,  -2.9077,   2.0045],\n",
      "          [  3.4768,   1.0987,  -4.0413,  ...,   2.4075,  -7.9049,  -3.2112],\n",
      "          ...,\n",
      "          [  2.3400,  -6.5402,  -4.7628,  ...,   8.0365,  -5.0185,   2.6872],\n",
      "          [  7.7205,  -5.6621,  -4.2838,  ...,  -2.6443,  -5.1610,   4.8454],\n",
      "          [ -3.1295,   5.9888,   2.1850,  ...,  -1.1024,   4.8072,  -3.6004]],\n",
      "\n",
      "         [[  1.3306,   3.9205,   0.2360,  ...,   6.2160,   4.7269,   2.5456],\n",
      "          [ -2.1774,  -3.2473,  -2.5274,  ...,   3.5847,  -2.9077,   2.0045],\n",
      "          [  3.4768,   1.0987,  -4.0413,  ...,   2.4075,  -7.9049,  -3.2112],\n",
      "          ...,\n",
      "          [  2.3400,  -6.5402,  -4.7628,  ...,   8.0365,  -5.0185,   2.6872],\n",
      "          [  7.7205,  -5.6621,  -4.2838,  ...,  -2.6443,  -5.1610,   4.8454],\n",
      "          [ -3.1295,   5.9888,   2.1850,  ...,  -1.1024,   4.8072,  -3.6004]],\n",
      "\n",
      "         [[ 11.1147,  -9.9184,  -9.6159,  ...,  -1.6974,   0.8207,  -0.3125],\n",
      "          [ -2.8696,   5.1269,   8.7037,  ...,   2.4385,   4.9442,  -2.5012],\n",
      "          [  6.3452,  12.3588,  -7.6058,  ...,  -7.8992,   5.7964,   2.5619],\n",
      "          ...,\n",
      "          [-12.5425,   2.0395,   4.7404,  ...,   2.9999,  -2.0421,  -7.5536],\n",
      "          [  2.6996,  15.9890,  10.6854,  ...,  -1.6150,  -5.8491,  -8.0258],\n",
      "          [ -3.5123,  14.4440, -10.6059,  ...,   0.8664,   8.3178,   2.6813]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  1.3306,   3.9205,   0.2360,  ...,   6.2160,   4.7269,   2.5456],\n",
      "          [ -2.1774,  -3.2473,  -2.5274,  ...,   3.5847,  -2.9077,   2.0045],\n",
      "          [  3.4768,   1.0987,  -4.0413,  ...,   2.4075,  -7.9049,  -3.2112],\n",
      "          ...,\n",
      "          [  2.3400,  -6.5402,  -4.7628,  ...,   8.0365,  -5.0185,   2.6872],\n",
      "          [  7.7205,  -5.6621,  -4.2838,  ...,  -2.6443,  -5.1610,   4.8454],\n",
      "          [ -3.1295,   5.9888,   2.1850,  ...,  -1.1024,   4.8072,  -3.6004]],\n",
      "\n",
      "         [[  1.3306,   3.9205,   0.2360,  ...,   6.2160,   4.7269,   2.5456],\n",
      "          [ -2.1774,  -3.2473,  -2.5274,  ...,   3.5847,  -2.9077,   2.0045],\n",
      "          [  3.4768,   1.0987,  -4.0413,  ...,   2.4075,  -7.9049,  -3.2112],\n",
      "          ...,\n",
      "          [  2.3400,  -6.5402,  -4.7628,  ...,   8.0365,  -5.0185,   2.6872],\n",
      "          [  7.7205,  -5.6621,  -4.2838,  ...,  -2.6443,  -5.1610,   4.8454],\n",
      "          [ -3.1295,   5.9888,   2.1850,  ...,  -1.1024,   4.8072,  -3.6004]],\n",
      "\n",
      "         [[  1.3306,   3.9205,   0.2360,  ...,   6.2160,   4.7269,   2.5456],\n",
      "          [ -2.1774,  -3.2473,  -2.5274,  ...,   3.5847,  -2.9077,   2.0045],\n",
      "          [  3.4768,   1.0987,  -4.0413,  ...,   2.4075,  -7.9049,  -3.2112],\n",
      "          ...,\n",
      "          [  2.3400,  -6.5402,  -4.7628,  ...,   8.0365,  -5.0185,   2.6872],\n",
      "          [  7.7205,  -5.6621,  -4.2838,  ...,  -2.6443,  -5.1610,   4.8454],\n",
      "          [ -3.1295,   5.9888,   2.1850,  ...,  -1.1024,   4.8072,  -3.6004]]],\n",
      "\n",
      "\n",
      "        [[[  1.3306,   3.9205,   0.2360,  ...,   6.2160,   4.7269,   2.5456],\n",
      "          [ -2.1774,  -3.2473,  -2.5274,  ...,   3.5847,  -2.9077,   2.0045],\n",
      "          [  3.4768,   1.0987,  -4.0413,  ...,   2.4075,  -7.9049,  -3.2112],\n",
      "          ...,\n",
      "          [  2.3400,  -6.5402,  -4.7628,  ...,   8.0365,  -5.0185,   2.6872],\n",
      "          [  7.7205,  -5.6621,  -4.2838,  ...,  -2.6443,  -5.1610,   4.8454],\n",
      "          [ -3.1295,   5.9888,   2.1850,  ...,  -1.1024,   4.8072,  -3.6004]],\n",
      "\n",
      "         [[  1.3306,   3.9205,   0.2360,  ...,   6.2160,   4.7269,   2.5456],\n",
      "          [ -2.1774,  -3.2473,  -2.5274,  ...,   3.5847,  -2.9077,   2.0045],\n",
      "          [  3.4768,   1.0987,  -4.0413,  ...,   2.4075,  -7.9049,  -3.2112],\n",
      "          ...,\n",
      "          [  2.3400,  -6.5402,  -4.7628,  ...,   8.0365,  -5.0185,   2.6872],\n",
      "          [  7.7205,  -5.6621,  -4.2838,  ...,  -2.6443,  -5.1610,   4.8454],\n",
      "          [ -3.1295,   5.9888,   2.1850,  ...,  -1.1024,   4.8072,  -3.6004]],\n",
      "\n",
      "         [[  1.3306,   3.9205,   0.2360,  ...,   6.2160,   4.7269,   2.5456],\n",
      "          [ -2.1774,  -3.2473,  -2.5274,  ...,   3.5847,  -2.9077,   2.0045],\n",
      "          [  3.4768,   1.0987,  -4.0413,  ...,   2.4075,  -7.9049,  -3.2112],\n",
      "          ...,\n",
      "          [  2.3400,  -6.5402,  -4.7628,  ...,   8.0365,  -5.0185,   2.6872],\n",
      "          [  7.7205,  -5.6621,  -4.2838,  ...,  -2.6443,  -5.1610,   4.8454],\n",
      "          [ -3.1295,   5.9888,   2.1850,  ...,  -1.1024,   4.8072,  -3.6004]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  1.3306,   3.9205,   0.2360,  ...,   6.2160,   4.7269,   2.5456],\n",
      "          [ -2.1774,  -3.2473,  -2.5274,  ...,   3.5847,  -2.9077,   2.0045],\n",
      "          [  3.4768,   1.0987,  -4.0413,  ...,   2.4075,  -7.9049,  -3.2112],\n",
      "          ...,\n",
      "          [  2.3400,  -6.5402,  -4.7628,  ...,   8.0365,  -5.0185,   2.6872],\n",
      "          [  7.7205,  -5.6621,  -4.2838,  ...,  -2.6443,  -5.1610,   4.8454],\n",
      "          [ -3.1295,   5.9888,   2.1850,  ...,  -1.1024,   4.8072,  -3.6004]],\n",
      "\n",
      "         [[  1.3306,   3.9205,   0.2360,  ...,   6.2160,   4.7269,   2.5456],\n",
      "          [ -2.1774,  -3.2473,  -2.5274,  ...,   3.5847,  -2.9077,   2.0045],\n",
      "          [  3.4768,   1.0987,  -4.0413,  ...,   2.4075,  -7.9049,  -3.2112],\n",
      "          ...,\n",
      "          [  2.3400,  -6.5402,  -4.7628,  ...,   8.0365,  -5.0185,   2.6872],\n",
      "          [  7.7205,  -5.6621,  -4.2838,  ...,  -2.6443,  -5.1610,   4.8454],\n",
      "          [ -3.1295,   5.9888,   2.1850,  ...,  -1.1024,   4.8072,  -3.6004]],\n",
      "\n",
      "         [[  1.3306,   3.9205,   0.2360,  ...,   6.2160,   4.7269,   2.5456],\n",
      "          [ -2.1774,  -3.2473,  -2.5274,  ...,   3.5847,  -2.9077,   2.0045],\n",
      "          [  3.4768,   1.0987,  -4.0413,  ...,   2.4075,  -7.9049,  -3.2112],\n",
      "          ...,\n",
      "          [  2.3400,  -6.5402,  -4.7628,  ...,   8.0365,  -5.0185,   2.6872],\n",
      "          [  7.7205,  -5.6621,  -4.2838,  ...,  -2.6443,  -5.1610,   4.8454],\n",
      "          [ -3.1295,   5.9888,   2.1850,  ...,  -1.1024,   4.8072,  -3.6004]]]])\n",
      "Shape of Tensor:  torch.Size([3, 10, 30, 100])\n"
     ]
    }
   ],
   "source": [
    "indexed_tensor = product[long_tensor]\n",
    "print('Indexed Tensor: \\n' , indexed_tensor)\n",
    "\n",
    "print('Shape of Tensor: ' , indexed_tensor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Use  torch.mean  to average across the last dimension in the tensor from step 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.6802,  0.1478, -0.4493,  0.5894, -0.2018, -0.3625, -0.0960,\n",
      "          -0.0247, -0.1436, -0.6260, -0.2898, -1.0251, -0.8783, -0.6275,\n",
      "           0.8946, -0.2514, -0.2000,  0.4910, -0.9260, -0.1619, -0.6153,\n",
      "           0.1295, -0.4846,  0.0144,  0.1611, -0.2952,  0.0742,  0.5718,\n",
      "          -0.5532,  0.2340],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022]],\n",
      "\n",
      "        [[ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.4735,  0.7238, -0.1419, -0.9488,  0.5956,  0.7184,  0.2160,\n",
      "           0.6073, -0.5318, -0.0625,  0.3924,  0.0524, -0.2982, -0.5807,\n",
      "          -0.1722, -0.1946, -0.4367,  1.2988, -0.0685,  0.0796,  0.0273,\n",
      "           0.6769,  0.1218, -0.0738,  0.3466, -0.3643, -0.3065, -0.2245,\n",
      "          -0.9614, -0.3894],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022]],\n",
      "\n",
      "        [[ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.0275,  0.1283,  0.1626, -0.3484,  0.0907, -0.5778,  1.2049,\n",
      "          -0.0767, -0.5373,  0.6632,  0.0676,  0.2128, -0.2092,  0.6887,\n",
      "           0.0982,  1.1918, -0.3527, -1.0559, -0.1062, -0.4807, -0.6931,\n",
      "          -0.5391, -0.3794,  0.1930, -0.5290,  0.4545,  0.0504, -0.6461,\n",
      "           0.3622,  1.0058],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022],\n",
      "         [ 0.2324, -0.4181, -0.0926, -0.2284, -0.4701,  0.0532, -0.2603,\n",
      "           0.8504,  0.1164,  0.2217,  0.2363,  0.6357, -0.4133, -0.0121,\n",
      "           0.3394, -0.3771,  0.0648,  0.0354,  0.2620, -0.5153, -0.4681,\n",
      "           0.6021,  0.2546, -0.7532, -0.1000, -0.6862,  0.3918,  0.0464,\n",
      "           0.2880,  0.6022]]])\n",
      "Shape of Tensor:  torch.Size([3, 10, 30])\n"
     ]
    }
   ],
   "source": [
    "mean_tensor = torch.mean(indexed_tensor, \n",
    "                         dim = 3)\n",
    "print(mean_tensor)\n",
    "\n",
    "print('Shape of Tensor: ' , mean_tensor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Redo step 2. on the GPU and compare results from step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product of tensor_a and tensor_b on GPU: \n",
      " tensor([[[-13.2621,   9.3731, -11.7893,  ...,  -7.3152,  11.6473,  -6.2425],\n",
      "         [  1.6365,  -3.0511,   2.1437,  ...,   1.7488,  -6.4826,  -3.2745],\n",
      "         [ -5.7323,   2.4026,   5.0012,  ...,   2.1264,   1.2310,   7.0712],\n",
      "         ...,\n",
      "         [  1.7914,  -0.2842,  -0.6901,  ...,   7.1259,  -8.9405,  -0.2058],\n",
      "         [ -7.0371,   2.5656,   5.3010,  ...,   2.4419, -14.7984,   0.6097],\n",
      "         [ -6.4793,  -5.5452,   6.9085,  ...,  -0.6199,  -7.2145, -11.2881]],\n",
      "\n",
      "        [[  1.3306,   3.9205,   0.2360,  ...,   6.2160,   4.7269,   2.5456],\n",
      "         [ -2.1774,  -3.2473,  -2.5274,  ...,   3.5847,  -2.9077,   2.0045],\n",
      "         [  3.4768,   1.0987,  -4.0413,  ...,   2.4075,  -7.9049,  -3.2112],\n",
      "         ...,\n",
      "         [  2.3400,  -6.5402,  -4.7628,  ...,   8.0365,  -5.0185,   2.6872],\n",
      "         [  7.7205,  -5.6621,  -4.2838,  ...,  -2.6443,  -5.1610,   4.8454],\n",
      "         [ -3.1295,   5.9888,   2.1850,  ...,  -1.1024,   4.8072,  -3.6004]],\n",
      "\n",
      "        [[ -7.4345,  14.9439,  -2.7215,  ...,  -0.5436,  14.2772,   5.2867],\n",
      "         [ -7.5322,   2.3207,   2.3941,  ...,   3.0348,  -5.0813,   2.7684],\n",
      "         [ -6.3394,  -1.6578, -15.2204,  ...,  -2.6148,  -5.7177,  -5.8000],\n",
      "         ...,\n",
      "         [  0.3471,   0.5292,  -0.0286,  ...,  -2.2514,  10.0249,  -1.1148],\n",
      "         [ -1.6198,   1.6737,  -5.8284,  ...,   5.0135,   5.6666,   5.0532],\n",
      "         [ -4.8812,   3.6147,  -4.2578,  ...,  -1.2021,   6.5086,  -3.4150]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -2.7385,   4.9662,  -9.2138,  ...,  -3.9418,   9.0622,   5.7966],\n",
      "         [  0.2036,  -2.6212,   9.1101,  ...,   2.3419,   0.1791,  -3.0513],\n",
      "         [  2.2207,   6.2934,  -3.2702,  ...,  -4.9527,   1.3526,   1.8392],\n",
      "         ...,\n",
      "         [ -2.9905,  -2.7910,  -0.6549,  ...,   1.0246,  -2.0625,   4.3803],\n",
      "         [  3.4376,  -1.1878,   8.1888,  ...,  -7.7703, -13.7241,  -4.7959],\n",
      "         [  2.5718,   1.2749,   1.5723,  ...,   2.4818,  -7.6246,  -1.5750]],\n",
      "\n",
      "        [[ -3.4492,   8.9613,   8.8209,  ...,   0.3223,  -4.2518,  -0.8447],\n",
      "         [  2.6030,   1.3937,  -3.2947,  ...,  -2.0370,   0.7166,  -1.9964],\n",
      "         [  1.3181,   0.9509,  -0.7915,  ...,  -5.9073,   1.7024,   0.3803],\n",
      "         ...,\n",
      "         [ -0.9745,   9.3337,  -3.9029,  ...,  -0.6372,  -8.6131,   4.0391],\n",
      "         [ -9.5980,   9.0772,   3.4929,  ...,  -4.9908,   2.9416,  -2.1731],\n",
      "         [  5.7465,  11.4815,   1.3964,  ...,   1.5567,   1.8218,  -2.8327]],\n",
      "\n",
      "        [[  9.5255,  -3.4434,  -1.5909,  ...,   1.7131,   4.0838,   1.2086],\n",
      "         [ -6.5496,  -0.7251,  -2.5107,  ...,   2.6548,   0.7187,   2.6837],\n",
      "         [ -8.2246,  -4.2586,  -0.1263,  ...,  -1.6266,  -8.5336,  -0.6474],\n",
      "         ...,\n",
      "         [  1.2935,  -3.2013,  -2.8900,  ...,  -6.9646,   1.5004,  -0.2604],\n",
      "         [ -4.0238,  -3.3850,  -0.6968,  ...,  -2.2987,   2.7812,   7.5491],\n",
      "         [-14.1996,   1.4640,  -9.2722,  ...,   0.8289,  -5.4982,   8.6846]]])\n",
      "Shape of Tensor:  torch.Size([29, 30, 100])\n"
     ]
    }
   ],
   "source": [
    "tensor_a_cuda = tensor_a.to(device = device)\n",
    "tensor_b_cuda = tensor_b.to(device = device)\n",
    "product_gpu = torch.matmul(tensor_a, tensor_b)\n",
    "print('Product of tensor_a and tensor_b on GPU: \\n' , product_gpu)\n",
    "\n",
    "print('Shape of Tensor: ' , product_gpu.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Write a pure PyTorch program to compute the value of $\\sqrt{2}$ up to 4 decimal places without using the square root or other math functions from any of the libraries. \n",
    "### Hint: Notice that the answer is the (positive) root of the equation, $$𝑥^2 −2 = 0$$ \n",
    "### To find the root, you might want to use \"Newton's Method\": $$𝑥_{𝑛+1} = 𝑥_{𝑛} − \\frac{𝑓(𝑥)}{𝑓′(𝑥)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fail-fast prototyping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building neural networks, you want things to either work or fail fast. Long iteration loops are \n",
    "the worst enemy of a machine learning practitioner. \\\n",
    "For e.g., while writing code, you might want to incrementally test your code by doing something \n",
    "like this:\n",
    "\n",
    "batch_size = 32 \\\n",
    "num_features = 512 \\\n",
    "embedding_size = 16\n",
    "\n",
    "\\# construct a dummy input \\\n",
    "x = torch.randn(batch_size, num_features)\n",
    "\n",
    "\\# we want to project the input to embedding_size \\\n",
    "fc = torch.nn.Linear(num_features, embedding_size)\n",
    "\n",
    "\\# test if that works \\\n",
    "print(fc(x).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fail-fast exercises"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. [Glove](https://nlp.stanford.edu/projects/glove/) has 300 dimension embeddings. Design an nn.Module that takes a sentence of max_len words, tokenizes words by spaces, represents the sentence by averaging the glove embeddings of constituent words. What is the shape of the resulting sentence embedding? When you implement this, you will need to make some assumptions. What are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2196017 words present in GloVe\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe Embeddings\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "GLOVE_DIM = 300\n",
    "glove = GloVe(name = '840B', \n",
    "              dim = GLOVE_DIM)\n",
    "\n",
    "print(f'Loaded {len(glove.itos)} words present in GloVe')\n",
    "\n",
    "embeddings_tensor = glove.vectors\n",
    "embeddings_tensor = embeddings_tensor.to(device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "NUM_SENT = 512\n",
    "sents = list()\n",
    "for i in range(NUM_SENT):\n",
    "    sents.append('This is the quest zero and it has a deadline this Sunday, March 29')\n",
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 300])\n",
      "tensor([[-4.5604e-02,  1.7180e-01, -8.1301e-02,  4.7771e-02, -1.2547e-02,\n",
      "         -1.1558e-01, -7.5017e-03, -1.1177e-01,  3.8822e-02,  2.2306e+00,\n",
      "         -1.5930e-01, -5.0211e-02, -1.2576e-01,  3.9923e-02, -1.0560e-01,\n",
      "         -1.7879e-01,  3.3900e-03,  1.1905e+00, -1.3859e-01,  2.1556e-02,\n",
      "         -5.3867e-02, -1.2833e-01,  6.1120e-02, -3.2409e-02,  8.4664e-02,\n",
      "          8.9627e-02, -8.6490e-02, -8.3099e-02, -1.1932e-01, -2.4925e-02,\n",
      "         -4.4643e-02,  7.1976e-02,  3.5064e-02, -2.2729e-02,  9.1324e-02,\n",
      "          4.1020e-03,  8.5674e-02, -4.3367e-02,  1.2036e-03, -2.2725e-01,\n",
      "          1.6723e-01,  2.0299e-01,  1.5572e-01, -1.5584e-01,  7.3151e-02,\n",
      "         -3.7005e-02, -1.9181e-01,  7.4557e-02, -1.4330e-01,  1.1176e-01,\n",
      "          8.2322e-03,  1.8108e-01, -4.9120e-02, -2.9479e-02,  4.8363e-02,\n",
      "          5.6336e-02,  2.6151e-02,  7.5750e-02,  1.1126e-01, -2.0988e-01,\n",
      "         -9.6492e-02,  6.5016e-02,  2.9249e-02,  1.1692e-01,  6.9829e-02,\n",
      "         -1.8340e-01, -1.3363e-01,  3.0138e-02,  1.6330e-02,  6.3146e-02,\n",
      "          4.7576e-02, -1.3977e-02,  1.0242e-01, -1.2967e-01, -3.6919e-02,\n",
      "          9.1614e-02,  3.8273e-02, -4.5660e-02, -1.1868e-01,  3.0705e-01,\n",
      "         -9.7282e-03,  4.9095e-02, -1.6945e-01,  1.5161e-01,  5.7374e-02,\n",
      "         -1.1019e-01, -1.9760e-01, -1.2057e-01,  3.0500e-01,  3.8362e-02,\n",
      "         -7.2322e-02,  1.6532e-01,  1.2932e-02, -7.5054e-02,  7.0397e-02,\n",
      "          9.1686e-02, -4.5714e-02, -6.3316e-02,  9.4555e-02,  1.7370e-01,\n",
      "         -3.6640e-02, -9.1180e-02, -1.6450e-01, -2.6746e-02,  7.9531e-03,\n",
      "         -7.0254e-01,  5.7901e-02,  1.3729e-01, -1.3604e-01, -1.1654e-01,\n",
      "          6.0795e-02,  1.7985e-03, -2.1384e-02, -2.1177e-02, -4.0723e-02,\n",
      "          1.3102e-01, -1.4873e-01,  2.1316e-01,  3.9930e-02, -1.3850e-01,\n",
      "          5.1204e-02, -7.2802e-02,  2.7882e-03, -1.4130e-02, -2.1058e-02,\n",
      "          1.0226e-01, -1.1095e-01, -1.8027e-01, -4.4426e-02, -3.3298e-02,\n",
      "          1.5529e-02, -1.4483e-02,  6.4244e-02,  1.1276e-01,  1.4843e-01,\n",
      "         -2.7529e-02, -5.5411e-02,  5.4986e-02, -1.0168e-01, -9.8121e-02,\n",
      "         -1.0654e+00,  5.0325e-02,  1.8815e-01,  7.7606e-02,  1.4520e-01,\n",
      "         -1.5210e-01,  2.8429e-02,  7.2754e-02,  1.7792e-01, -9.1062e-03,\n",
      "          1.2199e-02, -4.5307e-02,  1.8707e-01,  1.0359e-01,  5.5144e-03,\n",
      "          2.7590e-02, -1.0674e-01,  3.9082e-02, -1.3558e-01,  8.4497e-03,\n",
      "          5.9401e-02,  3.5032e-02, -1.1107e-01,  4.1742e-03, -1.4625e-01,\n",
      "         -1.9627e-01,  3.7615e-02, -4.6096e-02,  1.0045e-01, -1.2638e-01,\n",
      "         -1.7120e-02, -6.1360e-03,  1.2869e-01, -1.6960e-01, -2.0238e-01,\n",
      "          4.5545e-02,  1.4743e-01, -1.9244e-01, -4.1426e-02,  5.3588e-03,\n",
      "          2.1728e-02, -9.1708e-02, -1.3928e-01, -1.2467e-01, -1.6918e-01,\n",
      "         -1.0676e-01, -1.0439e-01, -6.4182e-02, -7.8346e-02,  1.5800e-02,\n",
      "         -2.2692e-01, -5.0263e-02, -3.0610e-01,  1.3636e-01,  1.3289e-01,\n",
      "          1.7109e-01, -5.9519e-02,  2.8948e-02, -2.4010e-02,  2.3475e-01,\n",
      "         -8.8040e-03, -6.0028e-02, -2.0014e-01,  1.3235e-01,  1.4194e-01,\n",
      "          2.5871e-02, -2.3389e-02,  1.1909e-01,  3.6117e-02,  3.0857e-02,\n",
      "         -1.9999e-02, -1.1453e-02, -9.5851e-02, -1.1327e-01,  4.5312e-02,\n",
      "          2.9022e-01, -1.3475e-01,  9.2048e-02, -1.3679e-01,  2.8867e-02,\n",
      "         -7.3581e-02, -4.0068e-02, -1.2756e-01,  3.6868e-03,  2.4202e-02,\n",
      "         -7.0108e-02,  1.0096e-01,  1.6123e-01,  4.1693e-02,  4.5953e-02,\n",
      "         -1.9059e-02,  4.6653e-02,  9.6804e-02,  9.0730e-02, -1.1901e-01,\n",
      "         -1.5368e-02, -3.9989e-02, -8.2614e-02, -2.1851e-01,  1.4986e-01,\n",
      "          4.0937e-02, -2.2934e-02, -2.2343e-02,  1.4073e-01,  3.5803e-02,\n",
      "         -2.8116e-01, -3.0646e-03, -1.4485e-01,  2.3792e-02, -4.4910e-02,\n",
      "         -2.0503e-01, -2.0290e-01,  1.1816e-01, -9.2644e-02,  5.7977e-03,\n",
      "          2.0142e-01,  6.5655e-02, -1.7024e-01,  9.7829e-03, -2.8923e-02,\n",
      "          1.0226e-01,  7.5721e-02, -2.7192e-03,  1.1195e-02,  3.6341e-02,\n",
      "         -1.4355e-01,  7.6557e-02, -7.7210e-02,  4.7464e-01, -8.9441e-02,\n",
      "          2.0857e-01, -3.4301e-02, -2.3352e-01, -2.3894e-01,  9.8284e-03,\n",
      "         -7.2733e-02, -5.0193e-02,  1.2248e-02,  1.9632e-01,  2.0684e-01,\n",
      "          2.6874e-01,  1.4279e-01, -8.0161e-02, -3.3960e-02,  6.7989e-03,\n",
      "         -1.8032e-01, -4.2907e-02, -9.4098e-03,  1.3184e-01, -8.3119e-02,\n",
      "         -1.2259e-01,  1.0436e-01,  7.2691e-02, -3.8155e-02,  7.7573e-02,\n",
      "         -1.1101e-01, -9.1185e-02, -6.9413e-02, -6.8885e-02,  8.7849e-02]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GloveEmbeddingAvg(nn.Module):\n",
    "    \n",
    "    def __init__(self, max_len):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings_tensor)\n",
    "        \n",
    "    def forward(self, \n",
    "                sent):\n",
    "        # Tokenize the sentence by spaces\n",
    "        tokens = sent.split(' ')[:self.max_len]\n",
    "        # Get idx of each token from the GloVe dictionary\n",
    "        glove_dict_indexes = [glove.stoi[token] for token in tokens]\n",
    "        # Convert it into Tensor\n",
    "        glove_dict_indexes = torch.tensor(glove_dict_indexes, \n",
    "                                          device = device)\n",
    "        # Get Word Embeddings for all tokens\n",
    "        word_embeds = self.embedding(glove_dict_indexes)\n",
    "        # Sentence Embedding = Average of Word Embeddings\n",
    "        sent_embeds = word_embeds.mean(dim = 0)\n",
    "        return sent_embeds.view(1, -1)\n",
    "\n",
    "MAX_LEN = 10\n",
    "glove_embeds_avg = GloveEmbeddingAvg(MAX_LEN).to(device)\n",
    "\n",
    "print(glove_embeds_avg(sents[0]).shape)\n",
    "print(glove_embeds_avg(sents[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How will you modify step 1. so that the sentence embeddings are in $R^{50}$ ?\n",
    "BONUS: Can you think of more than one way to do this? What are the implications of each method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50])\n",
      "tensor([[ 0.2243, -0.0319,  0.1634,  0.1289,  0.0428, -0.0822,  0.0282, -0.0104,\n",
      "         -0.1254,  0.0005,  0.0542, -0.0879, -0.0224,  0.0221, -0.0661,  0.0089,\n",
      "         -0.1289,  0.0929, -0.1153,  0.2188,  0.0277, -0.2442, -0.2005, -0.2073,\n",
      "         -0.0907,  0.1192,  0.0613,  0.0106, -0.1654,  0.0854,  0.2349,  0.0220,\n",
      "         -0.1359, -0.0507,  0.0487, -0.3426,  0.2141,  0.0548,  0.0302, -0.0428,\n",
      "          0.1925,  0.0710,  0.0021,  0.0956,  0.0110, -0.0648,  0.0865, -0.1772,\n",
      "         -0.2910, -0.0501]], device='mps:0', grad_fn=<LinearBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class GloveEmbeddingAvg_50_Dim(nn.Module):\n",
    "    \n",
    "    def __init__(self, max_len):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings_tensor)\n",
    "        self.fc = nn.Linear(GLOVE_DIM, 50)\n",
    "        \n",
    "    def forward(self, \n",
    "                sent):\n",
    "        # Tokenize the sentence by spaces\n",
    "        tokens = sent.split(' ')[:self.max_len]\n",
    "        # Get idx of each token from the GloVe dictionary\n",
    "        glove_dict_indexes = [glove.stoi[token] for token in tokens]\n",
    "        # Convert it into Tensor\n",
    "        glove_dict_indexes = torch.tensor(glove_dict_indexes, \n",
    "                                          device = device)\n",
    "        # Get Word Embeddings for all tokens\n",
    "        word_embeds = self.embedding(glove_dict_indexes)\n",
    "        # Sentence Embedding = Average of Word Embeddings\n",
    "        sent_embeds = word_embeds.mean(dim = 0)\n",
    "        return self.fc(sent_embeds.view(1, -1))\n",
    "\n",
    "MAX_LEN = 10\n",
    "glove_embeds_avg_50_dim = GloveEmbeddingAvg_50_Dim(MAX_LEN).to(device)\n",
    "\n",
    "print(glove_embeds_avg_50_dim(sents[0]).shape)\n",
    "print(glove_embeds_avg_50_dim(sents[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Quickly test your answer in step 2. with a batch of 512 sentences on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [90], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(sents), batch_size):\n\u001b[1;32m      6\u001b[0m     batch_sentences \u001b[39m=\u001b[39m sents[i:i\u001b[39m+\u001b[39mbatch_size]\n\u001b[0;32m----> 7\u001b[0m     sentence_embeddings \u001b[39m=\u001b[39m glove_embeds_avg_50_dim(batch_sentences)\n\u001b[1;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(sentence_embeddings\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/Documents/pvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [86], line 12\u001b[0m, in \u001b[0;36mGloveEmbeddingAvg_50_Dim.forward\u001b[0;34m(self, sent)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \n\u001b[1;32m     10\u001b[0m             sent):\n\u001b[1;32m     11\u001b[0m     \u001b[39m# Tokenize the sentence by spaces\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     tokens \u001b[39m=\u001b[39m sent\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)[:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_len]\n\u001b[1;32m     13\u001b[0m     \u001b[39m# Get idx of each token from the GloVe dictionary\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     glove_dict_indexes \u001b[39m=\u001b[39m [glove\u001b[39m.\u001b[39mstoi[token] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# sentences = [sent.to(device) for sent in sents]\n",
    "\n",
    "# Run forward pass\n",
    "batch_size = 512\n",
    "for i in range(0, len(sents), batch_size):\n",
    "    batch_sentences = sents[i:i+batch_size]\n",
    "    sentence_embeddings = glove_embeds_avg_50_dim(batch_sentences)\n",
    "    print(sentence_embeddings.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You almost implemented the model in the Deep Averaging Networks (DAN) paper!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Task: \n",
    "### Create a   MultiEmbedding  Module that can take two sets of indices, embed them, and concat the results. You might remember it from the previous lecture where we had to produce an embedding for \"green apple\" from embeddings of \"green\" and \"apple\". Your  MultiEmbedding class should work with the following test code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_emb, \n",
    "                 size_emb1, \n",
    "                 size_emb2):\n",
    "        super().__init__()\n",
    "        self.embedding_A = nn.Embedding(num_emb, size_emb1)\n",
    "        self.embedding_B = nn.Embedding(num_emb, size_emb2)\n",
    "        \n",
    "    def forward(self, \n",
    "                indices1, \n",
    "                indices2):\n",
    "        embed_A = self.embedding_A(indices1)\n",
    "        embed_B = self.embedding_B(indices2)\n",
    "        # Concatenate the Embeddings\n",
    "        return torch.cat((embed_A, embed_B), \n",
    "                         dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10, 500])\n"
     ]
    }
   ],
   "source": [
    "# Test code: instantiate a MultiEmbedding with the sizes for each embedding. \n",
    "# For this example, you can just randomly initialize each interior embedding. \n",
    "# In a practical setting, you might support methods for initializing with \n",
    "# combinations of embeddings, such as GloVe 300d vectors and word2vec 200d \n",
    "# vectors, yielding 500d embeddings. Both embeddings share a vocabulary/range \n",
    "# of supported indices indicated by `num_emb`\n",
    "\n",
    "NUM_EMB = 10000\n",
    "SIZE_EMB1 = 300\n",
    "SIZE_EMB2 = 200\n",
    "BATCH_SIZE = 64\n",
    "NUM_LENGTH = 10\n",
    "\n",
    "multiemb = MultiEmbedding(NUM_EMB, \n",
    "                          SIZE_EMB1, \n",
    "                          SIZE_EMB2).to(device)\n",
    "\n",
    "# You can then call this with a pair of indices where each value is in 0 <= i < num_emb\n",
    "indices1 =  torch.randint(0, \n",
    "                          NUM_EMB, \n",
    "                          (BATCH_SIZE, NUM_LENGTH), \n",
    "                          dtype = torch.long, \n",
    "                          device = device) # long tensor of shape (batch, num_length)\n",
    "indices2 =  torch.randint(0, \n",
    "                          NUM_EMB, \n",
    "                          (BATCH_SIZE, NUM_LENGTH), \n",
    "                          dtype = torch.long, \n",
    "                          device = device) # long tensor of shape (batch, num_length)\n",
    "output = multiemb(indices1, \n",
    "                  indices2)\n",
    "print(output.shape) # should be (batch, num_length, size_emb1 + size_emb2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Datasets and DataLoaders: \n",
    "### Read this short post on PyTorch Dataset and DataLoaders. Often in prototyping we need to generate dummy datasets to test our models. Implement a PyTorch Dataset class that generates up to  num_sentences  random sentences of length up to  max_len words. For each sentence, generate a binary label. You should be able to test your code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b37beb76676c8b0643f5764b5c5ae0ddf876ecbab29b433e279cae2d82963c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
