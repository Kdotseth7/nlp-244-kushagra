{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "0.14.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "print(torch.__version__)\n",
    "print(torchtext.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set device as GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    dev = 'mps'\n",
    "else:\n",
    "    dev = 'cpu'\n",
    "device = torch.device(dev)    \n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch warmup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use torch.randn to create two tensors of size (29, 30, 32) and (32, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of Size (29, 30, 32): \n",
      " tensor([[[ 1.4315,  0.2560,  0.8974,  ..., -2.6647, -1.3073,  0.1583],\n",
      "         [-0.6114, -0.3433, -1.3451,  ..., -0.2320, -1.5039,  2.9262],\n",
      "         [ 1.1454,  0.0537,  0.9149,  ...,  0.0663,  0.4858, -1.6646],\n",
      "         ...,\n",
      "         [-0.3856, -0.9098, -2.0143,  ..., -0.4296, -1.1758, -0.9505],\n",
      "         [ 0.8244, -0.3193, -0.5848,  ..., -0.3575, -1.0483, -0.0702],\n",
      "         [ 0.7738,  0.6565, -0.4229,  ...,  0.2583, -0.0219, -1.6786]],\n",
      "\n",
      "        [[-0.4414,  1.0857, -0.5175,  ..., -1.9632,  1.1334,  0.8850],\n",
      "         [ 0.1220, -0.1350,  0.2817,  ..., -0.1495, -0.0962, -1.2270],\n",
      "         [ 0.4937,  0.2788, -0.6888,  ..., -0.1183,  1.3999,  0.4678],\n",
      "         ...,\n",
      "         [ 1.5856, -1.0178,  0.2445,  ...,  0.5461, -1.5699,  0.7392],\n",
      "         [-0.3451, -0.4753,  2.6023,  ..., -0.5233, -0.2562,  1.7506],\n",
      "         [-0.6029,  0.4195, -0.3989,  ...,  1.0408, -0.4963, -0.2334]],\n",
      "\n",
      "        [[ 0.8383,  0.2752,  1.0120,  ...,  0.1503,  0.6751,  1.0003],\n",
      "         [-1.5565,  0.6634, -2.6744,  ...,  0.3630,  0.2708,  0.2465],\n",
      "         [ 0.1265,  0.9095, -0.5836,  ...,  0.1095, -1.3315,  1.8616],\n",
      "         ...,\n",
      "         [-0.9518,  0.4398,  0.3325,  ...,  2.0217, -0.0892, -2.4095],\n",
      "         [-0.8844,  0.2934, -1.4634,  ...,  1.5019,  0.8972,  0.2178],\n",
      "         [-0.1417, -0.3883, -0.4182,  ..., -0.4419,  1.7522, -0.9901]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.4474,  0.1290, -0.5595,  ...,  1.1140, -2.3805, -0.8742],\n",
      "         [-0.9079,  0.8682, -0.0142,  ..., -0.8263,  0.3574,  0.8860],\n",
      "         [-0.0206, -0.9600,  0.4872,  ...,  0.6817,  0.6760, -0.0894],\n",
      "         ...,\n",
      "         [-0.2038, -0.1995, -0.2455,  ...,  0.6288, -1.4242, -0.2804],\n",
      "         [-0.3699, -0.8958, -0.2257,  ...,  0.7720, -0.1855, -1.7357],\n",
      "         [ 1.0642, -0.0779, -0.5322,  ..., -0.3107, -1.7833, -0.2909]],\n",
      "\n",
      "        [[-1.1271,  0.2596, -0.3200,  ..., -0.7569,  0.0460,  0.6473],\n",
      "         [-1.1239, -0.0375,  1.8525,  ..., -0.3372, -0.2776,  0.0177],\n",
      "         [-0.7185, -0.8907,  0.9387,  ...,  0.1720, -0.3036, -0.0958],\n",
      "         ...,\n",
      "         [-0.2273,  1.8019,  0.6599,  ..., -2.4599, -2.5666, -0.7532],\n",
      "         [-1.2667, -0.4669,  1.8151,  ...,  0.1960, -1.4356, -0.2948],\n",
      "         [-0.2403, -0.0339,  0.8672,  ..., -0.4035, -1.1766, -0.9083]],\n",
      "\n",
      "        [[ 0.9316, -1.1720, -0.0856,  ...,  0.5880,  0.4387,  0.0510],\n",
      "         [-1.7490,  0.8311, -0.0908,  ...,  0.0330,  0.3900,  1.3674],\n",
      "         [ 0.9620,  1.9686,  1.3038,  ..., -1.0948, -0.2640,  0.8758],\n",
      "         ...,\n",
      "         [-3.2207,  1.2883, -0.7264,  ...,  1.2929, -0.4207, -1.9993],\n",
      "         [ 0.5855, -0.4526, -0.9505,  ...,  1.6864,  0.2911, -0.2842],\n",
      "         [-0.8823, -0.9764,  1.5046,  ..., -0.0713, -0.3287,  1.2047]]])\n",
      "Tensor of Size (32, 100): \n",
      " tensor([[ 0.7561, -0.8055,  1.5560,  ..., -0.8533, -0.4853, -0.1515],\n",
      "        [-1.5625,  0.9323,  0.8403,  ..., -0.9052, -0.7889, -0.0276],\n",
      "        [-0.3214,  1.1376,  1.2624,  ...,  0.7560, -2.0649, -0.4107],\n",
      "        ...,\n",
      "        [ 0.5209,  0.8830,  0.7466,  ...,  0.8953,  0.4060,  0.5957],\n",
      "        [ 0.7093, -0.6623, -1.9032,  ...,  1.2915, -0.0597, -1.8323],\n",
      "        [-0.2022,  0.0699, -0.1704,  ...,  1.0568, -1.4163, -0.4564]])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.randn(29, 30, 32)\n",
    "print('Tensor of Size (29, 30, 32): \\n' , tensor_a)\n",
    "\n",
    "tensor_b = torch.randn(32, 100)\n",
    "print('Tensor of Size (32, 100): \\n' , tensor_b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use  torch.matmul  to matrix multiply the two tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product of tensor_a and tensor_b: \n",
      " tensor([[[-1.2935e+01,  1.0707e+01,  2.3989e+00,  ..., -1.2342e+01,\n",
      "           1.3075e+00,  3.2499e+00],\n",
      "         [ 2.3836e+00, -7.7838e+00, -6.2557e+00,  ...,  3.4082e+00,\n",
      "           5.9395e+00,  7.6136e+00],\n",
      "         [ 6.9409e+00, -6.9336e-01, -1.8721e+00,  ...,  1.6076e+01,\n",
      "          -1.9905e+00,  7.5179e+00],\n",
      "         ...,\n",
      "         [-1.6975e+00,  6.7376e+00, -1.0056e+00,  ..., -1.6540e+01,\n",
      "           1.4331e+01,  4.9957e+00],\n",
      "         [-4.5444e+00,  7.5032e+00,  2.5883e+00,  ..., -9.4577e-01,\n",
      "          -2.1814e+00,  4.5546e+00],\n",
      "         [-2.6276e+00,  3.1530e+00,  8.3507e+00,  ..., -1.6448e+01,\n",
      "           2.5821e+00, -3.0676e+00]],\n",
      "\n",
      "        [[ 5.4222e+00, -6.4050e+00, -9.5461e-02,  ..., -1.0439e+01,\n",
      "          -6.7244e+00, -2.8128e+00],\n",
      "         [ 1.3937e+00,  2.8340e+00, -5.8951e+00,  ..., -1.4838e+00,\n",
      "           1.3206e+00,  3.5743e+00],\n",
      "         [ 1.0266e+01,  5.1957e+00,  6.6383e+00,  ..., -6.0728e+00,\n",
      "           2.4461e+00, -3.6607e+00],\n",
      "         ...,\n",
      "         [-4.3729e+00,  5.9844e+00,  2.9867e+00,  ..., -9.2773e+00,\n",
      "           2.5517e+00,  5.6538e+00],\n",
      "         [-6.3688e+00,  2.8970e+00,  3.2849e+00,  ...,  7.0410e+00,\n",
      "           3.2560e+00, -3.8223e+00],\n",
      "         [ 6.4466e-01,  5.0446e+00,  1.1436e+01,  ...,  3.5320e+00,\n",
      "          -2.0670e+00, -2.0816e+00]],\n",
      "\n",
      "        [[-3.0986e+00,  9.5812e+00,  1.5876e+00,  ...,  2.4863e+00,\n",
      "           6.5920e-02,  1.2564e+00],\n",
      "         [-8.7685e+00,  2.7233e-01,  1.4871e+00,  ..., -6.7436e+00,\n",
      "           9.4754e+00, -8.7629e-01],\n",
      "         [-1.1609e+01,  4.7631e+00,  3.8919e+00,  ...,  4.0621e+00,\n",
      "           2.9426e+00,  8.9158e+00],\n",
      "         ...,\n",
      "         [-1.0908e+01,  2.3546e+00,  2.4360e+00,  ...,  4.4684e+00,\n",
      "           3.0328e+00,  4.2456e+00],\n",
      "         [-8.0787e+00,  3.0227e+00,  1.1281e-01,  ...,  1.4984e+01,\n",
      "           1.0018e+01, -9.0426e+00],\n",
      "         [-2.9087e+00, -7.6347e+00,  3.0203e+00,  ...,  2.4480e+00,\n",
      "          -7.7035e+00, -1.4740e+01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-8.1788e+00,  8.8674e+00, -7.3693e-01,  ...,  2.5902e+00,\n",
      "           2.7675e+00,  7.7466e+00],\n",
      "         [-5.1496e+00,  3.5086e+00, -3.5146e+00,  ...,  2.4844e+00,\n",
      "           2.2639e+00,  1.7528e-01],\n",
      "         [ 6.1902e+00,  2.0335e+00,  6.4638e-01,  ..., -5.7800e+00,\n",
      "          -1.3023e+00, -3.7936e-01],\n",
      "         ...,\n",
      "         [ 9.0362e-01,  1.5561e+00,  1.2579e+01,  ...,  6.4267e+00,\n",
      "          -2.2734e+00, -6.3688e+00],\n",
      "         [-2.3345e+00, -4.2633e+00, -1.0061e+01,  ...,  8.4776e+00,\n",
      "           2.2912e+00,  5.8614e+00],\n",
      "         [ 9.4220e+00, -1.7101e+00,  7.8408e+00,  ..., -3.0075e+00,\n",
      "          -8.9621e+00,  5.7401e+00]],\n",
      "\n",
      "        [[ 9.5353e+00,  2.8743e+00,  4.6909e+00,  ..., -7.6369e-01,\n",
      "          -2.3512e+00, -4.0370e+00],\n",
      "         [-8.8055e+00,  1.0815e+01,  9.5747e-01,  ...,  8.2482e-01,\n",
      "           4.7813e+00,  3.0568e+00],\n",
      "         [-5.9513e+00,  2.2977e+00,  7.5078e+00,  ..., -5.1231e+00,\n",
      "          -3.2958e+00, -4.1434e+00],\n",
      "         ...,\n",
      "         [-6.8262e+00,  1.4623e+01,  7.9500e+00,  ..., -5.9183e+00,\n",
      "          -2.9453e+00,  2.5687e+00],\n",
      "         [-2.4890e+00,  6.9971e+00, -1.2709e+00,  ..., -3.4619e+00,\n",
      "           7.2219e-01,  7.5046e+00],\n",
      "         [-1.2298e+01, -5.8897e+00,  5.8404e+00,  ..., -3.3862e+00,\n",
      "           4.3833e+00, -6.5206e+00]],\n",
      "\n",
      "        [[ 5.2547e+00, -7.9891e+00, -5.6369e+00,  ...,  1.1911e+01,\n",
      "           3.6641e+00,  2.1104e+00],\n",
      "         [-1.1288e+01,  6.3395e-01, -7.2600e+00,  ...,  2.7503e+00,\n",
      "           1.5898e-02,  5.8348e+00],\n",
      "         [-1.0119e+01,  7.8805e-01,  9.8006e+00,  ...,  2.2148e+00,\n",
      "          -8.4746e+00, -5.9328e+00],\n",
      "         ...,\n",
      "         [-2.6173e+00,  3.8263e+00,  4.9663e+00,  ..., -1.4510e-01,\n",
      "           5.7367e-01, -5.1171e+00],\n",
      "         [-2.7997e+00,  5.5349e+00,  2.2690e+00,  ..., -6.8010e+00,\n",
      "           1.1825e+01,  6.0914e+00],\n",
      "         [ 2.7207e+00,  3.5142e+00,  2.5285e+00,  ...,  6.1345e+00,\n",
      "          -2.9840e+00,  4.6434e+00]]])\n",
      "Shape of Tensor:  torch.Size([29, 30, 100])\n"
     ]
    }
   ],
   "source": [
    "product = torch.matmul(tensor_a, \n",
    "                       tensor_b)\n",
    "print('Product of tensor_a and tensor_b: \\n' , product)\n",
    "\n",
    "print('Shape of Tensor: ' , product.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What is the difference between torch.matmul , torch.mm , torch.bmm , and torch.einsum , and the @ operator?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. torch.matmul() ->\n",
    "2. torch.mm() ->\n",
    "3. torch.bmm() ->\n",
    "4. torch.einsum() ->\n",
    "5. @ operator -> It is a shorthand for the torch.matmul() function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use torch.sum on the resulting tensor, passing the optional argument of dim=1 to sum across the 1st dimension. Before you run this, can you predict the size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Tensor across 1st Dimension: \n",
      " tensor([[-50.0316,  30.5396, -22.1811,  ..., -72.7584,  17.5020,  52.8702],\n",
      "        [-24.5906,  66.6769,  -3.0805,  ..., -51.3462,  15.9958,  17.6778],\n",
      "        [ -4.3321,  52.3295,  -0.1449,  ...,  38.0942,  11.8364, -38.1551],\n",
      "        ...,\n",
      "        [-17.1601,   8.4417, -27.3307,  ...,  21.3270, -37.3248,  24.2061],\n",
      "        [-38.0967,  67.0543,  42.0357,  ..., -42.0369, -36.2919,  -0.6370],\n",
      "        [-66.3404,  13.5168, -15.9936,  ...,   7.9241,  -7.6681,  40.2515]])\n",
      "Shape of Tensor:  torch.Size([29, 100])\n"
     ]
    }
   ],
   "source": [
    "tensor_sum = torch.sum(product, \n",
    "                       dim = 1)\n",
    "print('Sum of Tensor across 1st Dimension: \\n' , tensor_sum)\n",
    "\n",
    "print('Shape of Tensor: ' , tensor_sum.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create a new long tensor of size  (3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Tensor: \n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Updated Long Tensor: \n",
      " tensor([[2, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 4, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 6, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "long_tensor = torch.ones((3, 10), \n",
    "                         dtype = torch.long)\n",
    "print('Long Tensor: \\n' , long_tensor)\n",
    "\n",
    "long_tensor[0, 0] = 2\n",
    "long_tensor[1, 2] = 4\n",
    "long_tensor[2, 4] = 6\n",
    "\n",
    "print('Updated Long Tensor: \\n' , long_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Use this new long tensor to index into the tensor from step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed Tensor: \n",
      " tensor([[[[ -3.0986,   9.5812,   1.5876,  ...,   2.4863,   0.0659,   1.2564],\n",
      "          [ -8.7685,   0.2723,   1.4871,  ...,  -6.7436,   9.4754,  -0.8763],\n",
      "          [-11.6087,   4.7631,   3.8919,  ...,   4.0621,   2.9426,   8.9158],\n",
      "          ...,\n",
      "          [-10.9076,   2.3546,   2.4360,  ...,   4.4684,   3.0328,   4.2456],\n",
      "          [ -8.0787,   3.0227,   0.1128,  ...,  14.9838,  10.0184,  -9.0426],\n",
      "          [ -2.9087,  -7.6347,   3.0203,  ...,   2.4480,  -7.7035, -14.7399]],\n",
      "\n",
      "         [[  5.4222,  -6.4050,  -0.0955,  ..., -10.4389,  -6.7244,  -2.8128],\n",
      "          [  1.3937,   2.8340,  -5.8951,  ...,  -1.4838,   1.3206,   3.5743],\n",
      "          [ 10.2663,   5.1957,   6.6383,  ...,  -6.0728,   2.4461,  -3.6607],\n",
      "          ...,\n",
      "          [ -4.3729,   5.9844,   2.9867,  ...,  -9.2773,   2.5517,   5.6538],\n",
      "          [ -6.3688,   2.8970,   3.2849,  ...,   7.0410,   3.2560,  -3.8223],\n",
      "          [  0.6447,   5.0446,  11.4363,  ...,   3.5320,  -2.0670,  -2.0816]],\n",
      "\n",
      "         [[  5.4222,  -6.4050,  -0.0955,  ..., -10.4389,  -6.7244,  -2.8128],\n",
      "          [  1.3937,   2.8340,  -5.8951,  ...,  -1.4838,   1.3206,   3.5743],\n",
      "          [ 10.2663,   5.1957,   6.6383,  ...,  -6.0728,   2.4461,  -3.6607],\n",
      "          ...,\n",
      "          [ -4.3729,   5.9844,   2.9867,  ...,  -9.2773,   2.5517,   5.6538],\n",
      "          [ -6.3688,   2.8970,   3.2849,  ...,   7.0410,   3.2560,  -3.8223],\n",
      "          [  0.6447,   5.0446,  11.4363,  ...,   3.5320,  -2.0670,  -2.0816]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  5.4222,  -6.4050,  -0.0955,  ..., -10.4389,  -6.7244,  -2.8128],\n",
      "          [  1.3937,   2.8340,  -5.8951,  ...,  -1.4838,   1.3206,   3.5743],\n",
      "          [ 10.2663,   5.1957,   6.6383,  ...,  -6.0728,   2.4461,  -3.6607],\n",
      "          ...,\n",
      "          [ -4.3729,   5.9844,   2.9867,  ...,  -9.2773,   2.5517,   5.6538],\n",
      "          [ -6.3688,   2.8970,   3.2849,  ...,   7.0410,   3.2560,  -3.8223],\n",
      "          [  0.6447,   5.0446,  11.4363,  ...,   3.5320,  -2.0670,  -2.0816]],\n",
      "\n",
      "         [[  5.4222,  -6.4050,  -0.0955,  ..., -10.4389,  -6.7244,  -2.8128],\n",
      "          [  1.3937,   2.8340,  -5.8951,  ...,  -1.4838,   1.3206,   3.5743],\n",
      "          [ 10.2663,   5.1957,   6.6383,  ...,  -6.0728,   2.4461,  -3.6607],\n",
      "          ...,\n",
      "          [ -4.3729,   5.9844,   2.9867,  ...,  -9.2773,   2.5517,   5.6538],\n",
      "          [ -6.3688,   2.8970,   3.2849,  ...,   7.0410,   3.2560,  -3.8223],\n",
      "          [  0.6447,   5.0446,  11.4363,  ...,   3.5320,  -2.0670,  -2.0816]],\n",
      "\n",
      "         [[  5.4222,  -6.4050,  -0.0955,  ..., -10.4389,  -6.7244,  -2.8128],\n",
      "          [  1.3937,   2.8340,  -5.8951,  ...,  -1.4838,   1.3206,   3.5743],\n",
      "          [ 10.2663,   5.1957,   6.6383,  ...,  -6.0728,   2.4461,  -3.6607],\n",
      "          ...,\n",
      "          [ -4.3729,   5.9844,   2.9867,  ...,  -9.2773,   2.5517,   5.6538],\n",
      "          [ -6.3688,   2.8970,   3.2849,  ...,   7.0410,   3.2560,  -3.8223],\n",
      "          [  0.6447,   5.0446,  11.4363,  ...,   3.5320,  -2.0670,  -2.0816]]],\n",
      "\n",
      "\n",
      "        [[[  5.4222,  -6.4050,  -0.0955,  ..., -10.4389,  -6.7244,  -2.8128],\n",
      "          [  1.3937,   2.8340,  -5.8951,  ...,  -1.4838,   1.3206,   3.5743],\n",
      "          [ 10.2663,   5.1957,   6.6383,  ...,  -6.0728,   2.4461,  -3.6607],\n",
      "          ...,\n",
      "          [ -4.3729,   5.9844,   2.9867,  ...,  -9.2773,   2.5517,   5.6538],\n",
      "          [ -6.3688,   2.8970,   3.2849,  ...,   7.0410,   3.2560,  -3.8223],\n",
      "          [  0.6447,   5.0446,  11.4363,  ...,   3.5320,  -2.0670,  -2.0816]],\n",
      "\n",
      "         [[  5.4222,  -6.4050,  -0.0955,  ..., -10.4389,  -6.7244,  -2.8128],\n",
      "          [  1.3937,   2.8340,  -5.8951,  ...,  -1.4838,   1.3206,   3.5743],\n",
      "          [ 10.2663,   5.1957,   6.6383,  ...,  -6.0728,   2.4461,  -3.6607],\n",
      "          ...,\n",
      "          [ -4.3729,   5.9844,   2.9867,  ...,  -9.2773,   2.5517,   5.6538],\n",
      "          [ -6.3688,   2.8970,   3.2849,  ...,   7.0410,   3.2560,  -3.8223],\n",
      "          [  0.6447,   5.0446,  11.4363,  ...,   3.5320,  -2.0670,  -2.0816]],\n",
      "\n",
      "         [[ -8.6576,   4.6523,   6.3539,  ...,   4.0690,  -0.1426,  -9.9148],\n",
      "          [ -7.9730,   7.5935,   6.5238,  ...,  -5.4554,  -2.8149, -10.8683],\n",
      "          [ 10.3348,  -2.1619,   2.2174,  ...,   3.8396, -10.8498,  -7.0819],\n",
      "          ...,\n",
      "          [  2.2676,   0.8095,   5.2452,  ...,  -3.4539,  -4.5533,   6.6101],\n",
      "          [ -7.2370, -11.9520,  -3.4945,  ...,  13.8637,   2.5242,   6.3707],\n",
      "          [  8.3522,   2.1434,  -0.4317,  ...,   5.7091,   6.3067,   0.4033]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  5.4222,  -6.4050,  -0.0955,  ..., -10.4389,  -6.7244,  -2.8128],\n",
      "          [  1.3937,   2.8340,  -5.8951,  ...,  -1.4838,   1.3206,   3.5743],\n",
      "          [ 10.2663,   5.1957,   6.6383,  ...,  -6.0728,   2.4461,  -3.6607],\n",
      "          ...,\n",
      "          [ -4.3729,   5.9844,   2.9867,  ...,  -9.2773,   2.5517,   5.6538],\n",
      "          [ -6.3688,   2.8970,   3.2849,  ...,   7.0410,   3.2560,  -3.8223],\n",
      "          [  0.6447,   5.0446,  11.4363,  ...,   3.5320,  -2.0670,  -2.0816]],\n",
      "\n",
      "         [[  5.4222,  -6.4050,  -0.0955,  ..., -10.4389,  -6.7244,  -2.8128],\n",
      "          [  1.3937,   2.8340,  -5.8951,  ...,  -1.4838,   1.3206,   3.5743],\n",
      "          [ 10.2663,   5.1957,   6.6383,  ...,  -6.0728,   2.4461,  -3.6607],\n",
      "          ...,\n",
      "          [ -4.3729,   5.9844,   2.9867,  ...,  -9.2773,   2.5517,   5.6538],\n",
      "          [ -6.3688,   2.8970,   3.2849,  ...,   7.0410,   3.2560,  -3.8223],\n",
      "          [  0.6447,   5.0446,  11.4363,  ...,   3.5320,  -2.0670,  -2.0816]],\n",
      "\n",
      "         [[  5.4222,  -6.4050,  -0.0955,  ..., -10.4389,  -6.7244,  -2.8128],\n",
      "          [  1.3937,   2.8340,  -5.8951,  ...,  -1.4838,   1.3206,   3.5743],\n",
      "          [ 10.2663,   5.1957,   6.6383,  ...,  -6.0728,   2.4461,  -3.6607],\n",
      "          ...,\n",
      "          [ -4.3729,   5.9844,   2.9867,  ...,  -9.2773,   2.5517,   5.6538],\n",
      "          [ -6.3688,   2.8970,   3.2849,  ...,   7.0410,   3.2560,  -3.8223],\n",
      "          [  0.6447,   5.0446,  11.4363,  ...,   3.5320,  -2.0670,  -2.0816]]],\n",
      "\n",
      "\n",
      "        [[[  5.4222,  -6.4050,  -0.0955,  ..., -10.4389,  -6.7244,  -2.8128],\n",
      "          [  1.3937,   2.8340,  -5.8951,  ...,  -1.4838,   1.3206,   3.5743],\n",
      "          [ 10.2663,   5.1957,   6.6383,  ...,  -6.0728,   2.4461,  -3.6607],\n",
      "          ...,\n",
      "          [ -4.3729,   5.9844,   2.9867,  ...,  -9.2773,   2.5517,   5.6538],\n",
      "          [ -6.3688,   2.8970,   3.2849,  ...,   7.0410,   3.2560,  -3.8223],\n",
      "          [  0.6447,   5.0446,  11.4363,  ...,   3.5320,  -2.0670,  -2.0816]],\n",
      "\n",
      "         [[  5.4222,  -6.4050,  -0.0955,  ..., -10.4389,  -6.7244,  -2.8128],\n",
      "          [  1.3937,   2.8340,  -5.8951,  ...,  -1.4838,   1.3206,   3.5743],\n",
      "          [ 10.2663,   5.1957,   6.6383,  ...,  -6.0728,   2.4461,  -3.6607],\n",
      "          ...,\n",
      "          [ -4.3729,   5.9844,   2.9867,  ...,  -9.2773,   2.5517,   5.6538],\n",
      "          [ -6.3688,   2.8970,   3.2849,  ...,   7.0410,   3.2560,  -3.8223],\n",
      "          [  0.6447,   5.0446,  11.4363,  ...,   3.5320,  -2.0670,  -2.0816]],\n",
      "\n",
      "         [[  5.4222,  -6.4050,  -0.0955,  ..., -10.4389,  -6.7244,  -2.8128],\n",
      "          [  1.3937,   2.8340,  -5.8951,  ...,  -1.4838,   1.3206,   3.5743],\n",
      "          [ 10.2663,   5.1957,   6.6383,  ...,  -6.0728,   2.4461,  -3.6607],\n",
      "          ...,\n",
      "          [ -4.3729,   5.9844,   2.9867,  ...,  -9.2773,   2.5517,   5.6538],\n",
      "          [ -6.3688,   2.8970,   3.2849,  ...,   7.0410,   3.2560,  -3.8223],\n",
      "          [  0.6447,   5.0446,  11.4363,  ...,   3.5320,  -2.0670,  -2.0816]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  5.4222,  -6.4050,  -0.0955,  ..., -10.4389,  -6.7244,  -2.8128],\n",
      "          [  1.3937,   2.8340,  -5.8951,  ...,  -1.4838,   1.3206,   3.5743],\n",
      "          [ 10.2663,   5.1957,   6.6383,  ...,  -6.0728,   2.4461,  -3.6607],\n",
      "          ...,\n",
      "          [ -4.3729,   5.9844,   2.9867,  ...,  -9.2773,   2.5517,   5.6538],\n",
      "          [ -6.3688,   2.8970,   3.2849,  ...,   7.0410,   3.2560,  -3.8223],\n",
      "          [  0.6447,   5.0446,  11.4363,  ...,   3.5320,  -2.0670,  -2.0816]],\n",
      "\n",
      "         [[  5.4222,  -6.4050,  -0.0955,  ..., -10.4389,  -6.7244,  -2.8128],\n",
      "          [  1.3937,   2.8340,  -5.8951,  ...,  -1.4838,   1.3206,   3.5743],\n",
      "          [ 10.2663,   5.1957,   6.6383,  ...,  -6.0728,   2.4461,  -3.6607],\n",
      "          ...,\n",
      "          [ -4.3729,   5.9844,   2.9867,  ...,  -9.2773,   2.5517,   5.6538],\n",
      "          [ -6.3688,   2.8970,   3.2849,  ...,   7.0410,   3.2560,  -3.8223],\n",
      "          [  0.6447,   5.0446,  11.4363,  ...,   3.5320,  -2.0670,  -2.0816]],\n",
      "\n",
      "         [[  5.4222,  -6.4050,  -0.0955,  ..., -10.4389,  -6.7244,  -2.8128],\n",
      "          [  1.3937,   2.8340,  -5.8951,  ...,  -1.4838,   1.3206,   3.5743],\n",
      "          [ 10.2663,   5.1957,   6.6383,  ...,  -6.0728,   2.4461,  -3.6607],\n",
      "          ...,\n",
      "          [ -4.3729,   5.9844,   2.9867,  ...,  -9.2773,   2.5517,   5.6538],\n",
      "          [ -6.3688,   2.8970,   3.2849,  ...,   7.0410,   3.2560,  -3.8223],\n",
      "          [  0.6447,   5.0446,  11.4363,  ...,   3.5320,  -2.0670,  -2.0816]]]])\n",
      "Shape of Tensor:  torch.Size([3, 10, 30, 100])\n"
     ]
    }
   ],
   "source": [
    "indexed_tensor = product[long_tensor]\n",
    "print('Indexed Tensor: \\n' , indexed_tensor)\n",
    "\n",
    "print('Shape of Tensor: ' , indexed_tensor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Use  torch.mean  to average across the last dimension in the tensor from step 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3231,  0.2444, -0.5558, -0.2634,  0.3927,  0.0657,  0.5092,\n",
      "          -0.6199,  0.1039,  0.0898, -0.3455, -0.5221,  0.4114,  0.0585,\n",
      "           0.8071, -0.5141, -0.4428, -0.6532,  0.2653,  0.0141, -0.0559,\n",
      "           0.5494,  0.7992, -0.3725, -0.3948,  1.1148,  0.2472,  0.3472,\n",
      "          -0.5406,  0.2302],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760]],\n",
      "\n",
      "        [[-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [ 0.2241,  0.1053,  0.4631, -0.3674, -0.2609, -0.1406,  0.0030,\n",
      "          -0.5816, -0.0574,  0.2809,  0.1724,  0.6834, -0.0937, -0.6188,\n",
      "           0.1134, -0.3936, -0.0116, -0.0335,  0.4863, -0.3760,  0.5010,\n",
      "           1.0815,  0.6378,  0.2514,  0.0130, -0.1737, -0.7360, -0.1636,\n",
      "          -0.0152,  0.1068],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760]],\n",
      "\n",
      "        [[-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [ 0.7212, -0.2534, -0.0636,  0.1349, -0.8995,  0.5718, -0.1219,\n",
      "          -0.0409, -0.1523,  0.0222,  0.0309,  0.3786,  0.3327,  0.1424,\n",
      "           0.5789, -0.2864, -0.0746, -1.2658,  0.2252, -0.0487, -0.5862,\n",
      "           0.6564,  0.6894, -0.2294, -0.6183,  0.3193, -0.6155, -0.3624,\n",
      "          -0.2382,  0.5942],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760],\n",
      "         [-0.5990, -0.1416,  0.0622, -0.5265,  0.0953,  0.3992, -0.0054,\n",
      "           0.2694,  0.6207,  0.0644,  0.6903, -1.0322, -0.4999,  0.1124,\n",
      "           0.5429, -0.1972, -0.3976, -0.8685,  0.0675, -1.0969,  0.3917,\n",
      "          -1.0370, -0.9075, -0.1823,  0.6936,  0.1614, -0.8640, -0.2586,\n",
      "           0.3212,  0.0760]]])\n",
      "Shape of Tensor:  torch.Size([3, 10, 30])\n"
     ]
    }
   ],
   "source": [
    "mean_tensor = torch.mean(indexed_tensor, \n",
    "                         dim = 3)\n",
    "print(mean_tensor)\n",
    "\n",
    "print('Shape of Tensor: ' , mean_tensor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Redo step 2. on the GPU and compare results from step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product of tensor_a and tensor_b on GPU: \n",
      " tensor([[[-1.2935e+01,  1.0707e+01,  2.3989e+00,  ..., -1.2342e+01,\n",
      "           1.3075e+00,  3.2499e+00],\n",
      "         [ 2.3836e+00, -7.7838e+00, -6.2557e+00,  ...,  3.4082e+00,\n",
      "           5.9395e+00,  7.6136e+00],\n",
      "         [ 6.9409e+00, -6.9336e-01, -1.8721e+00,  ...,  1.6076e+01,\n",
      "          -1.9905e+00,  7.5179e+00],\n",
      "         ...,\n",
      "         [-1.6975e+00,  6.7376e+00, -1.0056e+00,  ..., -1.6540e+01,\n",
      "           1.4331e+01,  4.9957e+00],\n",
      "         [-4.5444e+00,  7.5032e+00,  2.5883e+00,  ..., -9.4577e-01,\n",
      "          -2.1814e+00,  4.5546e+00],\n",
      "         [-2.6276e+00,  3.1530e+00,  8.3507e+00,  ..., -1.6448e+01,\n",
      "           2.5821e+00, -3.0676e+00]],\n",
      "\n",
      "        [[ 5.4222e+00, -6.4050e+00, -9.5461e-02,  ..., -1.0439e+01,\n",
      "          -6.7244e+00, -2.8128e+00],\n",
      "         [ 1.3937e+00,  2.8340e+00, -5.8951e+00,  ..., -1.4838e+00,\n",
      "           1.3206e+00,  3.5743e+00],\n",
      "         [ 1.0266e+01,  5.1957e+00,  6.6383e+00,  ..., -6.0728e+00,\n",
      "           2.4461e+00, -3.6607e+00],\n",
      "         ...,\n",
      "         [-4.3729e+00,  5.9844e+00,  2.9867e+00,  ..., -9.2773e+00,\n",
      "           2.5517e+00,  5.6538e+00],\n",
      "         [-6.3688e+00,  2.8970e+00,  3.2849e+00,  ...,  7.0410e+00,\n",
      "           3.2560e+00, -3.8223e+00],\n",
      "         [ 6.4466e-01,  5.0446e+00,  1.1436e+01,  ...,  3.5320e+00,\n",
      "          -2.0670e+00, -2.0816e+00]],\n",
      "\n",
      "        [[-3.0986e+00,  9.5812e+00,  1.5876e+00,  ...,  2.4863e+00,\n",
      "           6.5920e-02,  1.2564e+00],\n",
      "         [-8.7685e+00,  2.7233e-01,  1.4871e+00,  ..., -6.7436e+00,\n",
      "           9.4754e+00, -8.7629e-01],\n",
      "         [-1.1609e+01,  4.7631e+00,  3.8919e+00,  ...,  4.0621e+00,\n",
      "           2.9426e+00,  8.9158e+00],\n",
      "         ...,\n",
      "         [-1.0908e+01,  2.3546e+00,  2.4360e+00,  ...,  4.4684e+00,\n",
      "           3.0328e+00,  4.2456e+00],\n",
      "         [-8.0787e+00,  3.0227e+00,  1.1281e-01,  ...,  1.4984e+01,\n",
      "           1.0018e+01, -9.0426e+00],\n",
      "         [-2.9087e+00, -7.6347e+00,  3.0203e+00,  ...,  2.4480e+00,\n",
      "          -7.7035e+00, -1.4740e+01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-8.1788e+00,  8.8674e+00, -7.3693e-01,  ...,  2.5902e+00,\n",
      "           2.7675e+00,  7.7466e+00],\n",
      "         [-5.1496e+00,  3.5086e+00, -3.5146e+00,  ...,  2.4844e+00,\n",
      "           2.2639e+00,  1.7528e-01],\n",
      "         [ 6.1902e+00,  2.0335e+00,  6.4638e-01,  ..., -5.7800e+00,\n",
      "          -1.3023e+00, -3.7936e-01],\n",
      "         ...,\n",
      "         [ 9.0362e-01,  1.5561e+00,  1.2579e+01,  ...,  6.4267e+00,\n",
      "          -2.2734e+00, -6.3688e+00],\n",
      "         [-2.3345e+00, -4.2633e+00, -1.0061e+01,  ...,  8.4776e+00,\n",
      "           2.2912e+00,  5.8614e+00],\n",
      "         [ 9.4220e+00, -1.7101e+00,  7.8408e+00,  ..., -3.0075e+00,\n",
      "          -8.9621e+00,  5.7401e+00]],\n",
      "\n",
      "        [[ 9.5353e+00,  2.8743e+00,  4.6909e+00,  ..., -7.6369e-01,\n",
      "          -2.3512e+00, -4.0370e+00],\n",
      "         [-8.8055e+00,  1.0815e+01,  9.5747e-01,  ...,  8.2482e-01,\n",
      "           4.7813e+00,  3.0568e+00],\n",
      "         [-5.9513e+00,  2.2977e+00,  7.5078e+00,  ..., -5.1231e+00,\n",
      "          -3.2958e+00, -4.1434e+00],\n",
      "         ...,\n",
      "         [-6.8262e+00,  1.4623e+01,  7.9500e+00,  ..., -5.9183e+00,\n",
      "          -2.9453e+00,  2.5687e+00],\n",
      "         [-2.4890e+00,  6.9971e+00, -1.2709e+00,  ..., -3.4619e+00,\n",
      "           7.2219e-01,  7.5046e+00],\n",
      "         [-1.2298e+01, -5.8897e+00,  5.8404e+00,  ..., -3.3862e+00,\n",
      "           4.3833e+00, -6.5206e+00]],\n",
      "\n",
      "        [[ 5.2547e+00, -7.9891e+00, -5.6369e+00,  ...,  1.1911e+01,\n",
      "           3.6641e+00,  2.1104e+00],\n",
      "         [-1.1288e+01,  6.3395e-01, -7.2600e+00,  ...,  2.7503e+00,\n",
      "           1.5898e-02,  5.8348e+00],\n",
      "         [-1.0119e+01,  7.8805e-01,  9.8006e+00,  ...,  2.2148e+00,\n",
      "          -8.4746e+00, -5.9328e+00],\n",
      "         ...,\n",
      "         [-2.6173e+00,  3.8263e+00,  4.9663e+00,  ..., -1.4510e-01,\n",
      "           5.7367e-01, -5.1171e+00],\n",
      "         [-2.7997e+00,  5.5349e+00,  2.2690e+00,  ..., -6.8010e+00,\n",
      "           1.1825e+01,  6.0914e+00],\n",
      "         [ 2.7207e+00,  3.5142e+00,  2.5285e+00,  ...,  6.1345e+00,\n",
      "          -2.9840e+00,  4.6434e+00]]])\n",
      "Shape of Tensor:  torch.Size([29, 30, 100])\n"
     ]
    }
   ],
   "source": [
    "tensor_a_cuda = tensor_a.to(device = device)\n",
    "tensor_b_cuda = tensor_b.to(device = device)\n",
    "product_gpu = torch.matmul(tensor_a, tensor_b)\n",
    "print('Product of tensor_a and tensor_b on GPU: \\n' , product_gpu)\n",
    "\n",
    "print('Shape of Tensor: ' , product_gpu.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Write a pure PyTorch program to compute the value of $\\sqrt{2}$ up to 4 decimal places without using the square root or other math functions from any of the libraries. \n",
    "### Hint: Notice that the answer is the (positive) root of the equation, $$𝑥^2 −2 = 0$$ \n",
    "### To find the root, you might want to use \"Newton's Method\": $$𝑥_{𝑛+1} = 𝑥_{𝑛} − \\frac{𝑓(𝑥)}{𝑓′(𝑥)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fail-fast prototyping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building neural networks, you want things to either work or fail fast. Long iteration loops are \n",
    "the worst enemy of a machine learning practitioner. \\\n",
    "For e.g., while writing code, you might want to incrementally test your code by doing something \n",
    "like this:\n",
    "\n",
    "batch_size = 32 \\\n",
    "num_features = 512 \\\n",
    "embedding_size = 16\n",
    "\n",
    "\\# construct a dummy input \\\n",
    "x = torch.randn(batch_size, num_features)\n",
    "\n",
    "\\# we want to project the input to embedding_size \\\n",
    "fc = torch.nn.Linear(num_features, embedding_size)\n",
    "\n",
    "\\# test if that works \\\n",
    "print(fc(x).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fail-fast exercises"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. [Glove](https://nlp.stanford.edu/projects/glove/) has 300 dimension embeddings. Design an nn.Module that takes a sentence of max_len words, tokenizes words by spaces, represents the sentence by averaging the glove embeddings of constituent words. What is the shape of the resulting sentence embedding? When you implement this, you will need to make some assumptions. What are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2196017 words present in GloVe\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe Embeddings\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "GLOVE_DIM = 300\n",
    "glove = GloVe(name = '840B', \n",
    "              dim = GLOVE_DIM)\n",
    "\n",
    "print(f'Loaded {len(glove.itos)} words present in GloVe')\n",
    "\n",
    "embeddings_tensor = glove.vectors\n",
    "embeddings_tensor = embeddings_tensor.to(device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "NUM_SENT = 512\n",
    "sents = list()\n",
    "for i in range(NUM_SENT):\n",
    "    sents.append('This is the quest zero and it has a deadline this Sunday March 29')\n",
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 300])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GloveEmbeddingAvg(nn.Module):\n",
    "    \n",
    "    def __init__(self, max_len):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings_tensor)\n",
    "        \n",
    "    def forward(self, \n",
    "                sent):\n",
    "        # Tokenize the sentence by spaces\n",
    "        tokens = sent.split(' ')[:self.max_len]\n",
    "        # Get idx of each token from the GloVe dictionary\n",
    "        glove_dict_indexes = [glove.stoi[token] for token in tokens]\n",
    "        # Convert it into Tensor\n",
    "        glove_dict_indexes = torch.tensor(glove_dict_indexes, \n",
    "                                          device = device)\n",
    "        # Get Word Embeddings for all tokens\n",
    "        word_embeds = self.embedding(glove_dict_indexes)\n",
    "        # Sentence Embedding = Average of Word Embeddings\n",
    "        sent_embeds = word_embeds.mean(dim = 0)\n",
    "        # Reshape Sentence Embedding as a 2D Tensor\n",
    "        return sent_embeds.view(1, -1)\n",
    "\n",
    "MAX_LEN = 10\n",
    "glove_embeds_avg = GloveEmbeddingAvg(MAX_LEN).to(device)\n",
    "\n",
    "print(glove_embeds_avg(sents[0]).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How will you modify step 1. so that the sentence embeddings are in $R^{50}$ ?\n",
    "BONUS: Can you think of more than one way to do this? What are the implications of each method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveEmbeddingAvg_50_Dim(nn.Module):\n",
    "    \n",
    "    def __init__(self, max_len):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings_tensor)\n",
    "        self.fc = nn.Linear(GLOVE_DIM, 50)\n",
    "        \n",
    "    def forward(self, \n",
    "                x):\n",
    "        # Slice each sentence to Max Length\n",
    "        x = x[:, :self.max_len]\n",
    "        # Get Word Embeddings for all tokens\n",
    "        word_embeds = self.embedding(x) # [BATCH_SIZE, MAX_LEN, GLOVE_DIM]\n",
    "        # Sentence Embedding = Average of Word Embeddings\n",
    "        sent_embeds = word_embeds.mean(dim = 0) # [MAX_LEN, GLOVE_DIM]\n",
    "        # Linear Layer to reduce Sentence Embedding Dimension to 50\n",
    "        return self.fc(sent_embeds) # [MAX_LEN, 50]\n",
    "\n",
    "MAX_LEN = 10\n",
    "glove_embeds_avg_50_dim = GloveEmbeddingAvg_50_Dim(MAX_LEN).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Quickly test your answer in step 2. with a batch of 512 sentences on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 50])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a Sentence\n",
    "def tokenize(sent):\n",
    "    # Tokenize the sentence by spaces\n",
    "    tokens = sent.split(' ')\n",
    "    # Get idx of each token from the GloVe dictionary\n",
    "    glove_dict_indexes = [glove.stoi[token] for token in tokens]\n",
    "    return glove_dict_indexes\n",
    "\n",
    "# Create Tokenized Sentence Corpus\n",
    "tokenized_sents = list()\n",
    "for sent in sents:\n",
    "    tokenized_sents.append(tokenize(sent))\n",
    "tokenized_sents = torch.tensor(tokenized_sents, \n",
    "                               device = device)\n",
    "\n",
    "# Run forward pass\n",
    "BATCH_SIZE = 512\n",
    "for i in range(0, len(tokenized_sents), BATCH_SIZE):\n",
    "    batch = tokenized_sents[i:i+BATCH_SIZE]\n",
    "    sentence_embeddings = glove_embeds_avg_50_dim(batch)\n",
    "    print(sentence_embeddings.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You almost implemented the model in the Deep Averaging Networks (DAN) paper!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Task: \n",
    "### Create a   MultiEmbedding  Module that can take two sets of indices, embed them, and concat the results. You might remember it from the previous lecture where we had to produce an embedding for \"green apple\" from embeddings of \"green\" and \"apple\". Your  MultiEmbedding class should work with the following test code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_emb, \n",
    "                 size_emb1, \n",
    "                 size_emb2):\n",
    "        super().__init__()\n",
    "        self.embedding_A = nn.Embedding(num_emb, size_emb1)\n",
    "        self.embedding_B = nn.Embedding(num_emb, size_emb2)\n",
    "        \n",
    "    def forward(self, \n",
    "                indices1, \n",
    "                indices2):\n",
    "        embed_A = self.embedding_A(indices1)\n",
    "        embed_B = self.embedding_B(indices2)\n",
    "        # Concatenate the Embeddings\n",
    "        return torch.cat((embed_A, embed_B), \n",
    "                         dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10, 500])\n"
     ]
    }
   ],
   "source": [
    "# Test code: instantiate a MultiEmbedding with the sizes for each embedding. \n",
    "# For this example, you can just randomly initialize each interior embedding. \n",
    "# In a practical setting, you might support methods for initializing with \n",
    "# combinations of embeddings, such as GloVe 300d vectors and word2vec 200d \n",
    "# vectors, yielding 500d embeddings. Both embeddings share a vocabulary/range \n",
    "# of supported indices indicated by `num_emb`\n",
    "\n",
    "NUM_EMB = 10000\n",
    "SIZE_EMB1 = 300\n",
    "SIZE_EMB2 = 200\n",
    "BATCH_SIZE = 64\n",
    "NUM_LENGTH = 10\n",
    "\n",
    "multiemb = MultiEmbedding(NUM_EMB, \n",
    "                          SIZE_EMB1, \n",
    "                          SIZE_EMB2).to(device)\n",
    "\n",
    "# You can then call this with a pair of indices where each value is in 0 <= i < num_emb\n",
    "indices1 =  torch.randint(0, \n",
    "                          NUM_EMB, \n",
    "                          (BATCH_SIZE, NUM_LENGTH), \n",
    "                          dtype = torch.long, \n",
    "                          device = device) # long tensor of shape (batch, num_length)\n",
    "indices2 =  torch.randint(0, \n",
    "                          NUM_EMB, \n",
    "                          (BATCH_SIZE, NUM_LENGTH), \n",
    "                          dtype = torch.long, \n",
    "                          device = device) # long tensor of shape (batch, num_length)\n",
    "output = multiemb(indices1, \n",
    "                  indices2)\n",
    "print(output.shape) # should be (batch, num_length, size_emb1 + size_emb2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Datasets and DataLoaders: \n",
    "### Read this short post on PyTorch Dataset and DataLoaders. Often in prototyping we need to generate dummy datasets to test our models. Implement a PyTorch Dataset class that generates up to  num_sentences  random sentences of length up to  max_len words. For each sentence, generate a binary label. You should be able to test your code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b37beb76676c8b0643f5764b5c5ae0ddf876ecbab29b433e279cae2d82963c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
