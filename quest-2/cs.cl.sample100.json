{"id":"cs/0407026","submitter":"Atsushi Fujii","authors":"Atsushi Fujii, Tetsuya Ishikawa","title":"Summarizing Encyclopedic Term Descriptions on the Web","comments":"7 pages, Proceedings of the 20th International Conference on\n  Computational Linguistics (to appear)","journal-ref":"Proceedings of the 20th International Conference on Computational\n  Linguistics (COLING 2004), pp.645-651, Aug. 2004","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  We are developing an automatic method to compile an encyclopedic corpus from\nthe Web. In our previous work, paragraph-style descriptions for a term are\nextracted from Web pages and organized based on domains. However, these\ndescriptions are independent and do not comprise a condensed text as in\nhand-crafted encyclopedias. To resolve this problem, we propose a summarization\nmethod, which produces a single text from multiple descriptions. The resultant\nsummary concisely describes a term from different viewpoints. We also show the\neffectiveness of our method by means of experiments.\n","versions":[{"version":"v1","created":"Sat, 10 Jul 2004 11:18:42 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Fujii","Atsushi",""],["Ishikawa","Tetsuya",""]]}
{"id":"cs/0407027","submitter":"Atsushi Fujii","authors":"Atsushi Fujii, Katunobu Itou, Tomoyosi Akiba, Tetsuya Ishikawa","title":"Unsupervised Topic Adaptation for Lecture Speech Retrieval","comments":"4 pages, Proceedings of the 8th International Conference on Spoken\n  Language Processing (to appear)","journal-ref":"Proceedings of the 8th International Conference on Spoken Language\n  Processing (ICSLP 2004), pp.2957-2960, Oct. 2004","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  We are developing a cross-media information retrieval system, in which users\ncan view specific segments of lecture videos by submitting text queries. To\nproduce a text index, the audio track is extracted from a lecture video and a\ntranscription is generated by automatic speech recognition. In this paper, to\nimprove the quality of our retrieval system, we extensively investigate the\neffects of adapting acoustic and language models on speech recognition. We\nperform an MLLR-based method to adapt an acoustic model. To obtain a corpus for\nlanguage model adaptation, we use the textbook for a target lecture to search a\nWeb collection for the pages associated with the lecture topic. We show the\neffectiveness of our method by means of experiments.\n","versions":[{"version":"v1","created":"Sat, 10 Jul 2004 11:45:57 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Fujii","Atsushi",""],["Itou","Katunobu",""],["Akiba","Tomoyosi",""],["Ishikawa","Tetsuya",""]]}
{"id":"cs/0407028","submitter":"Atsushi Fujii","authors":"Tomoyosi Akiba, Atsushi Fujii, Katunobu Itou","title":"Effects of Language Modeling on Speech-driven Question Answering","comments":"4 pages, Proceedings of the 8th International Conference on Spoken\n  Language Processing (to appear)","journal-ref":"Proceedings of the 8th International Conference on Spoken Language\n  Processing (ICSLP 2004), pp.1053-1056, Oct. 2004","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  We integrate automatic speech recognition (ASR) and question answering (QA)\nto realize a speech-driven QA system, and evaluate its performance. We adapt an\nN-gram language model to natural language questions, so that the input of our\nsystem can be recognized with a high accuracy. We target WH-questions which\nconsist of the topic part and fixed phrase used to ask about something. We\nfirst produce a general N-gram model intended to recognize the topic and\nemphasize the counts of the N-grams that correspond to the fixed phrases. Given\na transcription by the ASR engine, the QA engine extracts the answer candidates\nfrom target documents. We propose a passage retrieval method robust against\nrecognition errors in the transcription. We use the QA test collection produced\nin NTCIR, which is a TREC-style evaluation workshop, and show the effectiveness\nof our method by means of experiments.\n","versions":[{"version":"v1","created":"Sat, 10 Jul 2004 11:57:17 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Akiba","Tomoyosi",""],["Fujii","Atsushi",""],["Itou","Katunobu",""]]}
{"id":"cs/0407046","submitter":"Wojciech Skut","authors":"Wojciech Skut, Stefan Ulrich and Kathrine Hammervold","title":"A Bimachine Compiler for Ranked Tagging Rules","comments":"7 pages, 3 figures Proceedings of COLING 2004 (to appear)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  This paper describes a novel method of compiling ranked tagging rules into a\ndeterministic finite-state device called a bimachine. The rules are formulated\nin the framework of regular rewrite operations and allow unrestricted regular\nexpressions in both left and right rule contexts. The compiler is illustrated\nby an application within a speech synthesis system.\n","versions":[{"version":"v1","created":"Mon, 19 Jul 2004 11:57:42 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Skut","Wojciech",""],["Ulrich","Stefan",""],["Hammervold","Kathrine",""]]}
{"id":"cs/0408052","submitter":"Daniel Yacob","authors":"Daniel Yacob","title":"Application of the Double Metaphone Algorithm to Amharic Orthography","comments":"International Conference of Ethiopian Studies XV, 13 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  The Metaphone algorithm applies the phonetic encoding of orthographic\nsequences to simplify words prior to comparison. While Metaphone has been\nhighly successful for the English language, for which it was designed, it may\nnot be applied directly to Ethiopian languages. The paper details how the\nprinciples of Metaphone can be applied to Ethiopic script and uses Amharic as a\ncase study. Match results improve as specific considerations are made for\nAmharic writing practices. Results are shown to improve further when common\nerrors from Amharic input methods are considered.\n","versions":[{"version":"v1","created":"Sun, 22 Aug 2004 19:32:48 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Yacob","Daniel",""]]}
{"id":"cs/0408059","submitter":"Aristides Vagelatos","authors":"Ch. Tsalidis (1), G. Orphanos (1), A. Iordanidou (2), A. Vagelatos (3)\n  ((1) Neurosoft S.A. (2) Patra's University, (3) RACTI)","title":"Proofing Tools Technology at Neurosoft S.A.","comments":"Workshop on International Proofing Tools and Language Technologies\n  July 1-2, 2004, Patras, Greece","journal-ref":null,"doi":null,"report-no":"CTI T.R.: 2004.06.01","categories":"cs.CL","license":null,"abstract":"  The aim of this paper is to present the R&D activities carried out at\nNeurosoft S.A. regarding the development of proofing tools for Modern Greek.\nFirstly, we focus on infrastructure issues that we faced during our initial\nsteps. Subsequently, we describe the most important insights of three proofing\ntools developed by Neurosoft, i.e. the spelling checker, the hyphenator and the\nthesaurus, outlining their efficiencies and inefficiencies. Finally, we discuss\nsome improvement ideas and give our future directions.\n","versions":[{"version":"v1","created":"Thu, 26 Aug 2004 10:50:45 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Tsalidis","Ch.","","Neurosoft S.A"],["Orphanos","G.","","Neurosoft S.A"],["Iordanidou","A.","","Patra's University"],["Vagelatos","A.","","RACTI"]]}
{"id":"cs/0408060","submitter":"Docteur Francois Trouilleux","authors":"Gabriel G. Bes (GRIL), Lionel Lamadon (GRIL), Francois Trouilleux\n  (GRIL)","title":"Verbal chunk extraction in French using limited resources","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  A way of extracting French verbal chunks, inflected and infinitive, is\nexplored and tested on effective corpus. Declarative morphological and local\ngrammar rules specifying chunks and some simple contextual structures are used,\nrelying on limited lexical information and some simple heuristic/statistic\nproperties obtained from restricted corpora. The specific goals, the\narchitecture and the formalism of the system, the linguistic information on\nwhich it relies and the obtained results on effective corpus are presented.\n","versions":[{"version":"v1","created":"Thu, 26 Aug 2004 12:44:15 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Bes","Gabriel G.","","GRIL"],["Lamadon","Lionel","","GRIL"],["Trouilleux","Francois","","GRIL"]]}
{"id":"cs/0408061","submitter":"Aristides Vagelatos","authors":"Ch. Tsalidis (1), A. Vagelatos (2) and G. Orphanos (1) ((1) Neurosoft\n  S.A. (2) RACTI)","title":"An electronic dictionary as a basis for NLP tools: The Greek case","comments":"Traitement Automatique des Langues Naturelles (TALN) 2004, Fez,\n  Morocco","journal-ref":null,"doi":null,"report-no":"CTI T.R.: 2004.04.03","categories":"cs.CL","license":null,"abstract":"  The existence of a Dictionary in electronic form for Modern Greek (MG) is\nmandatory if one is to process MG at the morphological and syntactic levels\nsince MG is a highly inflectional language with marked stress and a spelling\nsystem with many characteristics carried over from Ancient Greek. Moreover,\nsuch a tool becomes necessary if one is to create efficient and sophisticated\nNLP applications with substantial linguistic backing and coverage. The present\npaper will focus on the deployment of such an electronic dictionary for Modern\nGreek, which was built in two phases: first it was constructed to be the basis\nfor a spelling correction schema and then it was reconstructed in order to\nbecome the platform for the deployment of a wider spectrum of NLP tools.\n","versions":[{"version":"v1","created":"Thu, 26 Aug 2004 13:17:38 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Tsalidis","Ch.",""],["Vagelatos","A.",""],["Orphanos","G.",""]]}
{"id":"cs/0409008","submitter":"Hendrik Feddes","authors":"Lea Cyrus and Hendrik Feddes","title":"A Model for Fine-Grained Alignment of Multilingual Texts","comments":"8 pages, 4 figures","journal-ref":"Proc. COLING 2004 Workshop on Multilingual Linguistic Resources\n  (MLR2004), Geneva, August 28, 2004, pp. 15-22","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  While alignment of texts on the sentential level is often seen as being too\ncoarse, and word alignment as being too fine-grained, bi- or multilingual texts\nwhich are aligned on a level in-between are a useful resource for many\npurposes. Starting from a number of examples of non-literal translations, which\ntend to make alignment difficult, we describe an alignment model which copes\nwith these cases by explicitly coding them. The model is based on\npredicate-argument structures and thus covers the middle ground between\nsentence and word alignment. The model is currently used in a recently\ninitiated project of a parallel English-German treebank (FuSe), which can in\nprinciple be extended with additional languages.\n","versions":[{"version":"v1","created":"Tue, 7 Sep 2004 13:46:50 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Cyrus","Lea",""],["Feddes","Hendrik",""]]}
{"id":"cs/0409058","submitter":"Lillian Lee","authors":"Bo Pang and Lillian Lee","title":"A Sentimental Education: Sentiment Analysis Using Subjectivity\n  Summarization Based on Minimum Cuts","comments":"Data available at\n  http://www.cs.cornell.edu/people/pabo/movie-review-data/","journal-ref":"Proceedings of the 42nd ACL, pp. 271--278, 2004","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  Sentiment analysis seeks to identify the viewpoint(s) underlying a text span;\nan example application is classifying a movie review as \"thumbs up\" or \"thumbs\ndown\". To determine this sentiment polarity, we propose a novel\nmachine-learning method that applies text-categorization techniques to just the\nsubjective portions of the document. Extracting these portions can be\nimplemented using efficient techniques for finding minimum cuts in graphs; this\ngreatly facilitates incorporation of cross-sentence contextual constraints.\n","versions":[{"version":"v1","created":"Wed, 29 Sep 2004 20:34:04 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Pang","Bo",""],["Lee","Lillian",""]]}
{"id":"cs/0412015","submitter":"Detlef Prescher","authors":"Detlef Prescher","title":"A Tutorial on the Expectation-Maximization Algorithm Including\n  Maximum-Likelihood Estimation and EM Training of Probabilistic Context-Free\n  Grammars","comments":"Presented at the 15th European Summer School in Logic, Language and\n  Information (ESSLLI 2003). Example 5 extended (and partially corrected)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  The paper gives a brief review of the expectation-maximization algorithm\n(Dempster 1977) in the comprehensible framework of discrete mathematics. In\nSection 2, two prominent estimation methods, the relative-frequency estimation\nand the maximum-likelihood estimation are presented. Section 3 is dedicated to\nthe expectation-maximization algorithm and a simpler variant, the generalized\nexpectation-maximization algorithm. In Section 4, two loaded dice are rolled. A\nmore interesting example is presented in Section 5: The estimation of\nprobabilistic context-free grammars.\n","versions":[{"version":"v1","created":"Fri, 3 Dec 2004 17:10:17 GMT"},{"version":"v2","created":"Fri, 11 Mar 2005 14:05:04 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Prescher","Detlef",""]]}
{"id":"cs/0412016","submitter":"Detlef Prescher","authors":"Detlef Prescher","title":"Inside-Outside Estimation Meets Dynamic EM","comments":"4 pages, some typos corrected","journal-ref":"Proceedings of IWPT 2001","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  We briefly review the inside-outside and EM algorithm for probabilistic\ncontext-free grammars. As a result, we formally prove that inside-outside\nestimation is a dynamic-programming variant of EM. This is interesting in its\nown right, but even more when considered in a theoretical context since the\nwell-known convergence behavior of inside-outside estimation has been confirmed\nby many experiments but apparently has never been formally proved. However,\nbeing a version of EM, inside-outside estimation also inherits the good\nconvergence behavior of EM. Therefore, the as yet imperfect line of\nargumentation can be transformed into a coherent proof.\n","versions":[{"version":"v1","created":"Fri, 3 Dec 2004 18:10:17 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Prescher","Detlef",""]]}
{"id":"cs/0412114","submitter":"Pierre Andrews","authors":"Martin Rajman and Martin Vesely and Pierre Andrews","title":"State of the Art, Evaluation and Recommendations regarding \"Document\n  Processing and Visualization Techniques\"","comments":"54 pages, Report of Working Group 1 for the European Network of\n  Excellence (NoE) in Text Mining and its Applications in Statistics (NEMIS)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  Several Networks of Excellence have been set up in the framework of the\nEuropean FP5 research program. Among these Networks of Excellence, the NEMIS\nproject focuses on the field of Text Mining.\n  Within this field, document processing and visualization was identified as\none of the key topics and the WG1 working group was created in the NEMIS\nproject, to carry out a detailed survey of techniques associated with the text\nmining process and to identify the relevant research topics in related research\nareas.\n  In this document we present the results of this comprehensive survey. The\nreport includes a description of the current state-of-the-art and practice, a\nroadmap for follow-up research in the identified areas, and recommendations for\nanticipated technological development in the domain of text mining.\n","versions":[{"version":"v1","created":"Wed, 29 Dec 2004 15:19:03 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Rajman","Martin",""],["Vesely","Martin",""],["Andrews","Pierre",""]]}
{"id":"cs/0412117","submitter":"Pierre Andrews","authors":"Pierre Andrews and Martin Rajman","title":"Thematic Annotation: extracting concepts out of documents","comments":"Technical report EPFL/LIA. 81 pages, 16 figures","journal-ref":null,"doi":null,"report-no":"IC/2004/68","categories":"cs.CL","license":null,"abstract":"  Contrarily to standard approaches to topic annotation, the technique used in\nthis work does not centrally rely on some sort of -- possibly statistical --\nkeyword extraction. In fact, the proposed annotation algorithm uses a large\nscale semantic database -- the EDR Electronic Dictionary -- that provides a\nconcept hierarchy based on hyponym and hypernym relations. This concept\nhierarchy is used to generate a synthetic representation of the document by\naggregating the words present in topically homogeneous document segments into a\nset of concepts best preserving the document's content.\n  This new extraction technique uses an unexplored approach to topic selection.\nInstead of using semantic similarity measures based on a semantic resource, the\nlater is processed to extract the part of the conceptual hierarchy relevant to\nthe document content. Then this conceptual hierarchy is searched to extract the\nmost relevant set of concepts to represent the topics discussed in the\ndocument. Notice that this algorithm is able to extract generic concepts that\nare not directly present in the document.\n","versions":[{"version":"v1","created":"Thu, 30 Dec 2004 02:01:45 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Andrews","Pierre",""],["Rajman","Martin",""]]}
{"id":"cs/0501078","submitter":"Liang Zhou","authors":"Liang Zhou, Miruna Ticrea and Eduard Hovy","title":"Multi-document Biography Summarization","comments":null,"journal-ref":"Proceedings of EMNLP, pp. 434-441, 2004","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  In this paper we describe a biography summarization system using sentence\nclassification and ideas from information retrieval. Although the individual\ntechniques are not new, assembling and applying them to generate multi-document\nbiographies is new. Our system was evaluated in DUC2004. It is among the top\nperformers in task 5-short summaries focused by person questions.\n","versions":[{"version":"v1","created":"Wed, 26 Jan 2005 22:43:17 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Zhou","Liang",""],["Ticrea","Miruna",""],["Hovy","Eduard",""]]}
{"id":"cs/0504022","submitter":"Lillian Lee","authors":"Lillian Lee","title":"A Matter of Opinion: Sentiment Analysis and Business Intelligence\n  (position paper)","comments":"2 pages","journal-ref":"Presented at the IBM Faculty Summit on the Architecture of\n  On-Demand Business, May 2004","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  A general-audience introduction to the area of \"sentiment analysis\", the\ncomputational treatment of subjective, opinion-oriented language (an example\napplication is determining whether a review is \"thumbs up\" or \"thumbs down\").\nSome challenges, applications to business-intelligence tasks, and potential\nfuture directions are described.\n","versions":[{"version":"v1","created":"Wed, 6 Apr 2005 20:04:55 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Lee","Lillian",""]]}
{"id":"cs/0510015","submitter":"Laurent Audibert","authors":"Laurent Audibert (DELIC)","title":"Word sense disambiguation criteria: a systematic study","comments":null,"journal-ref":"20th International Conference on Computational Linguistics\n  (COLING-2004) (2004) pp. 910-916","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  This article describes the results of a systematic in-depth study of the\ncriteria used for word sense disambiguation. Our study is based on 60 target\nwords: 20 nouns, 20 adjectives and 20 verbs. Our results are not always in line\nwith some practices in the field. For example, we show that omitting\nnon-content words decreases performance and that bigrams yield better results\nthan unigrams.\n","versions":[{"version":"v1","created":"Wed, 5 Oct 2005 14:23:19 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Audibert","Laurent","","DELIC"]]}
{"id":"cs/0511076","submitter":"Yves Laprie","authors":"Blaise Potard (INRIA Lorraine - LORIA), Yves Laprie (INRIA Lorraine -\n  LORIA)","title":"Using phonetic constraints in acoustic-to-articulatory inversion","comments":null,"journal-ref":"Proceedings of Interspeech, 9th European Conference on Speech\n  Communication and Technology (2005) 3217-3220","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  The goal of this work is to recover articulatory information from the speech\nsignal by acoustic-to-articulatory inversion. One of the main difficulties with\ninversion is that the problem is underdetermined and inversion methods\ngenerally offer no guarantee on the phonetical realism of the inverse\nsolutions. A way to adress this issue is to use additional phonetic\nconstraints. Knowledge of the phonetic caracteristics of French vowels enable\nthe derivation of reasonable articulatory domains in the space of Maeda\nparameters: given the formants frequencies (F1,F2,F3) of a speech sample, and\nthus the vowel identity, an \"ideal\" articulatory domain can be derived. The\nspace of formants frequencies is partitioned into vowels, using either\nspeaker-specific data or generic information on formants. Then, to each\narticulatory vector can be associated a phonetic score varying with the\ndistance to the \"ideal domain\" associated with the corresponding vowel.\nInversion experiments were conducted on isolated vowels and vowel-to-vowel\ntransitions. Articulatory parameters were compared with those obtained without\nusing these constraints and those measured from X-ray data.\n","versions":[{"version":"v1","created":"Mon, 21 Nov 2005 14:50:52 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Potard","Blaise","","INRIA Lorraine - LORIA"],["Laprie","Yves","","INRIA Lorraine -\n  LORIA"]]}
{"id":"cs/0511079","submitter":"Anne Bonneau","authors":"Jean-Baptiste Maj (LORIA), Anne Bonneau (LORIA), Dominique Fohr\n  (LORIA), Yves Laprie (LORIA)","title":"An elitist approach for extracting automatically well-realized speech\n  sounds with high confidence","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  This paper presents an \"elitist approach\" for extracting automatically\nwell-realized speech sounds with high confidence. The elitist approach uses a\nspeech recognition system based on Hidden Markov Models (HMM). The HMM are\ntrained on speech sounds which are systematically well-detected in an iterative\nprocedure. The results show that, by using the HMM models defined in the\ntraining phase, the speech recognizer detects reliably specific speech sounds\nwith a small rate of errors.\n","versions":[{"version":"v1","created":"Tue, 22 Nov 2005 07:06:43 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Maj","Jean-Baptiste","","LORIA"],["Bonneau","Anne","","LORIA"],["Fohr","Dominique","","LORIA"],["Laprie","Yves","","LORIA"]]}
{"id":"cs/0512102","submitter":"Andrij Rovenchak","authors":"Solomija Buk and Andrij Rovenchak","title":"Statistical Parameters of the Novel \"Perekhresni stezhky\" (\"The\n  Cross-Paths\") by Ivan Franko","comments":"11 pages","journal-ref":"Quantitative Linguistics 62: Exact methods in the study of\n  language and text: dedicated to Professor Gabriel Altmann on the occasion of\n  his 75th birthday / Ed. by P. Grzybek and R. Kohler (Berlin; New York: de\n  Gruyter), 39-48 (2007)","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  In the paper, a complex statistical characteristics of a Ukrainian novel is\ngiven for the first time. The distribution of word-forms with respect to their\nsize is studied. The linguistic laws by Zipf-Mandelbrot and Altmann-Menzerath\nare analyzed.\n","versions":[{"version":"v1","created":"Wed, 28 Dec 2005 13:45:54 GMT"}],"update_date":"2008-03-18","authors_parsed":[["Buk","Solomija",""],["Rovenchak","Andrij",""]]}
{"id":"cs/0601005","submitter":"Jinyun Ke","authors":"J-Y Ke and Y. Yao","title":"Analyzing language development from a network approach","comments":"22 pages, 12 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  In this paper we propose some new measures of language development using\nnetwork analyses, which is inspired by the recent surge of interests in network\nstudies of many real-world systems. Children's and care-takers' speech data\nfrom a longitudinal study are represented as a series of networks, word forms\nbeing taken as nodes and collocation of words as links. Measures on the\nproperties of the networks, such as size, connectivity, hub and authority\nanalyses, etc., allow us to make quantitative comparison so as to reveal\ndifferent paths of development. For example, the asynchrony of development in\nnetwork size and average degree suggests that children cannot be simply\nclassified as early talkers or late talkers by one or two measures. Children\nfollow different paths in a multi-dimensional space. They may develop faster in\none dimension but slower in another dimension. The network approach requires\nlittle preprocessing of words and analyses on sentence structures, and the\ncharacteristics of words and their usage emerge from the network and are\nindependent of any grammatical presumptions. We show that the change of the two\narticles \"the\" and \"a\" in their roles as important nodes in the network\nreflects the progress of children's syntactic development: the two articles\noften start in children's networks as hubs and later shift to authorities,\nwhile they are authorities constantly in the adult's networks. The network\nanalyses provide a new approach to study language development, and at the same\ntime language development also presents a rich area for network theories to\nexplore.\n","versions":[{"version":"v1","created":"Wed, 4 Jan 2006 18:33:53 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Ke","J-Y",""],["Yao","Y.",""]]}
{"id":"cs/0604027","submitter":"Laurent Romary","authors":"Majid Khayari (INIST), St\\'ephane Schneider (INIST), Isabelle Kramer\n  (LORIA), Laurent Romary (LORIA), the termsciences Collaboration","title":"Unification of multi-lingual scientific terminological resources using\n  the ISO 16642 standard. The TermSciences initiative","comments":"6p","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  This paper presents the TermSciences portal, which deals with the\nimplementation of a conceptual model that uses the recent ISO 16642 standard\n(Terminological Markup Framework). This standard turns out to be suitable for\nconcept modeling since it allowed for organizing the original resources by\nconcepts and to associate the various terms for a given concept. Additional\nstructuring is produced by sharing conceptual relationships, that is,\ncross-linking of resource results through the introduction of semantic\nrelations which may have initially be missing.\n","versions":[{"version":"v1","created":"Fri, 7 Apr 2006 13:10:30 GMT"}],"update_date":"2009-01-20","authors_parsed":[["Khayari","Majid","","INIST"],["Schneider","St\u00e9phane","","INIST"],["Kramer","Isabelle","","LORIA"],["Romary","Laurent","","LORIA"],["Collaboration","the termsciences",""]]}
{"id":"cs/0606006","submitter":"Laurent Romary","authors":"Peter Wittenburg (MPIPS), Daan Broeder (MPIPS), Wolfgang Klein\n  (MPIPS), Stephen Levinson (MPIPS), Laurent Romary (INRIA Lorraine - LORIA)","title":"Foundations of Modern Language Resource Archives","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  A number of serious reasons will convince an increasing amount of researchers\nto store their relevant material in centers which we will call \"language\nresource archives\". They combine the duty of taking care of long-term\npreservation as well as the task to give access to their material to different\nuser groups. Access here is meant in the sense that an active interaction with\nthe data will be made possible to support the integration of new data, new\nversions or commentaries of all sort. Modern Language Resource Archives will\nhave to adhere to a number of basic principles to fulfill all requirements and\nthey will have to be involved in federations to create joint language resource\ndomains making it even more simple for the researchers to access the data. This\npaper makes an attempt to formulate the essential pillars language resource\narchives have to adhere to.\n","versions":[{"version":"v1","created":"Thu, 1 Jun 2006 09:14:51 GMT"}],"update_date":"2009-01-20","authors_parsed":[["Wittenburg","Peter","","MPIPS"],["Broeder","Daan","","MPIPS"],["Klein","Wolfgang","","MPIPS"],["Levinson","Stephen","","MPIPS"],["Romary","Laurent","","INRIA Lorraine - LORIA"]]}
{"id":"cs/0606096","submitter":"Hendrik Feddes","authors":"Lea Cyrus","title":"Building a resource for studying translation shifts","comments":"6 pages, 1 figure","journal-ref":"Proc. LREC 2006, Genoa, May 24-26, 2006; pp. 1240-1245","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  This paper describes an interdisciplinary approach which brings together the\nfields of corpus linguistics and translation studies. It presents ongoing work\non the creation of a corpus resource in which translation shifts are explicitly\nannotated. Translation shifts denote departures from formal correspondence\nbetween source and target text, i.e. deviations that have occurred during the\ntranslation process. A resource in which such shifts are annotated in a\nsystematic way will make it possible to study those phenomena that need to be\naddressed if machine translation output is to resemble human translation. The\nresource described in this paper contains English source texts (parliamentary\nproceedings) and their German translations. The shift annotation is based on\npredicate-argument structures and proceeds in two steps: first, predicates and\ntheir arguments are annotated monolingually in a straightforward manner. Then,\nthe corresponding English and German predicates and arguments are aligned with\neach other. Whenever a shift - mainly grammatical or semantic -has occurred,\nthe alignment is tagged accordingly.\n","versions":[{"version":"v1","created":"Thu, 22 Jun 2006 13:26:52 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Cyrus","Lea",""]]}
{"id":"cs/0607051","submitter":"Catherine Recanati","authors":"Catherine Recanati (LIPN)","title":"Raisonner avec des diagrammes : perspectives cognitives et\n  computationnelles","comments":"paru initialement comme Rapport LIPN en 2004","journal-ref":"Intellectica 40 (2005) 9-42","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  Diagrammatic, analogical or iconic representations are often contrasted with\nlinguistic or logical representations, in which the shape of the symbols is\narbitrary. The aim of this paper is to make a case for the usefulness of\ndiagrams in inferential knowledge representation systems. Although commonly\nused, diagrams have for a long time suffered from the reputation of being only\na heuristic tool or a mere support for intuition. The first part of this paper\nis an historical background paying tribute to the logicians, psychologists and\ncomputer scientists who put an end to this formal prejudice against diagrams.\nThe second part is a discussion of their characteristics as opposed to those of\nlinguistic forms. The last part is aimed at reviving the interest for\nheterogeneous representation systems including both linguistic and diagrammatic\nrepresentations.\n","versions":[{"version":"v1","created":"Tue, 11 Jul 2006 19:13:25 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Recanati","Catherine","","LIPN"]]}
{"id":"cs/0609019","submitter":"Sophie Aubin","authors":"Sophie Aubin (LIPN), Thierry Hamon (LIPN)","title":"Improving Term Extraction with Terminological Resources","comments":null,"journal-ref":"Advances in Natural Language Processing 5th International\n  Conference on NLP, FinTAL 2006 (2006) 380","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  Studies of different term extractors on a corpus of the biomedical domain\nrevealed decreasing performances when applied to highly technical texts. The\ndifficulty or impossibility of customising them to new domains is an additional\nlimitation. In this paper, we propose to use external terminologies to\ninfluence generic linguistic data in order to augment the quality of the\nextraction. The tool we implemented exploits testified terms at different steps\nof the process: chunking, parsing and extraction of term candidates.\nExperiments reported here show that, using this method, more term candidates\ncan be acquired with a higher level of reliability. We further describe the\nextraction process involving endogenous disambiguation implemented in the term\nextractor YaTeA.\n","versions":[{"version":"v1","created":"Wed, 6 Sep 2006 11:41:27 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Aubin","Sophie","","LIPN"],["Hamon","Thierry","","LIPN"]]}
{"id":"cs/0609043","submitter":"Francoise Gayral","authors":"Fran\\c{c}oise Gayral (LIPN), Daniel Kayser (LIPN), Fran\\c{c}ois L\\'evy\n  (LIPN)","title":"Challenging the principle of compositionality in interpreting natural\n  language texts","comments":null,"journal-ref":"conference on Compositionality, Concepts and Cognition, Allemagne\n  (2004)","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  The paper aims at emphasizing that, even relaxed, the hypothesis of\ncompositionality has to face many problems when used for interpreting natural\nlanguage texts. Rather than fixing these problems within the compositional\nframework, we believe that a more radical change is necessary, and propose\nanother approach.\n","versions":[{"version":"v1","created":"Fri, 8 Sep 2006 14:01:57 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Gayral","Fran\u00e7oise","","LIPN"],["Kayser","Daniel","","LIPN"],["L\u00e9vy","Fran\u00e7ois","","LIPN"]]}
{"id":"cs/0609044","submitter":"Francoise Gayral","authors":"Fran\\c{c}oise Gayral (LIPN), Daniel Kayser (LIPN), Fran\\c{c}ois L\\'evy\n  (LIPN)","title":"The role of time in considering collections","comments":null,"journal-ref":"Journ\\'{e}es de S\\'{e}mantique et Mod\\'{e}lisation, France (2004)","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  The paper concerns the understanding of plurals in the framework of\nArtificial Intelligence and emphasizes the role of time. The construction of\ncollection(s) and their evolution across time is often crucial and has to be\naccounted for. The paper contrasts a \"de dicto\" collection where the collection\ncan be considered as persisting over these situations even if its members\nchange with a \"de re\" collection whose composition does not vary through time.\nIt expresses different criteria of choice between the two interpretations (de\nre and de dicto) depending on the context of enunciation.\n","versions":[{"version":"v1","created":"Fri, 8 Sep 2006 14:11:50 GMT"}],"update_date":"2016-08-16","authors_parsed":[["Gayral","Fran\u00e7oise","","LIPN"],["Kayser","Daniel","","LIPN"],["L\u00e9vy","Fran\u00e7ois","","LIPN"]]}
{"id":"cs/0609058","submitter":"Ralf Steinberger","authors":"Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz\n  Erjavec, Dan Tufis, Daniel Varga","title":"The JRC-Acquis: A multilingual aligned parallel corpus with 20+\n  languages","comments":"A multilingual textual resource with meta-data freely available for\n  download at http://langtech.jrc.it/JRC-Acquis.html","journal-ref":"Proceedings of the 5th International Conference on Language\n  Resources and Evaluation (LREC'2006), pp. 2142-2147. Genoa, Italy, 24-26 May\n  2006","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  We present a new, unique and freely available parallel corpus containing\nEuropean Union (EU) documents of mostly legal nature. It is available in all 20\nofficial EUanguages, with additional documents being available in the languages\nof the EU candidate countries. The corpus consists of almost 8,000 documents\nper language, with an average size of nearly 9 million words per language.\nPair-wise paragraph alignment information produced by two different aligners\n(Vanilla and HunAlign) is available for all 190+ language pair combinations.\nMost texts have been manually classified according to the EUROVOC subject\ndomains so that the collection can also be used to train and test multi-label\nclassification algorithms and keyword-assignment software. The corpus is\nencoded in XML, according to the Text Encoding Initiative Guidelines. Due to\nthe large number of parallel texts in many languages, the JRC-Acquis is\nparticularly suitable to carry out all types of cross-language research, as\nwell as to test and benchmark text analysis software across different languages\n(for instance for alignment, sentence splitting and term extraction).\n","versions":[{"version":"v1","created":"Tue, 12 Sep 2006 07:10:15 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Steinberger","Ralf",""],["Pouliquen","Bruno",""],["Widiger","Anna",""],["Ignat","Camelia",""],["Erjavec","Tomaz",""],["Tufis","Dan",""],["Varga","Daniel",""]]}
{"id":"cs/0610116","submitter":"Tuomo Kakkonen","authors":"Tuomo Kakkonen","title":"DepAnn - An Annotation Tool for Dependency Treebanks","comments":null,"journal-ref":"Proceedings of the 11th ESSLLI Student Session at the 18th\n  European Summer School in Logic, Language and Information (ESSLLI 2006), pp.\n  214-225. Malaga, Spain, 2006","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  DepAnn is an interactive annotation tool for dependency treebanks, providing\nboth graphical and text-based annotation interfaces. The tool is aimed for\nsemi-automatic creation of treebanks. It aids the manual inspection and\ncorrection of automatically created parses, making the annotation process\nfaster and less error-prone. A novel feature of the tool is that it enables the\nuser to view outputs from several parsers as the basis for creating the final\ntree to be saved to the treebank. DepAnn uses TIGER-XML, an XML-based general\nencoding format for both, representing the parser outputs and saving the\nannotated treebank. The tool includes an automatic consistency checker for\nsentence structures. In addition, the tool enables users to build structures\nmanually, add comments on the annotations, modify the tagsets, and mark\nsentences for further revision.\n","versions":[{"version":"v1","created":"Thu, 19 Oct 2006 17:42:57 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Kakkonen","Tuomo",""]]}
{"id":"cs/0610124","submitter":"Tuomo Kakkonen","authors":"Tuomo Kakkonen","title":"Dependency Treebanks: Methods, Annotation Schemes and Tools","comments":null,"journal-ref":"Proceedings of the 15th Nordic Conference of Computational\n  Linguistics (NODALIDA 2005), pp. 94-104. Joensuu, Finland, 2005","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  In this paper, current dependencybased treebanks are introduced and analyzed.\nThe methods used for building the resources, the annotation schemes applied,\nand the tools used (such as POS taggers, parsers and annotation software) are\ndiscussed.\n","versions":[{"version":"v1","created":"Fri, 20 Oct 2006 11:48:38 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Kakkonen","Tuomo",""]]}
{"id":"cs/0611026","submitter":"Susanne Salmon-Alt","authors":"Susanne Salmon-Alt (ATILF), Laurent Romary (INRIA Lorraine - LORIA),\n  Jean-Marie Pierrel (ATILF)","title":"Un mod\\`ele g\\'en\\'erique d'organisation de corpus en ligne: application\n  \\`a la FReeBank","comments":null,"journal-ref":"Traitement Automatique des Langues (TAL) 45 (2006) 145-169","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  The few available French resources for evaluating linguistic models or\nalgorithms on other linguistic levels than morpho-syntax are either\ninsufficient from quantitative as well as qualitative point of view or not\nfreely accessible. Based on this fact, the FREEBANK project intends to create\nFrench corpora constructed using manually revised output from a hybrid\nConstraint Grammar parser and annotated on several linguistic levels\n(structure, morpho-syntax, syntax, coreference), with the objective to make\nthem available on-line for research purposes. Therefore, we will focus on using\nstandard annotation schemes, integration of existing resources and maintenance\nallowing for continuous enrichment of the annotations. Prior to the actual\npresentation of the prototype that has been implemented, this paper describes a\ngeneric model for the organization and deployment of a linguistic resource\narchive, in compliance with the various works currently conducted within\ninternational standardization initiatives (TEI and ISO/TC 37/SC 4).\n","versions":[{"version":"v1","created":"Mon, 6 Nov 2006 14:37:27 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Salmon-Alt","Susanne","","ATILF"],["Romary","Laurent","","INRIA Lorraine - LORIA"],["Pierrel","Jean-Marie","","ATILF"]]}
{"id":"cs/0611069","submitter":"Guillaume Pitel","authors":"Guillaume Pitel (INRIA Lorraine - LORIA)","title":"Scaling Construction Grammar up to Production Systems: the SCIM","comments":null,"journal-ref":"Dans Scalable Natural Language Understanding 2006 (2006)","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  While a great effort has concerned the development of fully integrated\nmodular understanding systems, few researches have focused on the problem of\nunifying existing linguistic formalisms with cognitive processing models. The\nSituated Constructional Interpretation Model is one of these attempts. In this\nmodel, the notion of \"construction\" has been adapted in order to be able to\nmimic the behavior of Production Systems. The Construction Grammar approach\nestablishes a model of the relations between linguistic forms and meaning, by\nthe mean of constructions. The latter can be considered as pairings from a\ntopologically structured space to an unstructured space, in some way a special\nkind of production rules.\n","versions":[{"version":"v1","created":"Wed, 15 Nov 2006 12:35:45 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Pitel","Guillaume","","INRIA Lorraine - LORIA"]]}
{"id":"cs/0611113","submitter":"Maurice H. T. Ling","authors":"Maurice HT Ling","title":"An Anthological Review of Research Utilizing MontyLingua, a Python-Based\n  End-to-End Text Processor","comments":"9 pages","journal-ref":"Ling, Maurice HT. 2006. An Anthological Review of Research\n  Utilizing MontyLingua, a Python-Based End-to-End Text Processor. The Python\n  Papers 1 (1): 5-13","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  MontyLingua, an integral part of ConceptNet which is currently the largest\ncommonsense knowledge base, is an English text processor developed using Python\nprogramming language in MIT Media Lab. The main feature of MontyLingua is the\ncoverage for all aspects of English text processing from raw input text to\nsemantic meanings and summary generation, yet each component in MontyLingua is\nloosely-coupled to each other at the architectural and code level, which\nenabled individual components to be used independently or substituted. However,\nthere has been no review exploring the role of MontyLingua in recent research\nwork utilizing it. This paper aims to review the use of and roles played by\nMontyLingua and its components in research work published in 19 articles\nbetween October 2004 and August 2006. We had observed a diversified use of\nMontyLingua in many different areas, both generic and domain-specific. Although\nthe use of text summarizing component had not been observe, we are optimistic\nthat it will have a crucial role in managing the current trend of information\noverload in future research.\n","versions":[{"version":"v1","created":"Wed, 22 Nov 2006 03:24:54 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Ling","Maurice HT",""]]}
{"id":"cs/0701135","submitter":"Jinyun Ke","authors":"Jinyun KE","title":"Complex networks and human language","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  This paper introduces how human languages can be studied in light of recent\ndevelopment of network theories. There are two directions of exploration. One\nis to study networks existing in the language system. Various lexical networks\ncan be built based on different relationships between words, being semantic or\nsyntactic. Recent studies have shown that these lexical networks exhibit\nsmall-world and scale-free features. The other direction of exploration is to\nstudy networks of language users (i.e. social networks of people in the\nlinguistic community), and their role in language evolution. Social networks\nalso show small-world and scale-free features, which cannot be captured by\nrandom or regular network models. In the past, computational models of language\nchange and language emergence often assume a population to have a random or\nregular structure, and there has been little discussion how network structures\nmay affect the dynamics. In the second part of the paper, a series of\nsimulation models of diffusion of linguistic innovation are used to illustrate\nthe importance of choosing realistic conditions of population structure for\nmodeling language change. Four types of social networks are compared, which\nexhibit two categories of diffusion dynamics. While the questions about which\ntype of networks are more appropriate for modeling still remains, we give some\npreliminary suggestions for choosing the type of social networks for modeling.\n","versions":[{"version":"v1","created":"Mon, 22 Jan 2007 00:45:31 GMT"}],"update_date":"2007-05-23","authors_parsed":[["KE","Jinyun",""]]}
{"id":"cs/0701181","submitter":"Fionn Murtagh","authors":"Fionn Murtagh","title":"A Note on Local Ultrametricity in Text","comments":"18 pp","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  High dimensional, sparsely populated data spaces have been characterized in\nterms of ultrametric topology. This implies that there are natural, not\nnecessarily unique, tree or hierarchy structures defined by the ultrametric\ntopology. In this note we study the extent of local ultrametric topology in\ntexts, with the aim of finding unique ``fingerprints'' for a text or corpus,\ndiscriminating between texts from different domains, and opening up the\npossibility of exploiting hierarchical structures in the data. We use coherent\nand meaningful collections of over 1000 texts, comprising over 1.3 million\nwords.\n","versions":[{"version":"v1","created":"Sat, 27 Jan 2007 19:09:53 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Murtagh","Fionn",""]]}
{"id":"cs/0701194","submitter":"Andrij Rovenchak","authors":"Solomija Buk and Andrij Rovenchak","title":"Menzerath-Altmann Law for Syntactic Structures in Ukrainian","comments":"8 pages; submitted to the Proceedings of the International scientific\n  conference on Modern Methods in Linguistics held in honour of the anniversary\n  of Prof. Gabriel L. Altmann (October 23rd and 24th, 2006, Budmerice Castle,\n  Slovakia)","journal-ref":"Glottotheory. Vol. 1, No. 1, pp 10-17 (2008)","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  In the paper, the definition of clause suitable for an automated processing\nof a Ukrainian text is proposed. The Menzerath-Altmann law is verified on the\nsentence level and the parameters for the dependences of the clause length\ncounted in words and syllables on the sentence length counted in clauses are\ncalculated for \"Perekhresni Stezhky\" (\"The Cross-Paths\"), a novel by Ivan\nFranko.\n","versions":[{"version":"v1","created":"Tue, 30 Jan 2007 16:58:07 GMT"}],"update_date":"2008-10-09","authors_parsed":[["Buk","Solomija",""],["Rovenchak","Andrij",""]]}
{"id":"cs/0702081","submitter":"Rick Dale","authors":"Rick Dale","title":"Random Sentences from a Generalized Phrase-Structure Grammar Interpreter","comments":"Brief paper with source code and examples","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  In numerous domains in cognitive science it is often useful to have a source\nfor randomly generated corpora. These corpora may serve as a foundation for\nartificial stimuli in a learning experiment (e.g., Ellefson & Christiansen,\n2000), or as input into computational models (e.g., Christiansen & Dale, 2001).\nThe following compact and general C program interprets a phrase-structure\ngrammar specified in a text file. It follows parameters set at a Unix or\nUnix-based command-line and generates a corpus of random sentences from that\ngrammar.\n","versions":[{"version":"v1","created":"Wed, 14 Feb 2007 06:05:20 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Dale","Rick",""]]}
{"id":"cs/9809020","submitter":"Min-Yen Kan","authors":"Min-Yen Kan, Judith L. Klavans, Kathleen R. McKeown","title":"Linear Segmentation and Segment Significance","comments":"9 pages, US Letter, 4 figures. Software License can be found at\n  http://www.cs.columbia.edu/nlp/licenses/segmenterLicenseDownload.html","journal-ref":"Proceedings of 6th International Workshop of Very Large Corpora\n  (WVLC-6), Montreal, Quebec, Canada: Aug. 1998. pp. 197-205","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  We present a new method for discovering a segmental discourse structure of a\ndocument while categorizing segment function. We demonstrate how retrieval of\nnoun phrases and pronominal forms, along with a zero-sum weighting scheme,\ndetermines topicalized segmentation. Futhermore, we use term distribution to\naid in identifying the role that the segment performs in the document. Finally,\nwe present results of evaluation in terms of precision and recall which surpass\nearlier approaches.\n","versions":[{"version":"v1","created":"Tue, 15 Sep 1998 23:49:32 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Kan","Min-Yen",""],["Klavans","Judith L.",""],["McKeown","Kathleen R.",""]]}
{"id":"cs/9809022","submitter":"Bernd Ludwig","authors":"Bernd Ludwig, Guenther Goerz, Heinrich Niemann","title":"Modelling Users, Intentions, and Structure in Spoken Dialog","comments":"17 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  We outline how utterances in dialogs can be interpreted using a partial first\norder logic. We exploit the capability of this logic to talk about the truth\nstatus of formulae to define a notion of coherence between utterances and\nexplain how this coherence relation can serve for the construction of AND/OR\ntrees that represent the segmentation of the dialog. In a BDI model we\nformalize basic assumptions about dialog and cooperative behaviour of\nparticipants. These assumptions provide a basis for inferring speech acts from\ncoherence relations between utterances and attitudes of dialog participants.\nSpeech acts prove to be useful for determining dialog segments defined on the\nnotion of completing expectations of dialog participants. Finally, we sketch\nhow explicit segmentation signalled by cue phrases and performatives is covered\nby our dialog model.\n","versions":[{"version":"v1","created":"Thu, 17 Sep 1998 11:10:14 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Ludwig","Bernd",""],["Goerz","Guenther",""],["Niemann","Heinrich",""]]}
{"id":"cs/9809024","submitter":"Anoop Sarkar","authors":"XTAG Research Group (University of Pennsylvania)","title":"A Lexicalized Tree Adjoining Grammar for English","comments":"310 pages, 181 Postscript figures, uses 11pt, psfig.tex","journal-ref":null,"doi":null,"report-no":"IRCS Tech Report 98-18, ftp://ftp.cis.upenn.edu/pub/ircs/tr/98-18/","categories":"cs.CL","license":null,"abstract":"  This document describes a sizable grammar of English written in the TAG\nformalism and implemented for use with the XTAG system. This report and the\ngrammar described herein supersedes the TAG grammar described in an earlier\n1995 XTAG technical report. The English grammar described in this report is\nbased on the TAG formalism which has been extended to include lexicalization,\nand unification-based feature structures. The range of syntactic phenomena that\ncan be handled is large and includes auxiliaries (including inversion), copula,\nraising and small clause constructions, topicalization, relative clauses,\ninfinitives, gerunds, passives, adjuncts, it-clefts, wh-clefts, PRO\nconstructions, noun-noun modifications, extraposition, determiner sequences,\ngenitives, negation, noun-verb contractions, sentential adjuncts and\nimperatives. This technical report corresponds to the XTAG Release 8/31/98. The\nXTAG grammar is continuously updated with the addition of new analyses and\nmodification of old ones, and an online version of this report can be found at\nthe XTAG web page at http://www.cis.upenn.edu/~xtag/\n","versions":[{"version":"v1","created":"Fri, 18 Sep 1998 00:33:47 GMT"},{"version":"v2","created":"Fri, 18 Sep 1998 02:49:22 GMT"}],"update_date":"2012-08-27","authors_parsed":[["XTAG Research Group","",""]]}
{"id":"cs/9809026","submitter":"Anoop Sarkar","authors":"Mark-Jan Nederhof (DFKI), Anoop Sarkar (UPenn) and Giorgio Satta\n  (UPadova)","title":"Prefix Probabilities from Stochastic Tree Adjoining Grammars","comments":"7 pages, 2 Postscript figures, uses colacl.sty, graphicx.sty,\n  psfrag.sty","journal-ref":"In Proceedings of COLING-ACL '98 (Montreal)","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  Language models for speech recognition typically use a probability model of\nthe form Pr(a_n | a_1, a_2, ..., a_{n-1}). Stochastic grammars, on the other\nhand, are typically used to assign structure to utterances. A language model of\nthe above form is constructed from such grammars by computing the prefix\nprobability Sum_{w in Sigma*} Pr(a_1 ... a_n w), where w represents all\npossible terminations of the prefix a_1 ... a_n. The main result in this paper\nis an algorithm to compute such prefix probabilities given a stochastic Tree\nAdjoining Grammar (TAG). The algorithm achieves the required computation in\nO(n^6) time. The probability of subderivations that do not derive any words in\nthe prefix, but contribute structurally to its derivation, are precomputed to\nachieve termination. This algorithm enables existing corpus-based estimation\ntechniques for stochastic TAGs to be used for language modelling.\n","versions":[{"version":"v1","created":"Fri, 18 Sep 1998 03:45:45 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Nederhof","Mark-Jan","","DFKI"],["Sarkar","Anoop","","UPenn"],["Satta","Giorgio","","UPadova"]]}
{"id":"cs/9809027","submitter":"Anoop Sarkar","authors":"Anoop Sarkar (University of Pennsylvania)","title":"Conditions on Consistency of Probabilistic Tree Adjoining Grammars","comments":"7 pages, 4 Postscript figures, uses colacl.sty, graphicx.sty,\n  psfrag.sty","journal-ref":"In Proceedings of COLING-ACL '98 (Montreal)","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  Much of the power of probabilistic methods in modelling language comes from\ntheir ability to compare several derivations for the same string in the\nlanguage. An important starting point for the study of such cross-derivational\nproperties is the notion of _consistency_. The probability model defined by a\nprobabilistic grammar is said to be _consistent_ if the probabilities assigned\nto all the strings in the language sum to one. From the literature on\nprobabilistic context-free grammars (CFGs), we know precisely the conditions\nwhich ensure that consistency is true for a given CFG. This paper derives the\nconditions under which a given probabilistic Tree Adjoining Grammar (TAG) can\nbe shown to be consistent. It gives a simple algorithm for checking consistency\nand gives the formal justification for its correctness. The conditions derived\nhere can be used to ensure that probability models that use TAGs can be checked\nfor _deficiency_ (i.e. whether any probability mass is assigned to strings that\ncannot be generated).\n","versions":[{"version":"v1","created":"Fri, 18 Sep 1998 03:58:57 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Sarkar","Anoop","","University of Pennsylvania"]]}
{"id":"cs/9809028","submitter":"Anoop Sarkar","authors":"Anoop Sarkar (University of Pennsylvania)","title":"Separating Dependency from Constituency in a Tree Rewriting System","comments":"7 pages, 6 Postscript figures, uses fullname.sty","journal-ref":"In Proceedings of the Fifth Meeting on Mathematics of Language,\n  Saarbruecken, August 1997","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  In this paper we present a new tree-rewriting formalism called Link-Sharing\nTree Adjoining Grammar (LSTAG) which is a variant of synchronous TAGs. Using\nLSTAG we define an approach towards coordination where linguistic dependency is\ndistinguished from the notion of constituency. Such an approach towards\ncoordination that explicitly distinguishes dependencies from constituency gives\na better formal understanding of its representation when compared to previous\napproaches that use tree-rewriting systems which conflate the two issues.\n","versions":[{"version":"v1","created":"Fri, 18 Sep 1998 04:44:02 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Sarkar","Anoop","","University of Pennsylvania"]]}
{"id":"cs/9809029","submitter":"Anoop Sarkar","authors":"Anoop Sarkar (University of Pennsylvania)","title":"Incremental Parser Generation for Tree Adjoining Grammars","comments":"12 pages, 12 Postscript figures, uses fullname.sty","journal-ref":"Longer version of paper in Proceedings of the 34th Meeting of the\n  ACL, Student Session. Santa Cruz, June 1996","doi":"10.1063/1.1594535","report-no":null,"categories":"cs.CL","license":null,"abstract":"  This paper describes the incremental generation of parse tables for the\nLR-type parsing of Tree Adjoining Languages (TALs). The algorithm presented\nhandles modifications to the input grammar by updating the parser generated so\nfar. In this paper, a lazy generation of LR-type parsers for TALs is defined in\nwhich parse tables are created by need while parsing. We then describe an\nincremental parser generator for TALs which responds to modification of the\ninput grammar by updating parse tables built so far.\n","versions":[{"version":"v1","created":"Fri, 18 Sep 1998 05:03:48 GMT"}],"update_date":"2015-06-25","authors_parsed":[["Sarkar","Anoop","","University of Pennsylvania"]]}
{"id":"cs/9809050","submitter":"Wolfgang Lezius","authors":"Wolfgang Lezius (University of Paderborn), Reinhard Rapp (University\n  of Mainz), Manfred Wettler (University of Paderborn)","title":"A Freely Available Morphological Analyzer, Disambiguator and Context\n  Sensitive Lemmatizer for German","comments":"5 pages, Postscript only","journal-ref":"Proceedings of the COLING-ACL 1998, pp. 743-748","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  In this paper we present Morphy, an integrated tool for German morphology,\npart-of-speech tagging and context-sensitive lemmatization. Its large lexicon\nof more than 320,000 word forms plus its ability to process German compound\nnouns guarantee a wide morphological coverage. Syntactic ambiguities can be\nresolved with a standard statistical part-of-speech tagger. By using the output\nof the tagger, the lemmatizer can determine the correct root even for ambiguous\nword forms. The complete package is freely available and can be downloaded from\nthe World Wide Web.\n","versions":[{"version":"v1","created":"Wed, 23 Sep 1998 12:59:39 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Lezius","Wolfgang","","University of Paderborn"],["Rapp","Reinhard","","University\n  of Mainz"],["Wettler","Manfred","","University of Paderborn"]]}
{"id":"cs/9809106","submitter":"Markus Walther","authors":"Petra Barg and Markus Walther (University of Duesseldorf)","title":"Processing Unknown Words in HPSG","comments":"5 pp., 1 PostScript figure","journal-ref":"Proceedings COLING-ACL'98, vol.I, 91-95","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  The lexical acquisition system presented in this paper incrementally updates\nlinguistic properties of unknown words inferred from their surrounding context\nby parsing sentences with an HPSG grammar for German. We employ a gradual,\ninformation-based concept of ``unknownness'' providing a uniform treatment for\nthe range of completely known to maximally unknown lexical entries. ``Unknown''\ninformation is viewed as revisable information, which is either generalizable\nor specializable. Updating takes place after parsing, which only requires a\nmodified lexical lookup. Revisable pieces of information are identified by\ngrammar-specified declarations which provide access paths into the parse\nfeature structure. The updating mechanism revises the corresponding places in\nthe lexical feature structures iff the context actually provides new\ninformation. For revising generalizable information, type union is required. A\nworked-out example demonstrates the inferential capacity of our implemented\nsystem.\n","versions":[{"version":"v1","created":"Fri, 25 Sep 1998 11:02:08 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Barg","Petra","","University of Duesseldorf"],["Walther","Markus","","University of Duesseldorf"]]}
{"id":"cs/9809107","submitter":"Markus Walther","authors":"Markus Walther (University of Marburg)","title":"Computing Declarative Prosodic Morphology","comments":"10 pages","journal-ref":"Proceedings of SIGPHON'98, pp. 11-20 (COLING-ACL'98\n  Post-Conference Workshop on The Computation of Phonological Constraints)","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  This paper describes a computational, declarative approach to prosodic\nmorphology that uses inviolable constraints to denote small finite candidate\nsets which are filtered by a restrictive incremental optimization mechanism.\nThe new approach is illustrated with an implemented fragment of Modern Hebrew\nverbs couched in MicroCUF, an expressive constraint logic formalism. For\ngeneration and parsing of word forms, I propose a novel off-line technique to\neliminate run-time optimization. It produces a finite-state oracle that\nefficiently restricts the constraint interpreter's search space. As a\nbyproduct, unknown words can be analyzed without special mechanisms. Unlike\npure finite-state transducer approaches, this hybrid setup allows for more\nexpressivity in constraints to specify e.g. token identity for reduplication or\narithmetic constraints for phonetics.\n","versions":[{"version":"v1","created":"Fri, 25 Sep 1998 14:32:38 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Walther","Markus","","University of Marburg"]]}
{"id":"cs/9809112","submitter":"Lluis Padro","authors":"L. Padro & L. Marquez (Universitat Politecnica de Catalunya)","title":"On the Evaluation and Comparison of Taggers: The Effect of Noise in\n  Testing Corpora","comments":"Appears in proceedings of joint COLING-ACL 1998, Montreal, Canada","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  This paper addresses the issue of {\\sc pos} tagger evaluation. Such\nevaluation is usually performed by comparing the tagger output with a reference\ntest corpus, which is assumed to be error-free. Currently used corpora contain\nnoise which causes the obtained performance to be a distortion of the real\nvalue. We analyze to what extent this distortion may invalidate the comparison\nbetween taggers or the measure of the improvement given by a new system. The\nmain conclusion is that a more rigorous testing experimentation\nsetting/designing is needed to reliably evaluate and compare tagger accuracies.\n","versions":[{"version":"v1","created":"Mon, 28 Sep 1998 07:49:11 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Padro","L.","","Universitat Politecnica de Catalunya"],["Marquez","L.","","Universitat Politecnica de Catalunya"]]}
{"id":"cs/9809113","submitter":"Lluis Padro","authors":"L. Marquez, L. Padro & H. Rodriguez (Universitat Politecnica de\n  Catalunya)","title":"Improving Tagging Performance by Using Voting Taggers","comments":"Appears in proceedings of NLP+IA/TAL+AI'98. Moncton, New Brunswick,\n  Canada, 1998","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  We present a bootstrapping method to develop an annotated corpus, which is\nspecially useful for languages with few available resources. The method is\nbeing applied to develop a corpus of Spanish of over 5Mw. The method consists\non taking advantage of the collaboration of two different POS taggers. The\ncases in which both taggers agree present a higher accuracy and are used to\nretrain the taggers.\n","versions":[{"version":"v1","created":"Mon, 28 Sep 1998 07:50:55 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Marquez","L.","","Universitat Politecnica de\n  Catalunya"],["Padro","L.","","Universitat Politecnica de\n  Catalunya"],["Rodriguez","H.","","Universitat Politecnica de\n  Catalunya"]]}
{"id":"cs/9810014","submitter":"Min-Yen Kan","authors":"Judith L. Klavans (Columbia University), Kathleen R. McKeown (Columbia\n  University), Min-Yen Kan (Columbia University) and Susan Lee (University of\n  California at Berkeley)","title":"Resources for Evaluation of Summarization Techniques","comments":"LaTeX source, 5 pages, US Letter, uses lrec98.sty","journal-ref":"in Proc. of First International Conference on Language Resources\n  and Evaluation, Rubio, Gallardo, Castro, and Tejada (eds.), Granada, Spain,\n  1998","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  We report on two corpora to be used in the evaluation of component systems\nfor the tasks of (1) linear segmentation of text and (2) summary-directed\nsentence extraction. We present characteristics of the corpora, methods used in\nthe collection of user judgments, and an overview of the application of the\ncorpora to evaluating the component system. Finally, we discuss the problems\nand issues with construction of the test set which apply broadly to the\nconstruction of evaluation resources for language technologies.\n","versions":[{"version":"v1","created":"Tue, 13 Oct 1998 20:33:05 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Klavans","Judith L.","","Columbia University"],["McKeown","Kathleen R.","","Columbia\n  University"],["Kan","Min-Yen","","Columbia University"],["Lee","Susan","","University of\n  California at Berkeley"]]}
{"id":"cs/9810015","submitter":"William Schuler","authors":"Giorgio Satta (Universita di Padova) and William Schuler (University\n  of Pennsylvania)","title":"Restrictions on Tree Adjoining Languages","comments":"7 pages LaTeX + 5 eps figures","journal-ref":"Proceedings of COLING-ACL'98","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  Several methods are known for parsing languages generated by Tree Adjoining\nGrammars (TAGs) in O(n^6) worst case running time. In this paper we investigate\nwhich restrictions on TAGs and TAG derivations are needed in order to lower\nthis O(n^6) time complexity, without introducing large runtime constants, and\nwithout losing any of the generative power needed to capture the syntactic\nconstructions in natural language that can be handled by unrestricted TAGs. In\nparticular, we describe an algorithm for parsing a strict subclass of TAG in\nO(n^5), and attempt to show that this subclass retains enough generative power\nto make it useful in the general case.\n","versions":[{"version":"v1","created":"Tue, 13 Oct 1998 21:17:13 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Satta","Giorgio","","Universita di Padova"],["Schuler","William","","University\n  of Pennsylvania"]]}
{"id":"cs/9811008","submitter":"Philip Edmonds","authors":"Philip Edmonds (University of Toronto)","title":"Translating near-synonyms: Possibilities and preferences in the\n  interlingua","comments":"8 pages, LaTeX2e, 1 eps figure, uses colacl.sty, epsfig.sty, avm.sty,\n  times.sty","journal-ref":"Proceedings of the AMTA/SIG-IL Second Workshop on Interlinguas,\n  October 1998","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  This paper argues that an interlingual representation must explicitly\nrepresent some parts of the meaning of a situation as possibilities (or\npreferences), not as necessary or definite components of meaning (or\nconstraints). Possibilities enable the analysis and generation of nuance,\nsomething required for faithful translation. Furthermore, the representation of\nthe meaning of words, especially of near-synonyms, is crucial, because it\nspecifies which nuances words can convey in which contexts.\n","versions":[{"version":"v1","created":"Mon, 2 Nov 1998 21:29:41 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Edmonds","Philip","","University of Toronto"]]}
{"id":"cs/9811009","submitter":"Philip Edmonds","authors":"Philip Edmonds (University of Toronto)","title":"Choosing the Word Most Typical in Context Using a Lexical Co-occurrence\n  Network","comments":"3 pages, LaTeX2e, 1 ps figure, uses mathptm.sty, colacl.sty,\n  psfig.sty","journal-ref":"Proceedings of ACL-EACL '97, student session","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  This paper presents a partial solution to a component of the problem of\nlexical choice: choosing the synonym most typical, or expected, in context. We\napply a new statistical approach to representing the context of a word through\nlexical co-occurrence networks. The implementation was trained and evaluated on\na large corpus, and results show that the inclusion of second-order\nco-occurrence relations improves the performance of our implemented lexical\nchoice program.\n","versions":[{"version":"v1","created":"Mon, 2 Nov 1998 23:06:19 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Edmonds","Philip","","University of Toronto"]]}
{"id":"cs/9811016","submitter":"Martin Volk","authors":"Martin Volk and Gerold Schneider (University of Zurich)","title":"Comparing a statistical and a rule-based tagger for German","comments":"8 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  In this paper we present the results of comparing a statistical tagger for\nGerman based on decision trees and a rule-based Brill-Tagger for German. We\nused the same training corpus (and therefore the same tag-set) to train both\ntaggers. We then applied the taggers to the same test corpus and compared their\nrespective behavior and in particular their error rates. Both taggers perform\nsimilarly with an error rate of around 5%. From the detailed error analysis it\ncan be seen that the rule-based tagger has more problems with unknown words\nthan the statistical tagger. But the results are opposite for tokens that are\nmany-ways ambiguous. If the unknown words are fed into the taggers with the\nhelp of an external lexicon (such as the Gertwol system) the error rate of the\nrule-based tagger drops to 4.7%, and the respective rate of the statistical\ntaggers drops to around 3.7%. Combining the taggers by using the output of one\ntagger to help the other did not lead to any further improvement.\n","versions":[{"version":"v1","created":"Wed, 11 Nov 1998 11:06:34 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Volk","Martin","","University of Zurich"],["Schneider","Gerold","","University of Zurich"]]}
{"id":"cs/9811022","submitter":"Ciprian Chelba","authors":"Ciprian Chelba, Frederick Jelinek (CLSP The Johns Hopkins University)","title":"Expoiting Syntactic Structure for Language Modeling","comments":"changed ACM-class membership and buggy author names","journal-ref":"Proceedings of ACL'98, Montreal, Canada","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  The paper presents a language model that develops syntactic structure and\nuses it to extract meaningful information from the word history, thus enabling\nthe use of long distance dependencies. The model assigns probability to every\njoint sequence of words--binary-parse-structure with headword annotation and\noperates in a left-to-right manner --- therefore usable for automatic speech\nrecognition. The model, its probabilistic parameterization, and a set of\nexperiments meant to evaluate its predictive power are presented; an\nimprovement over standard trigram modeling is achieved.\n","versions":[{"version":"v1","created":"Thu, 12 Nov 1998 17:31:17 GMT"},{"version":"v2","created":"Tue, 25 Jan 2000 15:37:36 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Chelba","Ciprian","","CLSP The Johns Hopkins University"],["Jelinek","Frederick","","CLSP The Johns Hopkins University"]]}
{"id":"cs/9811025","submitter":"Ciprian Chelba","authors":"Ciprian Chelba (CLSP, The Johns Hopkins University, USA)","title":"A Structured Language Model","comments":"changed ACM-class membership, Proceedings of ACL-EACL'97, Student\n  Section, Madrid, Spain","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  The paper presents a language model that develops syntactic structure and\nuses it to extract meaningful information from the word history, thus enabling\nthe use of long distance dependencies. The model assigns probability to every\njoint sequence of words - binary-parse-structure with headword annotation. The\nmodel, its probabilistic parametrization, and a set of experiments meant to\nevaluate its predictive power are presented.\n","versions":[{"version":"v1","created":"Fri, 13 Nov 1998 16:53:15 GMT"},{"version":"v2","created":"Tue, 25 Jan 2000 15:46:48 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Chelba","Ciprian","","CLSP, The Johns Hopkins University, USA"]]}
{"id":"cs/9812001","submitter":"Li Hang","authors":"Hang LI (NEC Corporation)","title":"A Probabilistic Approach to Lexical Semantic Knowledge Acquisition and S\n  tructural Disambiguation","comments":"PhD. Thesis, Univ. of Tokyo, July 1998; latex file, eps figures; 136\n  pages, page numbers do not comfort to the original; ps font changes","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  In this thesis, I address the problem of automatically acquiring lexical\nsemantic knowledge, especially that of case frame patterns, from large corpus\ndata and using the acquired knowledge in structural disambiguation. The\napproach I adopt has the following characteristics: (1) dividing the problem\ninto three subproblems: case slot generalization, case dependency learning, and\nword clustering (thesaurus construction). (2) viewing each subproblem as that\nof statistical estimation and defining probability models for each subproblem,\n(3) adopting the Minimum Description Length (MDL) principle as learning\nstrategy, (4) employing efficient learning algorithms, and (5) viewing the\ndisambiguation problem as that of statistical prediction. Major contributions\nof this thesis include: (1) formalization of the lexical knowledge acquisition\nproblem, (2) development of a number of learning methods for lexical knowledge\nacquisition, and (3) development of a high-performance disambiguation method.\n","versions":[{"version":"v1","created":"Tue, 1 Dec 1998 11:43:32 GMT"},{"version":"v2","created":"Wed, 2 Dec 1998 01:24:22 GMT"},{"version":"v3","created":"Thu, 3 Dec 1998 04:18:33 GMT"}],"update_date":"2007-05-23","authors_parsed":[["LI","Hang","","NEC Corporation"]]}
{"id":"cs/9812005","submitter":"Oskari Heinonen","authors":"Oskari Heinonen (University of Helsinki)","title":"Optimal Multi-Paragraph Text Segmentation by Dynamic Programming","comments":"5 pages, 3 eps figures, LaTeX2e; includes errata; uses colacl, epsf,\n  times","journal-ref":"Proceedings of COLING-ACL '98, pp. 1484-1486, Montreal, Canada","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  There exist several methods of calculating a similarity curve, or a sequence\nof similarity values, representing the lexical cohesion of successive text\nconstituents, e.g., paragraphs. Methods for deciding the locations of fragment\nboundaries are, however, scarce. We propose a fragmentation method based on\ndynamic programming. The method is theoretically sound and guaranteed to\nprovide an optimal splitting on the basis of a similarity curve, a preferred\nfragment length, and a cost function defined. The method is especially useful\nwhen control on fragment size is of importance.\n","versions":[{"version":"v1","created":"Fri, 4 Dec 1998 16:16:35 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Heinonen","Oskari","","University of Helsinki"]]}
{"id":"cs/9812018","submitter":"Stephan Busemann","authors":"Stephan Busemann and Helmut Horacek (DFKI GmbH)","title":"A Flexible Shallow Approach to Text Generation","comments":"LaTeX, 10 pages","journal-ref":"Proc. 9th International Workshop on Natural Language Generation,\n  Niagara-on-the-Lake, Canada, August 1998, 238-247","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  In order to support the efficient development of NL generation systems, two\northogonal methods are currently pursued with emphasis: (1) reusable, general,\nand linguistically motivated surface realization components, and (2) simple,\ntask-oriented template-based techniques. In this paper we argue that, from an\napplication-oriented perspective, the benefits of both are still limited. In\norder to improve this situation, we suggest and evaluate shallow generation\nmethods associated with increased flexibility. We advise a close connection\nbetween domain-motivated and linguistic ontologies that supports the quick\nadaptation to new tasks and domains, rather than the reuse of general\nresources. Our method is especially designed for generating reports with\nlimited linguistic variations.\n","versions":[{"version":"v1","created":"Wed, 16 Dec 1998 16:37:01 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Busemann","Stephan","","DFKI GmbH"],["Horacek","Helmut","","DFKI GmbH"]]}
{"id":"cs/9901005","submitter":"Tom O'Hara","authors":"Janyce Wiebe, Thomas P. O'Hara, Thorsten Ohrstrom-Sandgren, and\n  Kenneth K. McKeever (New Mexico State University)","title":"An Empirical Approach to Temporal Reference Resolution (journal version)","comments":"Tar archive with LaTeX source, postscript figures, and style files","journal-ref":"Journal of Artificial Intelligence Research (JAIR), 9:247-293","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  Scheduling dialogs, during which people negotiate the times of appointments,\nare common in everyday life. This paper reports the results of an in-depth\nempirical investigation of resolving explicit temporal references in scheduling\ndialogs. There are four phases of this work: data annotation and evaluation,\nmodel development, system implementation and evaluation, and model evaluation\nand analysis. The system and model were developed primarily on one set of data,\nand then applied later to a much more complex data set, to assess the\ngeneralizability of the model for the task being performed. Many different\ntypes of empirical methods are applied to pinpoint the strengths and weaknesses\nof the approach. Detailed annotation instructions were developed and an\nintercoder reliability study was performed, showing that naive annotators can\nreliably perform the targeted annotations. A fully automatic system has been\ndeveloped and evaluated on unseen test data, with good results on both data\nsets. We adopt a pure realization of a recency-based focus model to identify\nprecisely when it is and is not adequate for the task being addressed. In\naddition to system results, an in-depth evaluation of the model itself is\npresented, based on detailed manual annotations. The results are that few\nerrors occur specifically due to the model of focus being used, and the set of\nanaphoric relations defined in the model are low in ambiguity for both data\nsets.\n","versions":[{"version":"v1","created":"Wed, 13 Jan 1999 17:37:00 GMT"}],"update_date":"2009-09-25","authors_parsed":[["Wiebe","Janyce","","New Mexico State University"],["O'Hara","Thomas P.","","New Mexico State University"],["Ohrstrom-Sandgren","Thorsten","","New Mexico State University"],["McKeever","Kenneth K.","","New Mexico State University"]]}
{"id":"cs/9902001","submitter":"Alexander Krotov","authors":"Alexander Krotov, Mark Hepple, Robert Gaizauskas and Yorick Wilks\n  (Department of Computer Science, University of Sheffield, UK)","title":"Compacting the Penn Treebank Grammar","comments":"5 pages, 2 figures","journal-ref":"In Proceedings of COLING-98 (Montreal), pages 699-703","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  Treebanks, such as the Penn Treebank (PTB), offer a simple approach to\nobtaining a broad coverage grammar: one can simply read the grammar off the\nparse trees in the treebank. While such a grammar is easy to obtain, a\nsquare-root rate of growth of the rule set with corpus size suggests that the\nderived grammar is far from complete and that much more treebanked text would\nbe required to obtain a complete grammar, if one exists at some limit. However,\nwe offer an alternative explanation in terms of the underspecification of\nstructures within the treebank. This hypothesis is explored by applying an\nalgorithm to compact the derived grammar by eliminating redundant rules --\nrules whose right hand sides can be parsed by other rules. The size of the\nresulting compacted grammar, which is significantly less than that of the full\ntreebank grammar, is shown to approach a limit. However, such a compacted\ngrammar does not yield very good performance figures. A version of the\ncompaction algorithm taking rule probabilities into account is proposed, which\nis argued to be more linguistically motivated. Combined with simple\nthresholding, this method can be used to give a 58% reduction in grammar size\nwithout significant change in parsing performance, and can produce a 69%\nreduction with some gain in recall, but a loss in precision.\n","versions":[{"version":"v1","created":"Sun, 31 Jan 1999 18:57:45 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Krotov","Alexander","","Department of Computer Science, University of Sheffield, UK"],["Hepple","Mark","","Department of Computer Science, University of Sheffield, UK"],["Gaizauskas","Robert","","Department of Computer Science, University of Sheffield, UK"],["Wilks","Yorick","","Department of Computer Science, University of Sheffield, UK"]]}
{"id":"cs/9902029","submitter":"Roberta Catizone","authors":"Yorick Wilks","title":"The \"Fodor\"-FODOR fallacy bites back","comments":null,"journal-ref":null,"doi":null,"report-no":"cs-98-13","categories":"cs.CL","license":null,"abstract":"  The paper argues that Fodor and Lepore are misguided in their attack on\nPustejovsky's Generative Lexicon, largely because their argument rests on a\ntraditional, but implausible and discredited, view of the lexicon on which it\nis effectively empty of content, a view that stands in the long line of\nexplaining word meaning (a) by ostension and then (b) explaining it by means of\na vacuous symbol in a lexicon, often the word itself after typographic\ntransmogrification. (a) and (b) both share the wrong belief that to a word must\ncorrespond a simple entity that is its meaning. I then turn to the semantic\nrules that Pustejovsky uses and argue first that, although they have novel\nfeatures, they are in a well-established Artificial Intelligence tradition of\nexplaining meaning by reference to structures that mention other structures\nassigned to words that may occur in close proximity to the first. It is argued\nthat Fodor and Lepore's view that there cannot be such rules is without\nfoundation, and indeed systems using such rules have proved their practical\nworth in computational systems. Their justification descends from line of\nargument, whose high points were probably Wittgenstein and Quine that meaning\nis not to be understood by simple links to the world, ostensive or otherwise,\nbut by the relationship of whole cultural representational structures to each\nother and to the world as a whole.\n","versions":[{"version":"v1","created":"Thu, 25 Feb 1999 14:41:24 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Wilks","Yorick",""]]}
{"id":"cs/9902030","submitter":"Roberta Catizone","authors":"Yorick Wilks","title":"Is Word Sense Disambiguation just one more NLP task?","comments":null,"journal-ref":null,"doi":null,"report-no":"cs-98-12","categories":"cs.CL","license":null,"abstract":"  This paper compares the tasks of part-of-speech (POS) tagging and\nword-sense-tagging or disambiguation (WSD), and argues that the tasks are not\nrelated by fineness of grain or anything like that, but are quite different\nkinds of task, particularly becuase there is nothing in POS corresponding to\nsense novelty. The paper also argues for the reintegration of sub-tasks that\nare being separated for evaluation\n","versions":[{"version":"v1","created":"Thu, 25 Feb 1999 14:41:32 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Wilks","Yorick",""]]}
{"id":"cs/9903003","submitter":"Steven Bird","authors":"Steven Bird and Mark Liberman (University of Pennsylvania)","title":"A Formal Framework for Linguistic Annotation","comments":"49 pages","journal-ref":null,"doi":null,"report-no":"Tech Report MS-CIS-99-01, Dept of Computer and Information Science","categories":"cs.CL","license":null,"abstract":"  `Linguistic annotation' covers any descriptive or analytic notations applied\nto raw language data. The basic data may be in the form of time functions --\naudio, video and/or physiological recordings -- or it may be textual. The added\nnotations may include transcriptions of all sorts (from phonetic features to\ndiscourse structures), part-of-speech and sense tagging, syntactic analysis,\n`named entity' identification, co-reference annotation, and so on. While there\nare several ongoing efforts to provide formats and tools for such annotations\nand to publish annotated linguistic databases, the lack of widely accepted\nstandards is becoming a critical problem. Proposed standards, to the extent\nthey exist, have focussed on file formats. This paper focuses instead on the\nlogical structure of linguistic annotations. We survey a wide variety of\nexisting annotation formats and demonstrate a common conceptual core, the\nannotation graph. This provides a formal framework for constructing,\nmaintaining and searching linguistic annotations, while remaining consistent\nwith many alternative data structures and file formats.\n","versions":[{"version":"v1","created":"Tue, 2 Mar 1999 12:30:55 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Bird","Steven","","University of Pennsylvania"],["Liberman","Mark","","University of Pennsylvania"]]}
{"id":"cs/9903008","submitter":"Diane Litman","authors":"Diane J. Litman (AT&T Labs - Research) and Shimei Pan (Columbia\n  University)","title":"Empirically Evaluating an Adaptable Spoken Dialogue System","comments":"to be published in the Proceedings of the 7th International\n  Conference on User Modeling (UM'99); uses llncs.cls,um97.sty","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  Recent technological advances have made it possible to build real-time,\ninteractive spoken dialogue systems for a wide variety of applications.\nHowever, when users do not respect the limitations of such systems, performance\ntypically degrades. Although users differ with respect to their knowledge of\nsystem limitations, and although different dialogue strategies make system\nlimitations more apparent to users, most current systems do not try to improve\nperformance by adapting dialogue behavior to individual users. This paper\npresents an empirical evaluation of TOOT, an adaptable spoken dialogue system\nfor retrieving train schedules on the web. We conduct an experiment in which 20\nusers carry out 4 tasks with both adaptable and non-adaptable versions of TOOT,\nresulting in a corpus of 80 dialogues. The values for a wide range of\nevaluation measures are then extracted from this corpus. Our results show that\nadaptable TOOT generally outperforms non-adaptable TOOT, and that the utility\nof adaptation depends on TOOT's initial dialogue strategies.\n","versions":[{"version":"v1","created":"Fri, 5 Mar 1999 22:03:13 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Litman","Diane J.","","AT&T Labs - Research"],["Pan","Shimei","","Columbia\n  University"]]}
{"id":"cs/9904008","submitter":"Gertjan van Noord","authors":"Dale Gerdemann and Gertjan van Noord","title":"Transducers from Rewrite Rules with Backreferences","comments":"8 pages, EACL 1999 Bergen","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  Context sensitive rewrite rules have been widely used in several areas of\nnatural language processing, including syntax, morphology, phonology and speech\nprocessing. Kaplan and Kay, Karttunen, and Mohri & Sproat have given various\nalgorithms to compile such rewrite rules into finite-state transducers. The\npresent paper extends this work by allowing a limited form of backreferencing\nin such rules. The explicit use of backreferencing leads to more elegant and\ngeneral solutions.\n","versions":[{"version":"v1","created":"Thu, 15 Apr 1999 14:00:41 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Gerdemann","Dale",""],["van Noord","Gertjan",""]]}
{"id":"cs/9904009","submitter":"Mark Lee","authors":"Mark Lee and Yorick Wilks","title":"An ascription-based approach to speech acts","comments":"6 pages","journal-ref":"Proceedings of COLING`96, Copenhagen. (1996)","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  The two principal areas of natural language processing research in pragmatics\nare belief modelling and speech act processing. Belief modelling is the\ndevelopment of techniques to represent the mental attitudes of a dialogue\nparticipant. The latter approach, speech act processing, based on speech act\ntheory, involves viewing dialogue in planning terms. Utterances in a dialogue\nare modelled as steps in a plan where understanding an utterance involves\nderiving the complete plan a speaker is attempting to achieve. However,\nprevious speech act based approaches have been limited by a reliance upon\nrelatively simplistic belief modelling techniques and their relationship to\nplanning and plan recognition. In particular, such techniques assume\nprecomputed nested belief structures. In this paper, we will present an\napproach to speech act processing based on novel belief modelling techniques\nwhere nested beliefs are propagated on demand.\n","versions":[{"version":"v1","created":"Thu, 15 Apr 1999 16:03:27 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Lee","Mark",""],["Wilks","Yorick",""]]}
{"id":"cs/9904018","submitter":"Janet E. Cahn","authors":"Janet E. Cahn","title":"A Computational Memory and Processing Model for Processing for Prosody","comments":"4 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  This paper links prosody to the information in a text and how it is processed\nby the speaker. It describes the operation and output of LOQ, a text-to-speech\nimplementation that includes a model of limited attention and working memory.\nAttentional limitations are key. Varying the attentional parameter in the\nsimulations varies in turn what counts as given and new in a text, and\ntherefore, the intonational contours with which it is uttered. Currently, the\nsystem produces prosody in three different styles: child-like, adult\nexpressive, and knowledgeable. This prosody also exhibits differences within\neach style -- no two simulations are alike. The limited resource approach\ncaptures some of the stylistic and individual variety found in natural prosody.\n","versions":[{"version":"v1","created":"Sat, 24 Apr 1999 23:45:26 GMT"}],"update_date":"2022-04-04","authors_parsed":[["Cahn","Janet E.",""]]}
{"id":"cs/9905001","submitter":"Rebecca Hwa","authors":"Rebecca Hwa","title":"Supervised Grammar Induction Using Training Data with Limited\n  Constituent Information","comments":"7 pages, 2 figures, to appear in the proc. of ACL '99","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  Corpus-based grammar induction generally relies on hand-parsed training data\nto learn the structure of the language. Unfortunately, the cost of building\nlarge annotated corpora is prohibitively expensive. This work aims to improve\nthe induction strategy when there are few labels in the training data. We show\nthat the most informative linguistic constituents are the higher nodes in the\nparse trees, typically denoting complex noun phrases and sentential clauses.\nThey account for only 20% of all constituents. For inducing grammars from\nsparsely labeled training data (e.g., only higher-level constituent labels), we\npropose an adaptation strategy, which produces grammars that parse almost as\nwell as grammars induced from fully labeled corpora. Our results suggest that\nfor a partial parser to replace human annotators, it must be able to\nautomatically extract higher-level constituents rather than base noun phrases.\n","versions":[{"version":"v1","created":"Sun, 2 May 1999 20:48:21 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Hwa","Rebecca",""]]}
{"id":"cs/9906003","submitter":"Melanie Siegel","authors":"Melanie Siegel","title":"The syntactic processing of particles in Japanese spoken language","comments":"8 pages","journal-ref":"Proceedings of the 13th Pacific Asia Conference on Language,\n  Information and Computation. 1999","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  Particles fullfill several distinct central roles in the Japanese language.\nThey can mark arguments as well as adjuncts, can be functional or have semantic\nfuntions. There is, however, no straightforward matching from particles to\nfunctions, as, e.g., GA can mark the subject, the object or an adjunct of a\nsentence. Particles can cooccur. Verbal arguments that could be identified by\nparticles can be eliminated in the Japanese sentence. And finally, in spoken\nlanguage particles are often omitted. A proper treatment of particles is thus\nnecessary to make an analysis of Japanese sentences possible. Our treatment is\nbased on an empirical investigation of 800 dialogues. We set up a type\nhierarchy of particles motivated by their subcategorizational and\nmodificational behaviour. This type hierarchy is part of the Japanese syntax in\nVERBMOBIL.\n","versions":[{"version":"v1","created":"Wed, 2 Jun 1999 12:03:14 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Siegel","Melanie",""]]}
{"id":"cs/9906009","submitter":"Thorsten Brants","authors":"Thorsten Brants","title":"Cascaded Markov Models","comments":"8 pages","journal-ref":"Proceedings of EACL-99, Bergen, Norway","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  This paper presents a new approach to partial parsing of context-free\nstructures. The approach is based on Markov Models. Each layer of the resulting\nstructure is represented by its own Markov Model, and output of a lower layer\nis passed as input to the next higher layer. An empirical evaluation of the\nmethod yields very good results for NP/PP chunking of German newspaper texts.\n","versions":[{"version":"v1","created":"Sun, 6 Jun 1999 17:36:34 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Brants","Thorsten",""]]}
{"id":"cs/9906014","submitter":"Gertjan van Noord","authors":"Gert Veldhuijzen van Zanten and Gosse Bouma and Khalil Sima'an and\n  Gertjan van Noord and Remko Bonnema","title":"Evaluation of the NLP Components of the OVIS2 Spoken Dialogue System","comments":"Proceedings of CLIN 99","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  The NWO Priority Programme Language and Speech Technology is a 5-year\nresearch programme aiming at the development of spoken language information\nsystems. In the Programme, two alternative natural language processing (NLP)\nmodules are developed in parallel: a grammar-based (conventional, rule-based)\nmodule and a data-oriented (memory-based, stochastic, DOP) module. In order to\ncompare the NLP modules, a formal evaluation has been carried out three years\nafter the start of the Programme. This paper describes the evaluation procedure\nand the evaluation results. The grammar-based component performs much better\nthan the data-oriented one in this comparison.\n","versions":[{"version":"v1","created":"Mon, 14 Jun 1999 10:06:31 GMT"}],"update_date":"2007-05-23","authors_parsed":[["van Zanten","Gert Veldhuijzen",""],["Bouma","Gosse",""],["Sima'an","Khalil",""],["van Noord","Gertjan",""],["Bonnema","Remko",""]]}
{"id":"cs/9906015","submitter":"Alexander Yeh","authors":"Lisa Ferro, Marc Vilain and Alexander Yeh","title":"Learning Transformation Rules to Find Grammatical Relations","comments":"10 pages. Uses latex-acl.sty and named.sty","journal-ref":"Computational Natural Language Learning (CoNLL-99), pages 43-52,\n  June, 1999. Bergen, Norway","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  Grammatical relationships are an important level of natural language\nprocessing. We present a trainable approach to find these relationships through\ntransformation sequences and error-driven learning. Our approach finds\ngrammatical relationships between core syntax groups and bypasses much of the\nparsing phase. On our training and test set, our procedure achieves 63.6%\nrecall and 77.3% precision (f-score = 69.8).\n","versions":[{"version":"v1","created":"Mon, 14 Jun 1999 22:06:24 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Ferro","Lisa",""],["Vilain","Marc",""],["Yeh","Alexander",""]]}
{"id":"cs/9906020","submitter":"Ion Androutsopoulos","authors":"I. Androutsopoulos (Software & Knowledge Engineering Lab, Institute of\n  Informatics & Telecommunications, NCSR Demokritos, Greece)","title":"Temporal Meaning Representations in a Natural Language Front-End","comments":"15 pages. To appear in the Proceedings of the 12th International\n  Symposium on Languages for Intensional Programming, Athens, Greece, 1999","journal-ref":"In Gergatsoulis, M. and Rondogiannis, P. (Eds.), Intensional\n  Programming II (Proceedings of the 12th International Symposium on Languages\n  for Intensional Programming, Athens, Greece, 1999), pp. 197-213, World\n  Scientific, 2000.","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  Previous work in the context of natural language querying of temporal\ndatabases has established a method to map automatically from a large subset of\nEnglish time-related questions to suitable expressions of a temporal logic-like\nlanguage, called TOP. An algorithm to translate from TOP to the TSQL2 temporal\ndatabase language has also been defined. This paper shows how TOP expressions\ncould be translated into a simpler logic-like language, called BOT. BOT is very\nclose to traditional first-order predicate logic (FOPL), and hence existing\nmethods to manipulate FOPL expressions can be exploited to interface to\ntime-sensitive applications other than TSQL2 databases, maintaining the\nexisting English-to-TOP mapping.\n","versions":[{"version":"v1","created":"Tue, 22 Jun 1999 08:28:26 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Androutsopoulos","I.","","Software & Knowledge Engineering Lab, Institute of\n  Informatics & Telecommunications, NCSR Demokritos, Greece"]]}
{"id":"cs/9906025","submitter":"Lluis Padro","authors":"J. Daude, L. Padro and G. Rigau (TALP Research Center. LSI Dept.\n  Universitat Politecnica de Catalunya. Barcelona)","title":"Mapping Multilingual Hierarchies Using Relaxation Labeling","comments":"8 pages. 1 eps figure","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  This paper explores the automatic construction of a multilingual Lexical\nKnowledge Base from pre-existing lexical resources. We present a new and robust\napproach for linking already existing lexical/semantic hierarchies. We used a\nconstraint satisfaction algorithm (relaxation labeling) to select --among all\nthe candidate translations proposed by a bilingual dictionary-- the right\nEnglish WordNet synset for each sense in a taxonomy automatically derived from\na Spanish monolingual dictionary. Although on average, there are 15 possible\nWordNet connections for each sense in the taxonomy, the method achieves an\naccuracy over 80%. Finally, we also propose several ways in which this\ntechnique could be applied to enrich and improve existing lexical databases.\n","versions":[{"version":"v1","created":"Thu, 24 Jun 1999 16:56:45 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Daude","J.","","TALP Research Center. LSI Dept.\n  Universitat Politecnica de Catalunya. Barcelona"],["Padro","L.","","TALP Research Center. LSI Dept.\n  Universitat Politecnica de Catalunya. Barcelona"],["Rigau","G.","","TALP Research Center. LSI Dept.\n  Universitat Politecnica de Catalunya. Barcelona"]]}
{"id":"cs/9906026","submitter":"Gertjan van Noord","authors":"Gertjan van Noord and Gosse Bouma and Rob Koeling and Mark-Jan\n  Nederhof","title":"Robust Grammatical Analysis for Spoken Dialogue Systems","comments":"Accepted for JNLE","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  We argue that grammatical analysis is a viable alternative to concept\nspotting for processing spoken input in a practical spoken dialogue system. We\ndiscuss the structure of the grammar, and a model for robust parsing which\ncombines linguistic sources of information and statistical sources of\ninformation. We discuss test results suggesting that grammatical processing\nallows fast and accurate processing of spoken input.\n","versions":[{"version":"v1","created":"Fri, 25 Jun 1999 08:16:23 GMT"}],"update_date":"2016-08-31","authors_parsed":[["van Noord","Gertjan",""],["Bouma","Gosse",""],["Koeling","Rob",""],["Nederhof","Mark-Jan",""]]}
{"id":"cs/9906034","submitter":"Davide Turcato","authors":"Davide Turcato, Paul McFetridge, Fred Popowich, Janine Toole","title":"A Unified Example-Based and Lexicalist Approach to Machine Translation","comments":"11 pages, to be presented at the 8th International Conference on\n  Theoretical and Methodological Issues in Machine Translation (TMI-99)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  We present an approach to Machine Translation that combines the ideas and\nmethodologies of the Example-Based and Lexicalist theoretical frameworks. The\napproach has been implemented in a multilingual Machine Translation system.\n","versions":[{"version":"v1","created":"Wed, 30 Jun 1999 23:06:09 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Turcato","Davide",""],["McFetridge","Paul",""],["Popowich","Fred",""],["Toole","Janine",""]]}
{"id":"cs/9907003","submitter":"Steven Bird","authors":"Steven Bird and Mark Liberman","title":"Annotation graphs as a framework for multidimensional linguistic data\n  analysis","comments":"10 pages, 10 figures, Towards Standards and Tools for Discourse\n  Tagging, Proceedings of the Workshop. pp. 1-10. Association for Computational\n  Linguistics","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  In recent work we have presented a formal framework for linguistic annotation\nbased on labeled acyclic digraphs. These `annotation graphs' offer a simple yet\npowerful method for representing complex annotation structures incorporating\nhierarchy and overlap. Here, we motivate and illustrate our approach using\ndiscourse-level annotations of text and speech data drawn from the CALLHOME,\nCOCONUT, MUC-7, DAMSL and TRAINS annotation schemes. With the help of domain\nspecialists, we have constructed a hybrid multi-level annotation for a fragment\nof the Boston University Radio Speech Corpus which includes the following\nlevels: segment, word, breath, ToBI, Tilt, Treebank, coreference and named\nentity. We show how annotation graphs can represent hybrid multi-level\nstructures which derive from a diverse set of file formats. We also show how\nthe approach facilitates substantive comparison of multiple annotations of a\nsingle signal based on different theoretical models. The discussion shows how\nannotation graphs open the door to wide-ranging integration of tools, formats\nand corpora.\n","versions":[{"version":"v1","created":"Mon, 5 Jul 1999 14:51:26 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Bird","Steven",""],["Liberman","Mark",""]]}
{"id":"cs/9907006","submitter":"Erik Tjong Kim Sang","authors":"Erik F. Tjong Kim Sang and Jorn Veenstra","title":"Representing Text Chunks","comments":"7 pages","journal-ref":"EACL'99, Bergen","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  Dividing sentences in chunks of words is a useful preprocessing step for\nparsing, information extraction and information retrieval. (Ramshaw and Marcus,\n1995) have introduced a \"convenient\" data representation for chunking by\nconverting it to a tagging task. In this paper we will examine seven different\ndata representations for the problem of recognizing noun phrase chunks. We will\nshow that the the data representation choice has a minor influence on chunking\nperformance. However, equipped with the most suitable data representation, our\nmemory-based learning chunker was able to improve the best published chunking\nresults for a standard data set.\n","versions":[{"version":"v1","created":"Tue, 6 Jul 1999 12:44:20 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Sang","Erik F. Tjong Kim",""],["Veenstra","Jorn",""]]}
{"id":"cs/9907007","submitter":"Atsushi Fujii","authors":"Atsushi Fujii and Tetsuya Ishikawa","title":"Cross-Language Information Retrieval for Technical Documents","comments":"9 pages, 5 Postscript figures, uses colacl.sty and psfig.tex","journal-ref":"Proceedings of the Joint ACL SIGDAT Conference on Empirical\n  Methods in Natural Language Processing and Very Large Corpora, pp.29-37, 1999","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  This paper proposes a Japanese/English cross-language information retrieval\n(CLIR) system targeting technical documents. Our system first translates a\ngiven query containing technical terms into the target language, and then\nretrieves documents relevant to the translated query. The translation of\ntechnical terms is still problematic in that technical terms are often compound\nwords, and thus new terms can be progressively created simply by combining\nexisting base words. In addition, Japanese often represents loanwords based on\nits phonogram. Consequently, existing dictionaries find it difficult to achieve\nsufficient coverage. To counter the first problem, we use a compound word\ntranslation method, which uses a bilingual dictionary for base words and\ncollocational statistics to resolve translation ambiguity. For the second\nproblem, we propose a transliteration method, which identifies phonetic\nequivalents in the target language. We also show the effectiveness of our\nsystem using a test collection for CLIR.\n","versions":[{"version":"v1","created":"Tue, 6 Jul 1999 16:25:46 GMT"},{"version":"v2","created":"Wed, 7 Jul 1999 09:12:31 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Fujii","Atsushi",""],["Ishikawa","Tetsuya",""]]}
{"id":"cs/9907008","submitter":"Janine Toole","authors":"Janine Toole, Fred Popowich, Devlan Nicholson, Davide Turcato, Paul\n  McFetridge","title":"Explanation-based Learning for Machine Translation","comments":"12 pages, 3 figures, To appear in Proceedings of the 8th\n  International Conference on Theoretical and Methodological Issues in Machine\n  Translation","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  In this paper we present an application of explanation-based learning (EBL)\nin the parsing module of a real-time English-Spanish machine translation system\ndesigned to translate closed captions. We discuss the efficiency/coverage\ntrade-offs available in EBL and introduce the techniques we use to increase\ncoverage while maintaining a high level of space and time efficiency. Our\nperformance results indicate that this approach is effective.\n","versions":[{"version":"v1","created":"Tue, 6 Jul 1999 18:35:41 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Toole","Janine",""],["Popowich","Fred",""],["Nicholson","Devlan",""],["Turcato","Davide",""],["McFetridge","Paul",""]]}
{"id":"cs/9907010","submitter":"David Elworthy","authors":"David Elworthy","title":"Language Identification With Confidence Limits","comments":"8 pages; needs colacl.sty. Appeared in Proceedings of the Sixth\n  Workshop on Very Large Corpora (COLING-ACL 98)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  A statistical classification algorithm and its application to language\nidentification from noisy input are described. The main innovation is to\ncompute confidence limits on the classification, so that the algorithm\nterminates when enough evidence to make a clear decision has been made, and so\navoiding problems with categories that have similar characteristics. A second\napplication, to genre identification, is briefly examined. The results show\nthat some of the problems of other language identification techniques can be\navoided, and illustrate a more important point: that a statistical language\nprocess can be used to provide feedback about its own success rate.\n","versions":[{"version":"v1","created":"Wed, 7 Jul 1999 09:28:40 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Elworthy","David",""]]}
{"id":"cs/9907012","submitter":"Guido Minnen","authors":"Guido Minnen (University of Sussex)","title":"Selective Magic HPSG Parsing","comments":"9 pages, LaTeX with 4 postscript figures (uses avm.sty, eaclap.sty\n  and psfig-scale.sty)","journal-ref":"Proceedings of EACL99, Bergen, Norway, June 8-11","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  We propose a parser for constraint-logic grammars implementing HPSG that\ncombines the advantages of dynamic bottom-up and advanced top-down control. The\nparser allows the user to apply magic compilation to specific constraints in a\ngrammar which as a result can be processed dynamically in a bottom-up and\ngoal-directed fashion. State of the art top-down processing techniques are used\nto deal with the remaining constraints. We discuss various aspects concerning\nthe implementation of the parser as part of a grammar development system.\n","versions":[{"version":"v1","created":"Thu, 8 Jul 1999 09:46:37 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Minnen","Guido","","University of Sussex"]]}
{"id":"cs/9907013","submitter":"Guido Minnen","authors":"John Carroll, Guido Minnen (University of Sussex), Ted Briscoe\n  (Cambridge University)","title":"Corpus Annotation for Parser Evaluation","comments":"7 pages, LaTeX (uses eaclap.sty)","journal-ref":"Proceedings of the EACL99 workshop on Linguistically Interpreted\n  Corpora (LINC), Bergen, Norway, June 12","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  We describe a recently developed corpus annotation scheme for evaluating\nparsers that avoids shortcomings of current methods. The scheme encodes\ngrammatical relations between heads and dependents, and has been used to mark\nup a new public-domain corpus of naturally occurring English text. We show how\nthe corpus can be used to evaluate the accuracy of a robust parser, and relate\nthe corpus to extant resources.\n","versions":[{"version":"v1","created":"Thu, 8 Jul 1999 10:08:59 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Carroll","John","","University of Sussex"],["Minnen","Guido","","University of Sussex"],["Briscoe","Ted","","Cambridge University"]]}
{"id":"cs/9907017","submitter":"Davide Turcato","authors":"Davide Turcato, Paul McFetridge, Fred Popowich and Janine Toole","title":"A Bootstrap Approach to Automatically Generating Lexical Transfer Rules","comments":"8 pages, 1 figure, to be presented at Machine Translation Summit VII,\n  September 13-17, 1999, Singapore","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  We describe a method for automatically generating Lexical Transfer Rules\n(LTRs) from word equivalences using transfer rule templates. Templates are\nskeletal LTRs, unspecified for words. New LTRs are created by instantiating a\ntemplate with words, provided that the words belong to the appropriate lexical\ncategories required by the template. We define two methods for creating an\ninventory of templates and using them to generate new LTRs. A simpler method\nconsists of extracting a finite set of templates from a sample of hand coded\nLTRs and directly using them in the generation process. A further method\nconsists of abstracting over the initial finite set of templates to define\nhigher level templates, where bilingual equivalences are defined in terms of\ncorrespondences involving phrasal categories. Phrasal templates are then mapped\nonto sets of lexical templates with the aid of grammars. In this way an\ninfinite set of lexical templates is recursively defined. New LTRs are created\nby parsing input words, matching a template at the phrasal level and using the\ncorresponding lexical categories to instantiate the lexical template. The\ndefinition of an infinite set of templates enables the automatic creation of\nLTRs for multi-word, non-compositional word equivalences of any cardinality.\n","versions":[{"version":"v1","created":"Fri, 9 Jul 1999 22:39:52 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Turcato","Davide",""],["McFetridge","Paul",""],["Popowich","Fred",""],["Toole","Janine",""]]}
{"id":"cs/9907021","submitter":"Jorg Spilker","authors":"Guenther Goerz, Joerg Spilker, Volker Strom, Hans Weber","title":"Architectural Considerations for Conversational Systems -- The\n  Verbmobil/INTARC Experience","comments":"10 pages, to appear in proceedings of First International Workshop on\n  Human Computer Conversation, Bellagio, Italy","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  The paper describes the speech to speech translation system INTARC, developed\nduring the first phase of the Verbmobil project. The general design goals of\nthe INTARC system architecture were time synchronous processing as well as\nincrementality and interactivity as a means to achieve a higher degree of\nrobustness and scalability. Interactivity means that in addition to the\nbottom-up (in terms of processing levels) data flow the ability to process\ntop-down restrictions considering the same signal segment for all processing\nlevels. The construction of INTARC 2.0, which has been operational since fall\n1996, followed an engineering approach focussing on the integration of symbolic\n(linguistic) and stochastic (recognition) techniques which led to a\ngeneralization of the concept of a ``one pass'' beam search.\n","versions":[{"version":"v1","created":"Wed, 14 Jul 1999 09:21:16 GMT"}],"update_date":"2019-08-17","authors_parsed":[["Goerz","Guenther",""],["Spilker","Joerg",""],["Strom","Volker",""],["Weber","Hans",""]]}
{"id":"cs/9908001","submitter":"Zvi Marx","authors":"Zvika Marx (1 and 2), Ido Dagan (1), Eli Shamir (2) ((1) Bar-Ilan\n  University, (2) The Hebrew University of Jerusalem)","title":"Detecting Sub-Topic Correspondence through Bipartite Term Clustering","comments":"html with 3 gif figures; generated from 7 pages MS-Word file","journal-ref":"Proceedings of ACL'99 Workshop on Unsupervised Learning in Natural\n  Language Processing, 1999, pp 45-51","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  This paper addresses a novel task of detecting sub-topic correspondence in a\npair of text fragments, enhancing common notions of text similarity. This task\nis addressed by coupling corresponding term subsets through bipartite\nclustering. The paper presents a cost-based clustering scheme and compares it\nwith a bipartite version of the single-link method, providing illustrating\nresults.\n","versions":[{"version":"v1","created":"Sun, 1 Aug 1999 14:02:57 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Marx","Zvika","","1 and 2"],["Dagan","Ido",""],["Shamir","Eli",""]]}
{"id":"cs/9909002","submitter":"Vincenzo Pallotta","authors":"Afzal Ballim and Vincenzo Pallotta","title":"Semantic robust parsing for noun extraction from natural language\n  queries","comments":null,"journal-ref":"Proceedings of WPDI'99 (Workshop on Procedures in Discourse\n  Interpretation),1999, Iasi - Romania","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  This paper describes how robust parsing techniques can be fruitful applied\nfor building a query generation module which is part of a pipelined NLP\narchitecture aimed at process natural language queries in a restricted domain.\nWe want to show that semantic robustness represents a key issue in those NLP\nsystems where it is more likely to have partial and ill-formed utterances due\nto various factors (e.g. noisy environments, low quality of speech recognition\nmodules, etc...) and where it is necessary to succeed, even if partially, in\nextracting some meaningful information.\n","versions":[{"version":"v1","created":"Thu, 2 Sep 1999 15:53:07 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Ballim","Afzal",""],["Pallotta","Vincenzo",""]]}
{"id":"cs/9910020","submitter":"Atsushi Fujii","authors":"Atsushi Fujii, Kentaro Inui, Takenobu Tokunaga, and Hozumi Tanaka","title":"Selective Sampling for Example-based Word Sense Disambiguation","comments":"25 pages, 14 Postscript figures","journal-ref":"Computational Linguistics, Vol.24, No.4, pp.573-597, 1998","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  This paper proposes an efficient example sampling method for example-based\nword sense disambiguation systems. To construct a database of practical size, a\nconsiderable overhead for manual sense disambiguation (overhead for\nsupervision) is required. In addition, the time complexity of searching a\nlarge-sized database poses a considerable problem (overhead for search). To\ncounter these problems, our method selectively samples a smaller-sized\neffective subset from a given example set for use in word sense disambiguation.\nOur method is characterized by the reliance on the notion of training utility:\nthe degree to which each example is informative for future example sampling\nwhen used for the training of the system. The system progressively collects\nexamples by selecting those with greatest utility. The paper reports the\neffectiveness of our method through experiments on about one thousand\nsentences. Compared to experiments with other example sampling methods, our\nmethod reduced both the overhead for supervision and the overhead for search,\nwithout the degeneration of the performance of the system.\n","versions":[{"version":"v1","created":"Sat, 23 Oct 1999 11:19:35 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Fujii","Atsushi",""],["Inui","Kentaro",""],["Tokunaga","Takenobu",""],["Tanaka","Hozumi",""]]}
{"id":"cs/9910022","submitter":"Mark-Jan Nederhof","authors":"Mark-Jan Nederhof","title":"Practical experiments with regular approximation of context-free\n  languages","comments":"28 pages. To appear in Computational Linguistics 26(1), March 2000","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  Several methods are discussed that construct a finite automaton given a\ncontext-free grammar, including both methods that lead to subsets and those\nthat lead to supersets of the original context-free language. Some of these\nmethods of regular approximation are new, and some others are presented here in\na more refined form with respect to existing literature. Practical experiments\nwith the different methods of regular approximation are performed for\nspoken-language input: hypotheses from a speech recognizer are filtered through\na finite automaton.\n","versions":[{"version":"v1","created":"Mon, 25 Oct 1999 15:00:52 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Nederhof","Mark-Jan",""]]}
{"id":"cs/9911006","submitter":"Masaki Murata","authors":"M. Murata, M. Utiyama, H. Isahara (CRL)","title":"Question Answering System Using Syntactic Information","comments":"6 pages, 0 figures. Computation and Language","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  Question answering task is now being done in TREC8 using English documents.\nWe examined question answering task in Japanese sentences. Our method selects\nthe answer by matching the question sentence with knowledge-based data written\nin natural language. We use syntactic information to obtain highly accurate\nanswers.\n","versions":[{"version":"v1","created":"Mon, 15 Nov 1999 05:48:03 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Murata","M.","","CRL"],["Utiyama","M.","","CRL"],["Isahara","H.","","CRL"]]}
{"id":"cs/9911011","submitter":"Markus Walther","authors":"Markus Walther (University of Marburg)","title":"One-Level Prosodic Morphology","comments":"64 pages, A4 size, LaTeX2e, 16 PostScript figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  Recent developments in theoretical linguistics have lead to a widespread\nacceptance of constraint-based analyses of prosodic morphology phenomena such\nas truncation, infixation, floating morphemes and reduplication. Of these,\nreduplication is particularly challenging for state-of-the-art computational\nmorphology, since it involves copying of some part of a phonological string. In\nthis paper I argue for certain extensions to the one-level model of phonology\nand morphology (Bird & Ellison 1994) to cover the computational aspects of\nprosodic morphology using finite-state methods. In a nutshell, enriched lexical\nrepresentations provide additional automaton arcs to repeat or skip sounds and\nalso to allow insertion of additional material. A kind of resource\nconsciousness is introduced to control this additional freedom, distinguishing\nbetween producer and consumer arcs. The non-finite-state copying aspect of\nreduplication is mapped to automata intersection, itself a non-finite-state\noperation. Bounded local optimization prunes certain automaton arcs that fail\nto contribute to linguistic optimisation criteria. The paper then presents\nimplemented case studies of Ulwa construct state infixation, German\nhypocoristic truncation and Tagalog over-applying reduplication that illustrate\nthe expressive power of this approach, before its merits and limitations are\ndiscussed and possible extensions are sketched. I conclude that the one-level\napproach to prosodic morphology presents an attractive way of extending\nfinite-state techniques to difficult phenomena that hitherto resisted elegant\ncomputational analyses.\n","versions":[{"version":"v1","created":"Fri, 19 Nov 1999 16:10:51 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Walther","Markus","","University of Marburg"]]}
{"id":"cs/9912003","submitter":"Masaki Murata","authors":"M. Murata, H. Isahara (CRL), M. Nagao (Kyoto University)","title":"Resolution of Indirect Anaphora in Japanese Sentences Using Examples 'X\n  no Y (Y of X)'","comments":"8 pages, 0 figures. Computation and Language","journal-ref":"ACL'99 Workshop on 'Coreference and Its Applications', Maryland,\n  USA, June 22, 1999","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  A noun phrase can indirectly refer to an entity that has already been\nmentioned. For example, ``I went into an old house last night. The roof was\nleaking badly and ...'' indicates that ``the roof'' is associated with `` an\nold house}'', which was mentioned in the previous sentence. This kind of\nreference (indirect anaphora) has not been studied well in natural language\nprocessing, but is important for coherence resolution, language understanding,\nand machine translation. In order to analyze indirect anaphora, we need a case\nframe dictionary for nouns that contains knowledge of the relationships between\ntwo nouns but no such dictionary presently exists. Therefore, we are forced to\nuse examples of ``X no Y'' (Y of X) and a verb case frame dictionary instead.\nWe tried estimating indirect anaphora using this information and obtained a\nrecall rate of 63% and a precision rate of 68% on test sentences. This\nindicates that the information of ``X no Y'' is useful to a certain extent when\nwe cannot make use of a noun case frame dictionary. We estimated the results\nthat would be given by a noun case frame dictionary, and obtained recall and\nprecision rates of 71% and 82% respectively. Finally, we proposed a way to\nconstruct a noun case frame dictionary by using examples of ``X no Y.''\n","versions":[{"version":"v1","created":"Mon, 13 Dec 1999 04:42:25 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Murata","M.","","CRL"],["Isahara","H.","","CRL"],["Nagao","M.","","Kyoto University"]]}
{"id":"cs/9912004","submitter":"Masaki Murata","authors":"M. Murata, H. Isahara (CRL), M. Nagao (Kyoto University)","title":"Pronoun Resolution in Japanese Sentences Using Surface Expressions and\n  Examples","comments":"8 pages, 0 figures. Computation and Language","journal-ref":"ACL'99 Workshop on 'Coreference and Its Applications', Maryland,\n  USA, June 22, 1999","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  In this paper, we present a method of estimating referents of demonstrative\npronouns, personal pronouns, and zero pronouns in Japanese sentences using\nexamples, surface expressions, topics and foci. Unlike conventional work which\nwas semantic markers for semantic constraints, we used examples for semantic\nconstraints and showed in our experiments that examples are as useful as\nsemantic markers. We also propose many new methods for estimating referents of\npronouns. For example, we use the form ``X of Y'' for estimating referents of\ndemonstrative adjectives. In addition to our new methods, we used many\nconventional methods. As a result, experiments using these methods obtained a\nprecision rate of 87% in estimating referents of demonstrative pronouns,\npersonal pronouns, and zero pronouns for training sentences, and obtained a\nprecision rate of 78% for test sentences.\n","versions":[{"version":"v1","created":"Mon, 13 Dec 1999 04:46:20 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Murata","M.","","CRL"],["Isahara","H.","","CRL"],["Nagao","M.","","Kyoto University"]]}
{"id":"cs/9912005","submitter":"Masaki Murata","authors":"M. Murata (CRL), M. Nagao (Kyoto University)","title":"An Estimate of Referent of Noun Phrases in Japanese Sentences","comments":"5 pages, 0 figures. Computation and Language","journal-ref":"Coling-ACL '98, Montrial, Canada, August 10, 1998 p912-916","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  In machine translation and man-machine dialogue, it is important to clarify\nreferents of noun phrases. We present a method for determining the referents of\nnoun phrases in Japanese sentences by using the referential properties,\nmodifiers, and possessors of noun phrases. Since the Japanese language has no\narticles, it is difficult to decide whether a noun phrase has an antecedent or\nnot. We had previously estimated the referential properties of noun phrases\nthat correspond to articles by using clue words in the sentences. By using\nthese referential properties, our system determined the referents of noun\nphrases in Japanese sentences. Furthermore we used the modifiers and possessors\nof noun phrases in determining the referents of noun phrases. As a result, on\ntraining sentences we obtained a precision rate of 82% and a recall rate of 85%\nin the determination of the referents of noun phrases that have antecedents. On\ntest sentences, we obtained a precision rate of 79% and a recall rate of 77%.\n","versions":[{"version":"v1","created":"Mon, 13 Dec 1999 05:20:40 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Murata","M.","","CRL"],["Nagao","M.","","Kyoto University"]]}
{"id":"cs/9912006","submitter":"Masaki Murata","authors":"M. Murata, M. Nagao (Kyoto University)","title":"Resolution of Verb Ellipsis in Japanese Sentence using Surface\n  Expressions and Examples","comments":"6 pages, 0 figures. Computation and Language","journal-ref":"Natural Language Processing Pacific Rim Symposium 1997 (NLPRS'97),\n  Cape Panwa Hotel, Phuket, Thailand, December 2-4, 1997 p75-80","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  Verbs are sometimes omitted in Japanese sentences. It is necessary to recover\nomitted verbs for purposes of language understanding, machine translation, and\nconversational processing. This paper describes a practical way to recover\nomitted verbs by using surface expressions and examples. We experimented the\nresolution of verb ellipses by using this information, and obtained a recall\nrate of 73% and a precision rate of 66% on test sentences.\n","versions":[{"version":"v1","created":"Mon, 13 Dec 1999 05:19:46 GMT"}],"update_date":"2009-09-25","authors_parsed":[["Murata","M.","","Kyoto University"],["Nagao","M.","","Kyoto University"]]}
{"id":"cs/9912007","submitter":"Masaki Murata","authors":"M. Murata, Q. Ma, K. Uchimoto, H. Isahara (CRL)","title":"An Example-Based Approach to Japanese-to-English Translation of Tense,\n  Aspect, and Modality","comments":"11 pages, 0 figures. Computation and Language","journal-ref":"TMI'99, Chester, UK, August 23, 1999","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  We have developed a new method for Japanese-to-English translation of tense,\naspect, and modality that uses an example-based method. In this method the\nsimilarity between input and example sentences is defined as the degree of\nsemantic matching between the expressions at the ends of the sentences. Our\nmethod also uses the k-nearest neighbor method in order to exclude the effects\nof noise; for example, wrongly tagged data in the bilingual corpora.\nExperiments show that our method can translate tenses, aspects, and modalities\nmore accurately than the top-level MT software currently available on the\nmarket can. Moreover, it does not require hand-craft rules.\n","versions":[{"version":"v1","created":"Mon, 13 Dec 1999 06:01:19 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Murata","M.","","CRL"],["Ma","Q.","","CRL"],["Uchimoto","K.","","CRL"],["Isahara","H.","","CRL"]]}
{"id":"cs/9912009","submitter":"Schwitter","authors":"Michael Hess","title":"Deduction over Mixed-Level Logic Representations for Text Passage\n  Retrieval","comments":"8 pages, Proceedings of the Eighth International Conference on Tools\n  with Artificial Intelligence (TAI'96), Los Alamitos CA","journal-ref":"IEEE Computer Society Press, 1996. 383-390","doi":"10.1109/TAI.1996.560480","report-no":null,"categories":"cs.CL","license":null,"abstract":"  A system is described that uses a mixed-level representation of (part of)\nmeaning of natural language documents (based on standard Horn Clause Logic) and\na variable-depth search strategy that distinguishes between the different\nlevels of abstraction in the knowledge representation to locate specific\npassages in the documents. Mixed-level representations as well as\nvariable-depth search strategies are applicable in fields outside that of NLP.\n","versions":[{"version":"v1","created":"Wed, 15 Dec 1999 11:02:22 GMT"}],"update_date":"2016-11-15","authors_parsed":[["Hess","Michael",""]]}
{"id":"cs/9912017","submitter":"Schwitter","authors":"Michael Hess","title":"Mixed-Level Knowledge Representation and Variable-Depth Inference in\n  Natural Language Processing","comments":"29 pages","journal-ref":"International Journal on Artificial Intelligence Tools (IJAIT),\n  vol 6, no 4, 1997. 481-509","doi":null,"report-no":null,"categories":"cs.CL","license":null,"abstract":"  A system is described that uses a mixed-level knowledge representation based\non standard Horn Clause Logic to represent (part of) the meaning of natural\nlanguage documents. A variable-depth search strategy is outlined that\ndistinguishes between the different levels of abstraction in the knowledge\nrepresentation to locate specific passages in the documents. A detailed\ndescription of the linguistic aspects of the system is given. Mixed-level\nrepresentations as well as variable-depth search strategies are applicable in\nfields outside that of NLP.\n","versions":[{"version":"v1","created":"Thu, 23 Dec 1999 15:48:26 GMT"}],"update_date":"2007-05-23","authors_parsed":[["Hess","Michael",""]]}
