{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies\n",
    "Uncomment if running the notebook for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List\n",
    "import json\n",
    "import random\n",
    "import gdown\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.base import Chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_ai_api_key = \"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\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = open_ai_api_key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go over the [LangChain](https://langchain.readthedocs.io/en/latest/index.html) documents and figure out how to set temperature for your requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_temp = 0.7\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", \n",
    "             temperature=default_temp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4.) Implement problem 3 using LangChain's LLM and the PromptTemplate classes and check if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_of_origin = \"India\"\n",
    "template_1 = \"Question: Come up with 10 girl baby names for babies born in {country_of_origin} which start and end with letter 'A.'\\nAnswer: \"\n",
    "prompt_1 = PromptTemplate(template=template_1, \n",
    "                          input_variables=[\"country_of_origin\"])\n",
    "formatted_prompt_1 = prompt_1.format(country_of_origin=country_of_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Come up with 10 girl baby names for babies born in India which start and end with letter 'A.'\n",
      "Answer: \n",
      "1. Aarna \n",
      "2. Aarushi \n",
      "3. Aadhya \n",
      "4. Aashna \n",
      "5. Aayat \n",
      "6. Aanya \n",
      "7. Aarika \n",
      "8. Aayushi \n",
      "9. Aamani \n",
      "10. Aashi\n"
     ]
    }
   ],
   "source": [
    "llm_answer_1 = llm(formatted_prompt_1)\n",
    "print(f\"{formatted_prompt_1}{llm_answer_1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.) Create a new prompt that takes a baby name and the country of origin, and comes up with a short (made up) biography. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "baby_name = \"Kunal\"\n",
    "country_of_origin = \"India\"\n",
    "template_2 = \"Question: Come up with a short (made up) biography for a baby named {baby_name}, born in {country_of_origin}.\\nAnswer: \"\n",
    "prompt_2 = PromptTemplate(template=template_2, \n",
    "                         input_variables=[\"country_of_origin\", \"baby_name\"])\n",
    "formatted_prompt_2 = prompt_2.format(baby_name=baby_name, \n",
    "                                     country_of_origin=country_of_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Come up with a short (made up) biography for a baby named Kunal, born in India.\n",
      "Answer: \n",
      "Kunal was born in India to loving parents. As a baby, he was full of energy and curiosity, and often spent his days exploring the world around him. He had a natural talent for music, and loved to play the drums and the harmonium. He enjoyed learning and was an avid reader, always eager to find out more about the world around him. As he grew older, Kunal made his mark in the world, becoming a respected leader in his community. He was a devoted family man, and dedicated himself to helping others. Kunal made an impact on the world, and his memory will never be forgotten.\n"
     ]
    }
   ],
   "source": [
    "llm_answer_2 = llm(formatted_prompt_2)\n",
    "print(f\"{formatted_prompt_2}{llm_answer_2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.) Follow the [LangChain example to create a custom chain class](https://langchain.readthedocs.io/en/latest/modules/chains/getting_started.html#create-a-custom-chain-with-the-chain-class), to create class that returns a list of dicts of {baby_name, biography}. You have to use the prompts you created in 4 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Chain Output:\n",
      "[{'baby_name': 'Alaya', 'biography': 'Alaya was born in India to loving parents who have always nurtured her with a deep appreciation and respect for her heritage. As a child, Alaya was a quick learner, soaking up the culture, music, and language of her homeland. She loved spending time outdoors, exploring the countryside and playing in the sunshine. As she grew, Alaya developed a passion for knowledge, which has served her well in her academic studies. She is a bright and inquisitive young girl, always eager to learn more about the world around her. Alaya is a kind and compassionate soul, with a big heart and a determination to make the world a better place.'}, {'baby_name': 'Aanya', 'biography': 'Aanya was born in India to loving parents. She is the youngest of two siblings, and the family loves to take trips to the beach together. As a baby, Aanya loves to explore and is constantly learning new things. She loves to cuddle up with her parents and listen to stories. Aanya loves to laugh and smile, and her contagious giggle can light up any room. She is loved by everyone who meets her and is always eager to learn and discover new things.'}, {'baby_name': 'Aalia', 'biography': 'Aalia was born in India in 2020. She is a curious and adventurous baby, always eager to explore the world around her. She loves to listen to music, dance, and play with her toys. Aalia loves spending time with her family and friends, and is always up for an adventure. She loves to laugh and smile, and is always full of energy. Aalia is a bright and happy baby who loves to learn new things.'}, {'baby_name': 'Arya', 'biography': 'Arya was born in India to a loving family. She is the youngest of four children and loves spending time with her siblings. She is an inquisitive baby, always asking questions and exploring her surroundings. She loves playing with her toys and singing her favorite nursery rhymes. Arya has a bright future ahead of her and her family is looking forward to watching her grow.'}, {'baby_name': 'Aarvi', 'biography': 'Aarvi was born in India to a loving family. She loves to smile and giggle and loves to spend time with her family. She loves to learn new things and is always eager to explore the world around her. Aarvi loves music, dancing, and playing in the park. She loves spending time outdoors, looking at the stars, and watching the birds fly by. Aarvi loves to travel, and she dreams of seeing the world one day. She is a bright and inquisitive baby who loves to share her enthusiasm with everyone she meets.'}, {'baby_name': 'Aaradhya', 'biography': 'Aaradhya was born in India to loving parents who were excited to welcome her into the world. She has been the light of their lives ever since. She loves to explore her surroundings and is always eager to learn new things. She loves playing with her toys and loves spending time with her family. Aaradhya is a curious, kind, and outgoing little one who loves to make people smile.'}, {'baby_name': 'Avni', 'biography': 'Avni was born in India to a loving family. She is full of life and loves to explore her surroundings. She loves to play with her siblings, and loves to be around her family. She loves music and loves to dance and sing. Avni is a bright and cheerful baby who is always ready to learn new things. She is loved by all and brings joy to everyone around her.'}, {'baby_name': 'Aamani', 'biography': \"Aamani was born in India to proud parents and has already been brought on many adventures by her family. At just four months old, she has experienced the warmth of the Indian sun, the smell of masala spices, and the sights and sounds of the bustling city. She loves to spend her days exploring her surroundings, learning new things, and playing with her toys. Aamani has already made a big impression on her family and friends, and they can't wait to watch her continue to grow and thrive in the world.\"}, {'baby_name': 'Anaya', 'biography': 'Anaya is a happy and adventurous baby born in India. She loves to explore her world and is always eager to learn more. Anaya loves to sing, dance, and laugh with her family and friends. She loves to try new foods and has a passion for discovering the beauty of her country. Anaya is full of life and loves to share her joy with everyone around her.'}, {'baby_name': 'Aarohi', 'biography': 'Aarohi was born into a large family in India. Her parents have always been supportive and encouraging of her dreams, and she has always had a passion for exploring new places and learning about different cultures. She loves to play with her siblings and loves to spend time outdoors. Even at a young age, Aarohi has already shown a keen interest in art, music, and reading. She is curious and inquisitive, always eager to learn more about the world around her. Her family is excited to see where life will take Aarohi in the future.'}]\n"
     ]
    }
   ],
   "source": [
    "class CustomChain(Chain):\n",
    "    # Baby name generator chain\n",
    "    chain_1: LLMChain\n",
    "    # Biography generator chain\n",
    "    chain_2: LLMChain\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        return list(self.chain_1.input_keys)\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return ['output']\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str]):\n",
    "        results = list()\n",
    "        # Get a list of baby names from the first chain\n",
    "        output_1 = self.chain_1.run(inputs)\n",
    "        output_1 = output_1.strip().splitlines()\n",
    "        # For each baby name, get a biography from the second chain\n",
    "        for i in output_1:\n",
    "            temp = dict()\n",
    "            baby_name = i.split(' ')[1]\n",
    "            inputs['baby_name'] = baby_name\n",
    "            output_2 = self.chain_2.run(inputs).strip()\n",
    "            temp['baby_name'] = baby_name\n",
    "            temp['biography'] = output_2\n",
    "            results.append(temp)\n",
    "        return {'output': results}\n",
    "    \n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
    "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
    "\n",
    "custom_chain = CustomChain(chain_1=chain_1, chain_2=chain_2)\n",
    "custom_chain_output = custom_chain.run(country_of_origin=country_of_origin)\n",
    "print(f\"Custom Chain Output:\\n{custom_chain_output}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArXiv Bulletin\n",
    "Every day several hundred, if not more, papers appear on ArXiv. For this task you will implement an ArXiv bulletin that gives a list of paper titles and why I should read it in one sentence. Your final output should look something like this (the exact papers will be different):\n",
    "\n",
    "1. Summarizing Encyclopedic Term Descriptions on the Web <br/>\n",
    "Why to read: This paper presents a summarization method to produce a single text from multiple descriptions to concisely describe a term from different viewpoints. <br/>\n",
    "2. Unsupervised Topic Adaptation for Lecture Speech Retrieval <br/>\n",
    "Why to read: This paper presents a novel approach to improve the quality of a cross-media information retrieval system by adapting acoustic and language models for automatic speech recognition. <br/>\n",
    "\n",
    "Download a sample of the NLP ArXiv dataset from here. It has metadata for 100 NLP papers as JSON records. For this exercise, you will randomly pick 10 records to show a proof of concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1_1oPRNSW7QWdlUs-APMV5Y7h6RxU_8gF\n",
      "To: /Users/kushagraseth/Documents/Repositories/nlp-244-kushagra/quest-2/cs.cl.sample100.json\n",
      "100%|██████████| 135k/135k [00:00<00:00, 2.03MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Paper List:\n",
      "[('Pronoun Resolution in Japanese Sentences Using Surface Expressions and\\n  Examples', \"In this paper, we present a method of estimating referents of demonstrative\\npronouns, personal pronouns, and zero pronouns in Japanese sentences using\\nexamples, surface expressions, topics and foci. Unlike conventional work which\\nwas semantic markers for semantic constraints, we used examples for semantic\\nconstraints and showed in our experiments that examples are as useful as\\nsemantic markers. We also propose many new methods for estimating referents of\\npronouns. For example, we use the form ``X of Y'' for estimating referents of\\ndemonstrative adjectives. In addition to our new methods, we used many\\nconventional methods. As a result, experiments using these methods obtained a\\nprecision rate of 87% in estimating referents of demonstrative pronouns,\\npersonal pronouns, and zero pronouns for training sentences, and obtained a\\nprecision rate of 78% for test sentences.\"), ('A Formal Framework for Linguistic Annotation', \"`Linguistic annotation' covers any descriptive or analytic notations applied\\nto raw language data. The basic data may be in the form of time functions --\\naudio, video and/or physiological recordings -- or it may be textual. The added\\nnotations may include transcriptions of all sorts (from phonetic features to\\ndiscourse structures), part-of-speech and sense tagging, syntactic analysis,\\n`named entity' identification, co-reference annotation, and so on. While there\\nare several ongoing efforts to provide formats and tools for such annotations\\nand to publish annotated linguistic databases, the lack of widely accepted\\nstandards is becoming a critical problem. Proposed standards, to the extent\\nthey exist, have focussed on file formats. This paper focuses instead on the\\nlogical structure of linguistic annotations. We survey a wide variety of\\nexisting annotation formats and demonstrate a common conceptual core, the\\nannotation graph. This provides a formal framework for constructing,\\nmaintaining and searching linguistic annotations, while remaining consistent\\nwith many alternative data structures and file formats.\"), ('A Bootstrap Approach to Automatically Generating Lexical Transfer Rules', 'We describe a method for automatically generating Lexical Transfer Rules\\n(LTRs) from word equivalences using transfer rule templates. Templates are\\nskeletal LTRs, unspecified for words. New LTRs are created by instantiating a\\ntemplate with words, provided that the words belong to the appropriate lexical\\ncategories required by the template. We define two methods for creating an\\ninventory of templates and using them to generate new LTRs. A simpler method\\nconsists of extracting a finite set of templates from a sample of hand coded\\nLTRs and directly using them in the generation process. A further method\\nconsists of abstracting over the initial finite set of templates to define\\nhigher level templates, where bilingual equivalences are defined in terms of\\ncorrespondences involving phrasal categories. Phrasal templates are then mapped\\nonto sets of lexical templates with the aid of grammars. In this way an\\ninfinite set of lexical templates is recursively defined. New LTRs are created\\nby parsing input words, matching a template at the phrasal level and using the\\ncorresponding lexical categories to instantiate the lexical template. The\\ndefinition of an infinite set of templates enables the automatic creation of\\nLTRs for multi-word, non-compositional word equivalences of any cardinality.'), ('A Bootstrap Approach to Automatically Generating Lexical Transfer Rules', 'We describe a method for automatically generating Lexical Transfer Rules\\n(LTRs) from word equivalences using transfer rule templates. Templates are\\nskeletal LTRs, unspecified for words. New LTRs are created by instantiating a\\ntemplate with words, provided that the words belong to the appropriate lexical\\ncategories required by the template. We define two methods for creating an\\ninventory of templates and using them to generate new LTRs. A simpler method\\nconsists of extracting a finite set of templates from a sample of hand coded\\nLTRs and directly using them in the generation process. A further method\\nconsists of abstracting over the initial finite set of templates to define\\nhigher level templates, where bilingual equivalences are defined in terms of\\ncorrespondences involving phrasal categories. Phrasal templates are then mapped\\nonto sets of lexical templates with the aid of grammars. In this way an\\ninfinite set of lexical templates is recursively defined. New LTRs are created\\nby parsing input words, matching a template at the phrasal level and using the\\ncorresponding lexical categories to instantiate the lexical template. The\\ndefinition of an infinite set of templates enables the automatic creation of\\nLTRs for multi-word, non-compositional word equivalences of any cardinality.'), ('The JRC-Acquis: A multilingual aligned parallel corpus with 20+\\n  languages', 'We present a new, unique and freely available parallel corpus containing\\nEuropean Union (EU) documents of mostly legal nature. It is available in all 20\\nofficial EUanguages, with additional documents being available in the languages\\nof the EU candidate countries. The corpus consists of almost 8,000 documents\\nper language, with an average size of nearly 9 million words per language.\\nPair-wise paragraph alignment information produced by two different aligners\\n(Vanilla and HunAlign) is available for all 190+ language pair combinations.\\nMost texts have been manually classified according to the EUROVOC subject\\ndomains so that the collection can also be used to train and test multi-label\\nclassification algorithms and keyword-assignment software. The corpus is\\nencoded in XML, according to the Text Encoding Initiative Guidelines. Due to\\nthe large number of parallel texts in many languages, the JRC-Acquis is\\nparticularly suitable to carry out all types of cross-language research, as\\nwell as to test and benchmark text analysis software across different languages\\n(for instance for alignment, sentence splitting and term extraction).'), ('A Probabilistic Approach to Lexical Semantic Knowledge Acquisition and S\\n  tructural Disambiguation', 'In this thesis, I address the problem of automatically acquiring lexical\\nsemantic knowledge, especially that of case frame patterns, from large corpus\\ndata and using the acquired knowledge in structural disambiguation. The\\napproach I adopt has the following characteristics: (1) dividing the problem\\ninto three subproblems: case slot generalization, case dependency learning, and\\nword clustering (thesaurus construction). (2) viewing each subproblem as that\\nof statistical estimation and defining probability models for each subproblem,\\n(3) adopting the Minimum Description Length (MDL) principle as learning\\nstrategy, (4) employing efficient learning algorithms, and (5) viewing the\\ndisambiguation problem as that of statistical prediction. Major contributions\\nof this thesis include: (1) formalization of the lexical knowledge acquisition\\nproblem, (2) development of a number of learning methods for lexical knowledge\\nacquisition, and (3) development of a high-performance disambiguation method.'), ('Summarizing Encyclopedic Term Descriptions on the Web', 'We are developing an automatic method to compile an encyclopedic corpus from\\nthe Web. In our previous work, paragraph-style descriptions for a term are\\nextracted from Web pages and organized based on domains. However, these\\ndescriptions are independent and do not comprise a condensed text as in\\nhand-crafted encyclopedias. To resolve this problem, we propose a summarization\\nmethod, which produces a single text from multiple descriptions. The resultant\\nsummary concisely describes a term from different viewpoints. We also show the\\neffectiveness of our method by means of experiments.'), ('Compacting the Penn Treebank Grammar', 'Treebanks, such as the Penn Treebank (PTB), offer a simple approach to\\nobtaining a broad coverage grammar: one can simply read the grammar off the\\nparse trees in the treebank. While such a grammar is easy to obtain, a\\nsquare-root rate of growth of the rule set with corpus size suggests that the\\nderived grammar is far from complete and that much more treebanked text would\\nbe required to obtain a complete grammar, if one exists at some limit. However,\\nwe offer an alternative explanation in terms of the underspecification of\\nstructures within the treebank. This hypothesis is explored by applying an\\nalgorithm to compact the derived grammar by eliminating redundant rules --\\nrules whose right hand sides can be parsed by other rules. The size of the\\nresulting compacted grammar, which is significantly less than that of the full\\ntreebank grammar, is shown to approach a limit. However, such a compacted\\ngrammar does not yield very good performance figures. A version of the\\ncompaction algorithm taking rule probabilities into account is proposed, which\\nis argued to be more linguistically motivated. Combined with simple\\nthresholding, this method can be used to give a 58% reduction in grammar size\\nwithout significant change in parsing performance, and can produce a 69%\\nreduction with some gain in recall, but a loss in precision.'), ('Random Sentences from a Generalized Phrase-Structure Grammar Interpreter', 'In numerous domains in cognitive science it is often useful to have a source\\nfor randomly generated corpora. These corpora may serve as a foundation for\\nartificial stimuli in a learning experiment (e.g., Ellefson & Christiansen,\\n2000), or as input into computational models (e.g., Christiansen & Dale, 2001).\\nThe following compact and general C program interprets a phrase-structure\\ngrammar specified in a text file. It follows parameters set at a Unix or\\nUnix-based command-line and generates a corpus of random sentences from that\\ngrammar.'), ('An Empirical Approach to Temporal Reference Resolution (journal version)', 'Scheduling dialogs, during which people negotiate the times of appointments,\\nare common in everyday life. This paper reports the results of an in-depth\\nempirical investigation of resolving explicit temporal references in scheduling\\ndialogs. There are four phases of this work: data annotation and evaluation,\\nmodel development, system implementation and evaluation, and model evaluation\\nand analysis. The system and model were developed primarily on one set of data,\\nand then applied later to a much more complex data set, to assess the\\ngeneralizability of the model for the task being performed. Many different\\ntypes of empirical methods are applied to pinpoint the strengths and weaknesses\\nof the approach. Detailed annotation instructions were developed and an\\nintercoder reliability study was performed, showing that naive annotators can\\nreliably perform the targeted annotations. A fully automatic system has been\\ndeveloped and evaluated on unseen test data, with good results on both data\\nsets. We adopt a pure realization of a recency-based focus model to identify\\nprecisely when it is and is not adequate for the task being addressed. In\\naddition to system results, an in-depth evaluation of the model itself is\\npresented, based on detailed manual annotations. The results are that few\\nerrors occur specifically due to the model of focus being used, and the set of\\nanaphoric relations defined in the model are low in ambiguity for both data\\nsets.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gdown.download('https://drive.google.com/uc?id=1_1oPRNSW7QWdlUs-APMV5Y7h6RxU_8gF')\n",
    "with open('cs.cl.sample100.json') as f:\n",
    "    data = f.readlines()\n",
    "parsed = [json.loads(x) for x in data]\n",
    "sample10 = random.choices(parsed, k=10)\n",
    "paper_list = [(i['title'].strip(), i['abstract'].strip()) for i in sample10]\n",
    "print(f\"Research Paper List:\\n{paper_list}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will have to use LangChain for this problem and come up with a suitable zero-shot prompt for \"why\". Be creative with your prompts and use the prompting best practices as your guide. You can experiment with a few prompt variations on for a single paper in Playground before you use it in your code. Make sure your code runs at T=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_temp = 0\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", \n",
    "             temperature=new_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pronoun Resolution in Japanese Sentences Using Surface Expressions and\n",
      "  Examples.\n",
      "Why to read: \n",
      "This paper presents a method of estimating referents of demonstrative pronouns, personal pronouns, and zero pronouns in Japanese sentences using examples, surface expressions, topics and foci, and achieves a precision rate of 87% for training sentences and 78% for test sentences.\n",
      "\n",
      "A Formal Framework for Linguistic Annotation.\n",
      "Why to read: \n",
      "This paper provides a formal framework for constructing, maintaining, and searching linguistic annotations, which is essential for NLP students to understand.\n",
      "\n",
      "A Bootstrap Approach to Automatically Generating Lexical Transfer Rules.\n",
      "Why to read: \n",
      "This paper presents a bootstrap approach to automatically generating lexical transfer rules from word equivalences, which can be used to create LTRs for multi-word, non-compositional word equivalences of any cardinality.\n",
      "\n",
      "A Bootstrap Approach to Automatically Generating Lexical Transfer Rules.\n",
      "Why to read: \n",
      "This paper presents a bootstrap approach to automatically generating lexical transfer rules from word equivalences, which can be used to create LTRs for multi-word, non-compositional word equivalences of any cardinality.\n",
      "\n",
      "The JRC-Acquis: A multilingual aligned parallel corpus with 20+\n",
      "  languages.\n",
      "Why to read: \n",
      "This paper presents the JRC-Acquis, a multilingual aligned parallel corpus with 20+ languages, which is suitable for cross-language research and testing and benchmarking of NLP software.\n",
      "\n",
      "A Probabilistic Approach to Lexical Semantic Knowledge Acquisition and S\n",
      "  tructural Disambiguation.\n",
      "Why to read: \n",
      "This paper presents a probabilistic approach to lexical semantic knowledge acquisition and structural disambiguation, which provides a formalization of the lexical knowledge acquisition problem and efficient learning algorithms for NLP students.\n",
      "\n",
      "Summarizing Encyclopedic Term Descriptions on the Web.\n",
      "Why to read: \n",
      "This paper presents an automatic method for summarizing encyclopedic term descriptions from the web, which can be used to create concise descriptions of terms from different viewpoints.\n",
      "\n",
      "Compacting the Penn Treebank Grammar.\n",
      "Why to read: \n",
      "This paper presents an algorithm for compressing the Penn Treebank grammar, which can reduce the size of the grammar while maintaining good parsing performance.\n",
      "\n",
      "Random Sentences from a Generalized Phrase-Structure Grammar Interpreter.\n",
      "Why to read: \n",
      "This paper provides a useful tool for NLP students to generate random sentences from a phrase-structure grammar, which can be used as artificial stimuli in learning experiments or as input into computational models.\n",
      "\n",
      "An Empirical Approach to Temporal Reference Resolution (journal version).\n",
      "Why to read: \n",
      "This paper provides an empirical approach to temporal reference resolution, with detailed annotation instructions, an intercoder reliability study, a fully automatic system, and an in-depth evaluation of the model, showing that few errors occur due to the model of focus being used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in paper_list:\n",
    "    paper_title = i[0]\n",
    "    abstract = i[1]\n",
    "    template =  \"\"\"Research Paper Title: {paper_title}.\\nAbstract: {abstract}.\\nSummarize in one sentence why should \n",
    "    I read this paper as an Natural Language Processing (NLP) student given Research Paper's Title and Abstract: \n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=template, \n",
    "                            input_variables=[\"paper_title\", \"abstract\"])\n",
    "    formatted_prompt = prompt.format(paper_title=paper_title, \n",
    "                                     abstract=abstract)\n",
    "    llm_answer = llm(formatted_prompt)\n",
    "    print(f\"{paper_title}.\\nWhy to read: {llm_answer}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2c87e16e3c5224f19fdcd75f65d952c55b5bf326ebfd42c1280c69b64e015be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
