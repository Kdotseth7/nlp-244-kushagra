{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch warmup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use torch.randn to create two tensors of size (29, 30, 32) and (32, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of Size (29, 30, 32): \n",
      " tensor([[[-1.3273e-01, -1.7137e-03,  2.3768e-01,  ...,  8.7736e-01,\n",
      "           9.9803e-01,  1.5487e+00],\n",
      "         [-7.8726e-01, -3.3921e-01,  8.3773e-01,  ..., -4.8650e-01,\n",
      "           1.6367e+00,  8.9340e-01],\n",
      "         [-1.5818e-02, -4.3789e-01,  1.3492e+00,  ...,  4.9213e-01,\n",
      "          -2.3609e-01, -5.7800e-01],\n",
      "         ...,\n",
      "         [ 1.6923e+00, -6.1207e-02,  1.3573e+00,  ...,  1.8294e+00,\n",
      "          -6.2201e-01, -2.5485e+00],\n",
      "         [-1.4159e+00,  2.0194e+00, -8.1503e-01,  ..., -9.1955e-01,\n",
      "          -3.3892e-01, -1.2415e+00],\n",
      "         [-1.0451e+00, -3.0325e-01, -1.4625e+00,  ...,  7.9478e-01,\n",
      "          -7.4056e-01,  5.7179e-01]],\n",
      "\n",
      "        [[ 7.6344e-02,  1.2227e+00, -2.9117e-01,  ...,  1.7126e+00,\n",
      "           6.5116e-01,  3.5459e-02],\n",
      "         [ 1.3650e+00, -8.8486e-01, -8.0388e-01,  ..., -3.9080e-01,\n",
      "           7.6656e-01, -1.0701e+00],\n",
      "         [ 2.8774e-01, -1.9205e+00,  5.7871e-02,  ...,  3.4832e-01,\n",
      "          -1.5781e+00,  4.5148e-01],\n",
      "         ...,\n",
      "         [-1.3874e+00,  1.0594e+00, -5.2626e-01,  ..., -2.0775e-01,\n",
      "          -2.2596e+00,  3.2965e-01],\n",
      "         [-3.9620e-01,  1.5521e+00, -5.2927e-01,  ..., -1.2159e+00,\n",
      "          -1.9228e-01,  4.0041e-01],\n",
      "         [ 1.1104e+00, -7.4992e-01, -2.7426e-01,  ..., -1.4076e-01,\n",
      "           6.9162e-01,  6.6093e-01]],\n",
      "\n",
      "        [[ 6.1095e-02,  1.5706e-01, -1.9473e+00,  ...,  2.3483e+00,\n",
      "           3.0090e+00,  1.1209e+00],\n",
      "         [-3.8505e-01, -7.6997e-01,  1.1043e+00,  ...,  1.9602e+00,\n",
      "           3.1282e-02,  5.7774e-01],\n",
      "         [ 4.6230e-01, -4.2863e-01, -1.8375e-01,  ..., -8.3899e-01,\n",
      "          -2.0330e+00, -4.6658e-01],\n",
      "         ...,\n",
      "         [-2.1382e-02, -7.5291e-01, -5.8962e-01,  ...,  3.7478e-01,\n",
      "          -3.1971e-01, -1.2235e+00],\n",
      "         [-4.9352e-02, -7.5865e-01,  3.3544e-01,  ...,  5.3816e-01,\n",
      "           2.3352e-01, -9.3514e-01],\n",
      "         [-1.4593e+00,  5.9149e-02,  7.0865e-01,  ..., -6.1734e-01,\n",
      "           1.2999e+00, -1.3577e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.0830e+00, -1.5776e+00,  2.2089e-01,  ...,  9.2912e-01,\n",
      "          -1.7497e+00, -1.4495e+00],\n",
      "         [-8.4367e-01, -9.3775e-01, -1.4888e-01,  ...,  1.5425e+00,\n",
      "          -2.6088e-01,  1.4705e+00],\n",
      "         [ 9.9388e-02,  9.7373e-01,  8.5209e-02,  ..., -2.0699e-01,\n",
      "           1.2625e+00,  5.7699e-01],\n",
      "         ...,\n",
      "         [-6.3681e-01, -1.4689e-01,  4.9734e-01,  ..., -5.2942e-02,\n",
      "           1.5211e-01,  1.6502e-01],\n",
      "         [-2.7766e-01,  4.6258e-01,  2.3323e+00,  ..., -5.1916e-02,\n",
      "           1.4913e-01,  2.5510e-01],\n",
      "         [-1.2026e+00, -1.9550e+00,  6.3625e-01,  ...,  8.9078e-01,\n",
      "           4.5858e-01, -3.1271e-01]],\n",
      "\n",
      "        [[-9.1677e-02, -8.9111e-01,  2.8384e-01,  ...,  1.5409e+00,\n",
      "          -1.8492e+00,  2.9940e-01],\n",
      "         [-7.1660e-01, -1.5816e+00,  8.5425e-02,  ...,  3.3389e-01,\n",
      "          -6.2897e-01,  2.5001e+00],\n",
      "         [-7.9341e-01, -6.2534e-01, -5.7705e-01,  ...,  4.5032e-02,\n",
      "          -1.6811e+00,  4.0370e-01],\n",
      "         ...,\n",
      "         [ 3.6730e-01,  1.1801e-01,  7.1859e-01,  ..., -2.0035e+00,\n",
      "          -3.6481e-02,  9.6013e-01],\n",
      "         [-3.4413e-01, -6.4038e-01,  1.4396e+00,  ...,  2.0289e+00,\n",
      "          -1.0978e+00, -4.3653e-01],\n",
      "         [ 1.2558e+00,  3.2495e-01, -7.7422e-01,  ...,  2.5026e+00,\n",
      "           1.3804e+00, -1.1542e+00]],\n",
      "\n",
      "        [[-7.1197e-01,  2.0211e+00,  8.2996e-01,  ...,  9.4993e-01,\n",
      "           6.8734e-01,  2.2579e+00],\n",
      "         [-4.3787e-01, -9.2612e-01, -1.6493e+00,  ..., -1.3396e+00,\n",
      "          -4.2136e-03, -2.3872e-01],\n",
      "         [ 9.9253e-02,  3.8025e-01,  9.5152e-01,  ...,  1.6525e+00,\n",
      "          -5.5551e-02,  1.0402e+00],\n",
      "         ...,\n",
      "         [ 2.5663e+00, -1.1641e+00, -2.2348e-01,  ..., -1.2941e+00,\n",
      "          -1.9380e+00,  1.5683e+00],\n",
      "         [ 1.8456e-02,  3.5975e-01, -7.0318e-01,  ..., -1.8772e-02,\n",
      "           1.2831e-01,  5.1688e-01],\n",
      "         [ 2.2779e-01,  1.3592e+00, -3.7149e-01,  ..., -5.5919e-01,\n",
      "           1.3500e+00, -8.5469e-01]]])\n",
      "Tensor of Size (32, 100): \n",
      " tensor([[ 0.2034, -1.4178,  0.2433,  ...,  0.1164,  0.3821, -0.3109],\n",
      "        [-0.3728,  0.7231, -0.6256,  ..., -1.0592,  1.0302,  0.4681],\n",
      "        [-0.2084, -2.3785,  1.5925,  ..., -0.9359, -0.5397, -0.2399],\n",
      "        ...,\n",
      "        [-1.6442,  1.2666,  0.2117,  ..., -0.3623, -0.6678, -0.4557],\n",
      "        [-0.9042,  0.2447, -0.5150,  ...,  0.3688, -1.6498, -0.9435],\n",
      "        [ 1.2494, -0.8541, -0.5172,  ..., -0.9497,  0.2238,  0.9364]])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.randn(29, 30, 32)\n",
    "print('Tensor of Size (29, 30, 32): \\n' , tensor_a)\n",
    "\n",
    "tensor_b = torch.randn(32, 100)\n",
    "print('Tensor of Size (32, 100): \\n' , tensor_b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use  torch.matmul  to matrix multiply the two tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product of tensor_a and tensor_b: \n",
      " tensor([[[ -0.3658,   4.0343,  -8.5939,  ...,  -1.0294,   0.6143,  11.5574],\n",
      "         [  1.8736,   2.3378,  -5.2487,  ...,  -5.5383,  -5.2335,  -0.3518],\n",
      "         [  4.6152,   5.2167,   6.0129,  ...,   9.1791,  -3.6223,   4.9273],\n",
      "         ...,\n",
      "         [-10.0908,   2.3655,   0.9489,  ...,   5.1504,  -6.2577,  -1.6819],\n",
      "         [ -2.1101,  -0.1993,  -2.9200,  ...,   5.3591,   4.2625,   3.9612],\n",
      "         [  0.3956,  14.5996,  -3.5194,  ...,   4.4043,   3.7146,  -3.2771]],\n",
      "\n",
      "        [[-10.0672,  -0.5467,  -0.0804,  ...,  -0.0164,   3.8181,  -1.3932],\n",
      "         [  5.8277,  -9.0610,  -3.6578,  ...,   7.0324,  -4.6470,   2.5972],\n",
      "         [  7.6433,   2.6269,  -4.5265,  ...,   5.9333,   1.9873,   1.4495],\n",
      "         ...,\n",
      "         [ -4.6386,   1.0373,  -2.2187,  ...,  -7.5864,   6.0699,   6.1841],\n",
      "         [ -5.8824,  -2.9219,  -0.2697,  ...,  -4.1298,   0.0664,   2.7122],\n",
      "         [ -5.8216,  -5.5138,  -8.5588,  ...,   7.4204,  -7.1499,   6.4899]],\n",
      "\n",
      "        [[ -3.8154,  11.2351,  -0.2199,  ...,   0.5313,  -6.6225,  -5.5016],\n",
      "         [ -1.0206,   1.8529,  -6.1232,  ...,   0.3614, -13.0106,   0.5022],\n",
      "         [ -0.7371,   4.3845,  -0.3101,  ...,  -4.9942,   4.9844,  -0.2857],\n",
      "         ...,\n",
      "         [ -1.2104,   2.2662,  -2.9051,  ...,  -0.1920,  -6.0320,   1.6292],\n",
      "         [ -6.3490,  -6.0993,  -0.6012,  ...,   6.1687,  -2.1058,  -0.7258],\n",
      "         [ -9.6136,  -1.6007,  -0.4758,  ...,   1.6273,  -7.7952,  -7.7196]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -3.4248,  -5.5380,   0.1238,  ...,  -1.0553,   4.8196,  -5.8301],\n",
      "         [ -0.8353,   4.7313,  -2.8922,  ...,  -9.2129,  -0.7720,   5.2152],\n",
      "         [ -3.7534,  -1.7656,  -1.7244,  ...,  -1.5781,  -1.0706,   3.4279],\n",
      "         ...,\n",
      "         [ -1.2730,  -6.9837,  -3.4509,  ...,  -7.0087,  -2.1995,   2.7813],\n",
      "         [  1.8265,  -3.0514,  -3.7135,  ...,  -1.6006,   0.4796,   3.1270],\n",
      "         [ -3.3742,  -0.3079,  -2.2527,  ...,  -1.6329, -10.1224,   2.0081]],\n",
      "\n",
      "        [[  1.0427,  -0.8485,   0.9419,  ...,   7.0411,  -0.8169,   0.9456],\n",
      "         [ -0.0811,   7.3028,  -3.2562,  ...,  -7.1901,   2.0148,  -1.5306],\n",
      "         [ 16.0999,   6.7834,   8.5587,  ...,  -1.8605,   8.2669,   2.5640],\n",
      "         ...,\n",
      "         [  2.7346,  -9.3741,   4.6307,  ...,   3.6333,   6.1202,   3.1335],\n",
      "         [ -0.2224,   3.3956,   8.9591,  ...,   0.2665,  -3.4031,  -2.7272],\n",
      "         [ -7.6570,  12.8674,  -3.7557,  ...,   3.7591,  -2.9073,  -1.5597]],\n",
      "\n",
      "        [[  1.6040,   4.3083,   7.3247,  ..., -12.3871,   1.1489,  -5.4480],\n",
      "         [ -3.5059,   2.3534,   3.8640,  ...,   8.3884,   1.5739,   6.9883],\n",
      "         [ -3.9859,   1.9283,   3.5753,  ...,  -6.2013,  -5.3853,  -7.9443],\n",
      "         ...,\n",
      "         [  0.7796, -10.2436,   0.8244,  ...,  -6.5111,   3.1270,   2.5468],\n",
      "         [  8.1520,   1.8420,  -2.9454,  ...,   3.0338,   1.3907,   5.1244],\n",
      "         [ -2.9256,   5.5007,  -1.1262,  ...,   7.2804,  -0.1185,   2.5875]]])\n",
      "Shape of Tensor:  torch.Size([29, 30, 100])\n"
     ]
    }
   ],
   "source": [
    "product = torch.matmul(tensor_a, tensor_b)\n",
    "print('Product of tensor_a and tensor_b: \\n' , product)\n",
    "\n",
    "print('Shape of Tensor: ' , product.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What is the difference between torch.matmul , torch.mm , torch.bmm , and torch.einsum , and the @ operator?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use torch.sum on the resulting tensor, passing the optional argument of dim=1 to sum across the 1st dimension. Before you run this, can you predict the size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Tensor across 1st Dimension: \n",
      " tensor([[ 30.6956, -30.5862,   9.6243,  ...,   9.5548,  21.0395,  46.9285],\n",
      "        [ -7.2162, -24.5919, -10.7819,  ...,  50.1187,  23.1065,  30.1112],\n",
      "        [ 13.0457,  53.7973, -44.9612,  ...,  32.0453, -23.2937,  48.4604],\n",
      "        ...,\n",
      "        [ 19.1404,  -5.7801, -81.3716,  ...,  11.1412, -47.7008,   1.6392],\n",
      "        [ 15.1754,  30.8802,   6.2086,  ...,  17.5357,  24.6202,  20.4633],\n",
      "        [ 53.6354,  -1.9531,   5.9686,  ...,   2.8311,  30.3693,  59.3527]])\n",
      "Shape of Tensor:  torch.Size([29, 100])\n"
     ]
    }
   ],
   "source": [
    "tensor_sum = torch.sum(product, dim = 1)\n",
    "print('Sum of Tensor across 1st Dimension: \\n' , tensor_sum)\n",
    "\n",
    "print('Shape of Tensor: ' , tensor_sum.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create a new long tensor of size  (3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Tensor: \n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Updated Long Tensor: \n",
      " tensor([[2, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 4, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 6, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "long_tensor = torch.ones((3, 10), dtype = torch.long)\n",
    "print('Long Tensor: \\n' , long_tensor)\n",
    "\n",
    "long_tensor[0, 0] = 2\n",
    "long_tensor[1, 2] = 4\n",
    "long_tensor[2, 4] = 6\n",
    "\n",
    "print('Updated Long Tensor: \\n' , long_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Use this new long tensor to index into the tensor from step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed Tensor: \n",
      " tensor([[[[ -3.8154,  11.2351,  -0.2199,  ...,   0.5313,  -6.6225,  -5.5016],\n",
      "          [ -1.0206,   1.8529,  -6.1232,  ...,   0.3614, -13.0106,   0.5022],\n",
      "          [ -0.7371,   4.3845,  -0.3101,  ...,  -4.9942,   4.9844,  -0.2857],\n",
      "          ...,\n",
      "          [ -1.2104,   2.2662,  -2.9051,  ...,  -0.1920,  -6.0320,   1.6292],\n",
      "          [ -6.3490,  -6.0993,  -0.6012,  ...,   6.1687,  -2.1058,  -0.7258],\n",
      "          [ -9.6136,  -1.6007,  -0.4758,  ...,   1.6273,  -7.7952,  -7.7196]],\n",
      "\n",
      "         [[-10.0672,  -0.5467,  -0.0804,  ...,  -0.0164,   3.8181,  -1.3932],\n",
      "          [  5.8277,  -9.0610,  -3.6578,  ...,   7.0324,  -4.6470,   2.5972],\n",
      "          [  7.6433,   2.6269,  -4.5265,  ...,   5.9333,   1.9873,   1.4495],\n",
      "          ...,\n",
      "          [ -4.6386,   1.0373,  -2.2187,  ...,  -7.5864,   6.0699,   6.1841],\n",
      "          [ -5.8824,  -2.9219,  -0.2697,  ...,  -4.1298,   0.0664,   2.7122],\n",
      "          [ -5.8216,  -5.5138,  -8.5588,  ...,   7.4204,  -7.1499,   6.4899]],\n",
      "\n",
      "         [[-10.0672,  -0.5467,  -0.0804,  ...,  -0.0164,   3.8181,  -1.3932],\n",
      "          [  5.8277,  -9.0610,  -3.6578,  ...,   7.0324,  -4.6470,   2.5972],\n",
      "          [  7.6433,   2.6269,  -4.5265,  ...,   5.9333,   1.9873,   1.4495],\n",
      "          ...,\n",
      "          [ -4.6386,   1.0373,  -2.2187,  ...,  -7.5864,   6.0699,   6.1841],\n",
      "          [ -5.8824,  -2.9219,  -0.2697,  ...,  -4.1298,   0.0664,   2.7122],\n",
      "          [ -5.8216,  -5.5138,  -8.5588,  ...,   7.4204,  -7.1499,   6.4899]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-10.0672,  -0.5467,  -0.0804,  ...,  -0.0164,   3.8181,  -1.3932],\n",
      "          [  5.8277,  -9.0610,  -3.6578,  ...,   7.0324,  -4.6470,   2.5972],\n",
      "          [  7.6433,   2.6269,  -4.5265,  ...,   5.9333,   1.9873,   1.4495],\n",
      "          ...,\n",
      "          [ -4.6386,   1.0373,  -2.2187,  ...,  -7.5864,   6.0699,   6.1841],\n",
      "          [ -5.8824,  -2.9219,  -0.2697,  ...,  -4.1298,   0.0664,   2.7122],\n",
      "          [ -5.8216,  -5.5138,  -8.5588,  ...,   7.4204,  -7.1499,   6.4899]],\n",
      "\n",
      "         [[-10.0672,  -0.5467,  -0.0804,  ...,  -0.0164,   3.8181,  -1.3932],\n",
      "          [  5.8277,  -9.0610,  -3.6578,  ...,   7.0324,  -4.6470,   2.5972],\n",
      "          [  7.6433,   2.6269,  -4.5265,  ...,   5.9333,   1.9873,   1.4495],\n",
      "          ...,\n",
      "          [ -4.6386,   1.0373,  -2.2187,  ...,  -7.5864,   6.0699,   6.1841],\n",
      "          [ -5.8824,  -2.9219,  -0.2697,  ...,  -4.1298,   0.0664,   2.7122],\n",
      "          [ -5.8216,  -5.5138,  -8.5588,  ...,   7.4204,  -7.1499,   6.4899]],\n",
      "\n",
      "         [[-10.0672,  -0.5467,  -0.0804,  ...,  -0.0164,   3.8181,  -1.3932],\n",
      "          [  5.8277,  -9.0610,  -3.6578,  ...,   7.0324,  -4.6470,   2.5972],\n",
      "          [  7.6433,   2.6269,  -4.5265,  ...,   5.9333,   1.9873,   1.4495],\n",
      "          ...,\n",
      "          [ -4.6386,   1.0373,  -2.2187,  ...,  -7.5864,   6.0699,   6.1841],\n",
      "          [ -5.8824,  -2.9219,  -0.2697,  ...,  -4.1298,   0.0664,   2.7122],\n",
      "          [ -5.8216,  -5.5138,  -8.5588,  ...,   7.4204,  -7.1499,   6.4899]]],\n",
      "\n",
      "\n",
      "        [[[-10.0672,  -0.5467,  -0.0804,  ...,  -0.0164,   3.8181,  -1.3932],\n",
      "          [  5.8277,  -9.0610,  -3.6578,  ...,   7.0324,  -4.6470,   2.5972],\n",
      "          [  7.6433,   2.6269,  -4.5265,  ...,   5.9333,   1.9873,   1.4495],\n",
      "          ...,\n",
      "          [ -4.6386,   1.0373,  -2.2187,  ...,  -7.5864,   6.0699,   6.1841],\n",
      "          [ -5.8824,  -2.9219,  -0.2697,  ...,  -4.1298,   0.0664,   2.7122],\n",
      "          [ -5.8216,  -5.5138,  -8.5588,  ...,   7.4204,  -7.1499,   6.4899]],\n",
      "\n",
      "         [[-10.0672,  -0.5467,  -0.0804,  ...,  -0.0164,   3.8181,  -1.3932],\n",
      "          [  5.8277,  -9.0610,  -3.6578,  ...,   7.0324,  -4.6470,   2.5972],\n",
      "          [  7.6433,   2.6269,  -4.5265,  ...,   5.9333,   1.9873,   1.4495],\n",
      "          ...,\n",
      "          [ -4.6386,   1.0373,  -2.2187,  ...,  -7.5864,   6.0699,   6.1841],\n",
      "          [ -5.8824,  -2.9219,  -0.2697,  ...,  -4.1298,   0.0664,   2.7122],\n",
      "          [ -5.8216,  -5.5138,  -8.5588,  ...,   7.4204,  -7.1499,   6.4899]],\n",
      "\n",
      "         [[ -2.0031,   0.0744,   4.7432,  ...,   7.4315,  -7.6460,  -4.5075],\n",
      "          [ -4.0762,   9.7628,  -5.2478,  ...,   2.0859,  -2.0979,  -0.6349],\n",
      "          [ -7.0332,  -0.6277,   8.1626,  ...,  -5.3184,  -4.5931,  -6.2132],\n",
      "          ...,\n",
      "          [ -6.8897,   0.9253,  -9.4481,  ...,  -1.5678,  -3.0495,  -4.7011],\n",
      "          [-12.5338,  -1.0603,   6.3341,  ...,  -7.9409,  -0.8296,  -0.1396],\n",
      "          [ -6.4337, -14.7850,  11.8747,  ...,  -7.9576,   4.9658,   2.5540]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-10.0672,  -0.5467,  -0.0804,  ...,  -0.0164,   3.8181,  -1.3932],\n",
      "          [  5.8277,  -9.0610,  -3.6578,  ...,   7.0324,  -4.6470,   2.5972],\n",
      "          [  7.6433,   2.6269,  -4.5265,  ...,   5.9333,   1.9873,   1.4495],\n",
      "          ...,\n",
      "          [ -4.6386,   1.0373,  -2.2187,  ...,  -7.5864,   6.0699,   6.1841],\n",
      "          [ -5.8824,  -2.9219,  -0.2697,  ...,  -4.1298,   0.0664,   2.7122],\n",
      "          [ -5.8216,  -5.5138,  -8.5588,  ...,   7.4204,  -7.1499,   6.4899]],\n",
      "\n",
      "         [[-10.0672,  -0.5467,  -0.0804,  ...,  -0.0164,   3.8181,  -1.3932],\n",
      "          [  5.8277,  -9.0610,  -3.6578,  ...,   7.0324,  -4.6470,   2.5972],\n",
      "          [  7.6433,   2.6269,  -4.5265,  ...,   5.9333,   1.9873,   1.4495],\n",
      "          ...,\n",
      "          [ -4.6386,   1.0373,  -2.2187,  ...,  -7.5864,   6.0699,   6.1841],\n",
      "          [ -5.8824,  -2.9219,  -0.2697,  ...,  -4.1298,   0.0664,   2.7122],\n",
      "          [ -5.8216,  -5.5138,  -8.5588,  ...,   7.4204,  -7.1499,   6.4899]],\n",
      "\n",
      "         [[-10.0672,  -0.5467,  -0.0804,  ...,  -0.0164,   3.8181,  -1.3932],\n",
      "          [  5.8277,  -9.0610,  -3.6578,  ...,   7.0324,  -4.6470,   2.5972],\n",
      "          [  7.6433,   2.6269,  -4.5265,  ...,   5.9333,   1.9873,   1.4495],\n",
      "          ...,\n",
      "          [ -4.6386,   1.0373,  -2.2187,  ...,  -7.5864,   6.0699,   6.1841],\n",
      "          [ -5.8824,  -2.9219,  -0.2697,  ...,  -4.1298,   0.0664,   2.7122],\n",
      "          [ -5.8216,  -5.5138,  -8.5588,  ...,   7.4204,  -7.1499,   6.4899]]],\n",
      "\n",
      "\n",
      "        [[[-10.0672,  -0.5467,  -0.0804,  ...,  -0.0164,   3.8181,  -1.3932],\n",
      "          [  5.8277,  -9.0610,  -3.6578,  ...,   7.0324,  -4.6470,   2.5972],\n",
      "          [  7.6433,   2.6269,  -4.5265,  ...,   5.9333,   1.9873,   1.4495],\n",
      "          ...,\n",
      "          [ -4.6386,   1.0373,  -2.2187,  ...,  -7.5864,   6.0699,   6.1841],\n",
      "          [ -5.8824,  -2.9219,  -0.2697,  ...,  -4.1298,   0.0664,   2.7122],\n",
      "          [ -5.8216,  -5.5138,  -8.5588,  ...,   7.4204,  -7.1499,   6.4899]],\n",
      "\n",
      "         [[-10.0672,  -0.5467,  -0.0804,  ...,  -0.0164,   3.8181,  -1.3932],\n",
      "          [  5.8277,  -9.0610,  -3.6578,  ...,   7.0324,  -4.6470,   2.5972],\n",
      "          [  7.6433,   2.6269,  -4.5265,  ...,   5.9333,   1.9873,   1.4495],\n",
      "          ...,\n",
      "          [ -4.6386,   1.0373,  -2.2187,  ...,  -7.5864,   6.0699,   6.1841],\n",
      "          [ -5.8824,  -2.9219,  -0.2697,  ...,  -4.1298,   0.0664,   2.7122],\n",
      "          [ -5.8216,  -5.5138,  -8.5588,  ...,   7.4204,  -7.1499,   6.4899]],\n",
      "\n",
      "         [[-10.0672,  -0.5467,  -0.0804,  ...,  -0.0164,   3.8181,  -1.3932],\n",
      "          [  5.8277,  -9.0610,  -3.6578,  ...,   7.0324,  -4.6470,   2.5972],\n",
      "          [  7.6433,   2.6269,  -4.5265,  ...,   5.9333,   1.9873,   1.4495],\n",
      "          ...,\n",
      "          [ -4.6386,   1.0373,  -2.2187,  ...,  -7.5864,   6.0699,   6.1841],\n",
      "          [ -5.8824,  -2.9219,  -0.2697,  ...,  -4.1298,   0.0664,   2.7122],\n",
      "          [ -5.8216,  -5.5138,  -8.5588,  ...,   7.4204,  -7.1499,   6.4899]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-10.0672,  -0.5467,  -0.0804,  ...,  -0.0164,   3.8181,  -1.3932],\n",
      "          [  5.8277,  -9.0610,  -3.6578,  ...,   7.0324,  -4.6470,   2.5972],\n",
      "          [  7.6433,   2.6269,  -4.5265,  ...,   5.9333,   1.9873,   1.4495],\n",
      "          ...,\n",
      "          [ -4.6386,   1.0373,  -2.2187,  ...,  -7.5864,   6.0699,   6.1841],\n",
      "          [ -5.8824,  -2.9219,  -0.2697,  ...,  -4.1298,   0.0664,   2.7122],\n",
      "          [ -5.8216,  -5.5138,  -8.5588,  ...,   7.4204,  -7.1499,   6.4899]],\n",
      "\n",
      "         [[-10.0672,  -0.5467,  -0.0804,  ...,  -0.0164,   3.8181,  -1.3932],\n",
      "          [  5.8277,  -9.0610,  -3.6578,  ...,   7.0324,  -4.6470,   2.5972],\n",
      "          [  7.6433,   2.6269,  -4.5265,  ...,   5.9333,   1.9873,   1.4495],\n",
      "          ...,\n",
      "          [ -4.6386,   1.0373,  -2.2187,  ...,  -7.5864,   6.0699,   6.1841],\n",
      "          [ -5.8824,  -2.9219,  -0.2697,  ...,  -4.1298,   0.0664,   2.7122],\n",
      "          [ -5.8216,  -5.5138,  -8.5588,  ...,   7.4204,  -7.1499,   6.4899]],\n",
      "\n",
      "         [[-10.0672,  -0.5467,  -0.0804,  ...,  -0.0164,   3.8181,  -1.3932],\n",
      "          [  5.8277,  -9.0610,  -3.6578,  ...,   7.0324,  -4.6470,   2.5972],\n",
      "          [  7.6433,   2.6269,  -4.5265,  ...,   5.9333,   1.9873,   1.4495],\n",
      "          ...,\n",
      "          [ -4.6386,   1.0373,  -2.2187,  ...,  -7.5864,   6.0699,   6.1841],\n",
      "          [ -5.8824,  -2.9219,  -0.2697,  ...,  -4.1298,   0.0664,   2.7122],\n",
      "          [ -5.8216,  -5.5138,  -8.5588,  ...,   7.4204,  -7.1499,   6.4899]]]])\n",
      "Shape of Tensor:  torch.Size([3, 10, 30, 100])\n"
     ]
    }
   ],
   "source": [
    "indexed_tensor = product[long_tensor]\n",
    "print('Indexed Tensor: \\n' , indexed_tensor)\n",
    "\n",
    "print('Shape of Tensor: ' , indexed_tensor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Use  torch.mean  to average across the last dimension in the tensor from step 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.3454, -0.2317, -0.0933,  0.4291, -0.1312,  1.3061,  0.5642,\n",
      "           1.1304,  0.1062,  0.1260, -0.2958,  0.1313, -0.5186,  0.8559,\n",
      "           0.8028,  0.1708,  0.1969, -0.9095, -0.1092, -0.0769,  0.0858,\n",
      "           0.1287, -0.2001,  0.1188, -0.2245,  0.3429, -0.2672, -0.7560,\n",
      "          -0.2014, -0.2246],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575]],\n",
      "\n",
      "        [[-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.0336,  0.0264,  0.9859, -0.4310,  0.5316,  0.7740,  0.4998,\n",
      "           0.5335,  0.7258,  0.9583, -0.2257,  1.0147,  0.0906, -0.5147,\n",
      "          -1.0576,  0.5242,  0.4613, -0.4154, -0.4653, -0.9870, -0.4760,\n",
      "          -0.8975, -0.6678, -1.0397, -0.4077, -0.0841,  0.3796, -0.3590,\n",
      "           0.1998,  0.6399],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575]],\n",
      "\n",
      "        [[-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.9909,  0.0905, -1.0183,  0.3124,  0.1924,  0.7919, -0.3500,\n",
      "          -0.2553, -0.0238,  0.1422,  0.0159, -0.4323, -0.0594,  0.4480,\n",
      "          -0.2875,  0.5102, -0.4241, -0.1257,  0.2220, -0.5029,  0.0808,\n",
      "           0.3513,  0.5204, -0.3028,  1.4386, -0.6248,  0.2835,  0.3410,\n",
      "           1.2113, -1.1412],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575],\n",
      "         [-0.1650, -1.0124,  0.5759, -0.2350, -1.0106,  0.3657, -0.2067,\n",
      "           0.0975,  0.2823, -0.8947,  0.7574,  0.1022, -0.7763,  0.0437,\n",
      "           0.1217,  0.8906, -0.8341,  0.2223,  0.4081, -0.7304,  0.6348,\n",
      "          -0.2456,  1.1863, -0.2763,  0.5241,  0.0163,  0.4372,  0.6273,\n",
      "           1.1752, -0.1575]]])\n",
      "Shape of Tensor:  torch.Size([3, 10, 30])\n"
     ]
    }
   ],
   "source": [
    "mean_tensor = torch.mean(indexed_tensor, dim = 3)\n",
    "print(mean_tensor)\n",
    "\n",
    "print('Shape of Tensor: ' , mean_tensor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Redo step 2. on the GPU and compare results from step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [180], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tensor_a_cuda \u001b[39m=\u001b[39m tensor_a\u001b[39m.\u001b[39mto(device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m tensor_b_cuda \u001b[39m=\u001b[39m tensor_b\u001b[39m.\u001b[39mto(device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m product_gpu \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(tensor_a, tensor_b)\n",
      "File \u001b[0;32m~/Documents/pvenv/lib/python3.10/site-packages/torch/cuda/__init__.py:211\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    210\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    214\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "tensor_a_cuda = tensor_a.to(device = 'cuda')\n",
    "tensor_b_cuda = tensor_b.to(device = 'cuda')\n",
    "product_gpu = torch.matmul(tensor_a, tensor_b)\n",
    "print('Product of tensor_a and tensor_b on GPU: \\n' , product_gpu)\n",
    "\n",
    "print('Shape of Tensor: ' , product_gpu.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Write a pure PyTorch program to compute the value of  up to 4 decimal places without using the square root or other math functions from any of the libraries. \n",
    "### Hint: Notice that the answer is the (positive) root of the equation, $$𝑥^2 −2 = 0$$ \n",
    "### To find the root, you might want to use \"Newton's Method\": $$𝑥_{𝑛+1} = 𝑥_{𝑛} − \\frac{𝑓(𝑥)}{𝑓′(𝑥)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fail-fast prototyping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building neural networks, you want things to either work or fail fast. Long iteration loops are \n",
    "the worst enemy of a machine learning practitioner. \\\n",
    "For e.g., while writing code, you might want to incrementally test your code by doing something \n",
    "like this:\n",
    "\n",
    "batch_size = 32 \\\n",
    "num_features = 512 \\\n",
    "embedding_size = 16\n",
    "\n",
    "\\# construct a dummy input \\\n",
    "x = torch.randn(batch_size, num_features)\n",
    "\n",
    "\\# we want to project the input to embedding_size \\\n",
    "fc = torch.nn.Linear(num_features, embedding_size)\n",
    "\n",
    "\\# test if that works \\\n",
    "print(fc(x).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fail-fast exercises"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. [Glove](https://nlp.stanford.edu/projects/glove/) has 300 dimension embeddings. Design an nn.Module that takes a sentence of max_len words, tokenizes words by spaces, represents the sentence by averaging the glove embeddings of constituent words. What is the shape of the resulting sentence embedding? When you implement this, you will need to make some assumptions. What are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b37beb76676c8b0643f5764b5c5ae0ddf876ecbab29b433e279cae2d82963c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
