{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    \"\"\"Returns the device to be used for model training.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")  # For new Mac M1 or M2 chips\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(EPOCHS=5, SEED=42, BATCH_SIZE=128, EMBED_DIM=100, HIDDEN_DIM=256, NUM_LAYERS=2, BIDIRECTIONAL=False, OPTIMIZER='AdamW', LOSS_FN='BCELoss', SCORE_FN='F1_Score', LEARNING_RATE=0.001, DROPOUT=0.2)\n"
     ]
    }
   ],
   "source": [
    "argument_parser = argparse.ArgumentParser()\n",
    "argument_parser.add_argument(\"--epochs\", dest = \"EPOCHS\", type = int, default = 5)\n",
    "argument_parser.add_argument(\"--seed\", dest = \"SEED\", type = int, default = 42)\n",
    "argument_parser.add_argument(\"--batch_size\", dest = \"BATCH_SIZE\", type = int, default = 128)\n",
    "argument_parser.add_argument(\"--embed_dim\", dest = \"EMBED_DIM\", type = int, default = 100)\n",
    "argument_parser.add_argument(\"--hidden_dim\", dest = \"HIDDEN_DIM\", type = int, default = 256)\n",
    "argument_parser.add_argument(\"--num_layers\", dest = \"NUM_LAYERS\", type = int, default = 2)\n",
    "argument_parser.add_argument(\"--bidirectional\", dest = \"BIDIRECTIONAL\", type = bool, default = False)\n",
    "argument_parser.add_argument(\"--optimizer\", dest = \"OPTIMIZER\", type = str, default = 'AdamW')\n",
    "argument_parser.add_argument(\"--loss_fn\", dest = \"LOSS_FN\", type = str, default = 'BCELoss')\n",
    "argument_parser.add_argument(\"--score_fn\", dest = \"SCORE_FN\", type = str, default = 'F1_Score')\n",
    "argument_parser.add_argument(\"--learning_rate\", dest = \"LEARNING_RATE\", type = float, default =  1e-3)\n",
    "argument_parser.add_argument(\"--dropout\", dest = \"DROPOUT\", type = float, default =  0.2)\n",
    "args, _ = argument_parser.parse_known_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reproducible(seed:int = 42) -> None:\n",
    "    \"\"\"\n",
    "    Set random seed in a bunch of libraries.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    _numpy_rng = np.random.default_rng(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read clickbait headlines\n",
    "with gzip.open('dataset/clickbait_data.gz', 'rb') as f:\n",
    "    clickbait_df = pd.read_csv(f, sep='\\t', header=None, names=['headline'])\n",
    "clickbait_df['label'] = 1\n",
    "\n",
    "# Read non-clickbait headlines\n",
    "with gzip.open('dataset/non_clickbait_data.gz', 'rb') as f:\n",
    "    non_clickbait_df = pd.read_csv(f, sep='\\t', header=None, names=['headline'])\n",
    "non_clickbait_df['label'] = 0\n",
    "\n",
    "# Merge clickbait and non-clickbait dataframes\n",
    "data = pd.concat([clickbait_df, non_clickbait_df])\n",
    "\n",
    "# Shuffle the rows to randomize the order\n",
    "data = data.sample(frac=1, random_state=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of clickbait headlines: 50.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage of clickbait headlines\n",
    "clickbait_percentage = (data[data['label'] == 1]['label'].count() / len(data)) * 100\n",
    "\n",
    "# Print the percentage of clickbait headlines\n",
    "print(f\"Percentage of clickbait headlines: {clickbait_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 22400\n",
      "Dev size: 4800\n",
      "Test size: 4800\n"
     ]
    }
   ],
   "source": [
    "train, dev = train_test_split(data, test_size=0.3, random_state=args.SEED)\n",
    "dev, test = train_test_split(dev, test_size=0.5, random_state=args.SEED)\n",
    "print(f\"Train size: {len(train)}\")\n",
    "print(f\"Dev size: {len(dev)}\")\n",
    "print(f\"Test size: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, df: pd.DataFrame) -> None:\n",
    "        self.df = df\n",
    "        self.tokenized_df = self.tokenize()\n",
    "        self.unique_words = self.load_unique_words()\n",
    "        self.word2idx = {word:idx for idx, word in enumerate(self.unique_words)}\n",
    "        self.word2idx['<pad>'] = len(self.word2idx)\n",
    "        self.word2idx['<unk>'] = len(self.word2idx) + 1\n",
    "        self.idx2word = {idx:word for word, idx in self.word2idx.items()}\n",
    "\n",
    "    def load_unique_words(self) -> list:\n",
    "        word_list = [word for _, line in self.df['tokenized'].items() for word in line]\n",
    "        return Counter(word_list)\n",
    "    \n",
    "    def tokenize(self) -> pd.DataFrame:\n",
    "        self.df['tokenized'] = self.df['headline'].apply(lambda line: [\"<sos>\"] + line.strip().split() + [\"<eos>\"])\n",
    "        return self.df['tokenized']\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.word2idx)\n",
    "\n",
    "vocab = Vocabulary(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(EPOCHS=5, SEED=42, BATCH_SIZE=128, EMBED_DIM=100, HIDDEN_DIM=256, NUM_LAYERS=2, BIDIRECTIONAL=False, OPTIMIZER='AdamW', LOSS_FN='BCELoss', SCORE_FN='F1_Score', LEARNING_RATE=0.001, DROPOUT=0.2, INPUT_DIM=29430, OUTPUT_DIM=1)\n"
     ]
    }
   ],
   "source": [
    "argument_parser.add_argument(\"--input_dim\", dest = \"INPUT_DIM\", type = int, default = len(vocab.word2idx))\n",
    "argument_parser.add_argument(\"--output_dim\", dest = \"OUTPUT_DIM\", type = int, default = 1)\n",
    "args, _ = argument_parser.parse_known_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClickbaitDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, vocab: Vocabulary) -> None:\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "        self.tokenized_df = self.tokenize()\n",
    "        self.word2idx = self.vocab.word2idx\n",
    "        self.idx2word = self.vocab.idx2word\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        line = self.tokenized_df.iloc[idx]\n",
    "        text = [self.word2idx[word] if word in self.word2idx else self.word2idx['<unk>'] for word in line]\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        return text, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def tokenize(self) -> pd.DataFrame:\n",
    "        self.df['tokenized'] = self.df['headline'].apply(lambda line: [\"<sos>\"] + line.strip().split() + [\"<eos>\"])\n",
    "        return self.df['tokenized']\n",
    "\n",
    "train_ds = ClickbaitDataset(train, vocab)\n",
    "dev_ds = ClickbaitDataset(dev, vocab)\n",
    "test_ds = ClickbaitDataset(test, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Adds padding token '0' at the end of each text vector to make it\n",
    "    of the same length as the maximum length input in the batch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        a tuple of padded text tensors, label tensors and text lengths tensor in a batch\n",
    "    \"\"\"\n",
    "    texts, labels = zip(*batch)\n",
    "    \n",
    "    texts_tensor = [torch.tensor(text, device = device) for text in texts]\n",
    "    labels_tensor = torch.tensor(labels, device = device)\n",
    "\n",
    "    lengths = [len(text) for text in texts]\n",
    "    lengths = torch.tensor(lengths, device = 'cpu') # Lengths need to be on CPU\n",
    "    \n",
    "    texts_padded = pad_sequence(texts_tensor, batch_first = False, padding_value = vocab.word2idx['<pad>'])\n",
    "    \n",
    "    return texts_padded, labels_tensor, lengths\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=args.BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n",
    "dev_loader = DataLoader(dev_ds, batch_size=args.BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=args.BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim, \n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 dropout):\n",
    "        super(LSTM, self).__init__()\n",
    "        # Initialize Embedding Layer\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        # Initialzie LSTM layer to process the vector sequences \n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim,\n",
    "                            num_layers = n_layers,\n",
    "                            bidirectional = bidirectional,\n",
    "                            dropout = dropout,\n",
    "                            batch_first = False)\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        # Initialize Dense layer to predict\n",
    "        self.fc = nn.Linear(hidden_dim * num_directions, output_dim)\n",
    "        # Initialize dropout to improve with regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, \n",
    "                x, \n",
    "                x_lengths):\n",
    "        # Embedding Layer\n",
    "        embedded = self.embedding(x)\n",
    "        # Dropout Layer before LSTM Layer\n",
    "        embedded = self.dropout(embedded)\n",
    "        # Packed Sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, \n",
    "                                                            x_lengths, \n",
    "                                                            batch_first = False, \n",
    "                                                            enforce_sorted = False)\n",
    "        # LSTM Layer\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        # Unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, \n",
    "                                                                  batch_first = False)\n",
    "        # Concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers and Apply Dropout\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        # Fully Connected Layer\n",
    "        output = self.fc(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Model:  LSTM(\n",
      "  (embedding): Embedding(29430, 100)\n",
      "  (lstm): LSTM(100, 256, num_layers=2, dropout=0.2)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(args.INPUT_DIM, \n",
    "             args.EMBED_DIM, \n",
    "             args.HIDDEN_DIM, \n",
    "             args.OUTPUT_DIM, \n",
    "             args.NUM_LAYERS, \n",
    "             args.BIDIRECTIONAL, \n",
    "             args.DROPOUT).to(device)\n",
    "\n",
    "print('LSTM Model: ', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Train Function\n",
    "def train(loader, \n",
    "          model, \n",
    "          optimizer, \n",
    "          loss_fn):\n",
    "    model.train()\n",
    "    losses = list()\n",
    "    pbar = tqdm(loader, desc = 'Training...', colour = 'red')\n",
    "    for x, y, x_lengths in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate y_pred\n",
    "        y_pred = model(x, x_lengths).squeeze(1)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y.float())\n",
    "        pbar.set_postfix({'Loss': loss.item()})\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Calculate gradients for w/b\n",
    "        loss.backward()  \n",
    "        # Update weights according to optimizer rules\n",
    "        optimizer.step()          \n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "# Model Evaluate Function\n",
    "def evaluate(loader, \n",
    "             model, \n",
    "             loss_fn, \n",
    "             score_fn):\n",
    "    model.eval()\n",
    "    losses = list()\n",
    "    pbar = tqdm(loader, desc = 'Evaluation...', colour = 'green')\n",
    "    for x, y, x_lengths in pbar:\n",
    "\n",
    "        # Calculate y_pred\n",
    "        y_pred = model(x, x_lengths).squeeze(1)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y.float())\n",
    "        pbar.set_postfix({'Loss': loss.item()})\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        score = score_fn(y, y_pred)\n",
    "              \n",
    "    return sum(losses) / len(losses), score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|\u001b[31m          \u001b[0m| 0/175 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "philox_cuda_state for an unexpected CUDA generator used during capture. In regions captured by CUDA graphs, you may only use the default CUDA RNG generator on the device that's current when capture begins. If you need a non-default (user-supplied) generator, or a generator on another device, please file an issue.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[236], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     24\u001b[0m \u001b[39m# Avg Train Loss, Train Accuracy\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m train_loss \u001b[39m=\u001b[39m train(train_loader, \n\u001b[1;32m     26\u001b[0m                    model, \n\u001b[1;32m     27\u001b[0m                    optimizer, \n\u001b[1;32m     28\u001b[0m                    loss_fn)\n\u001b[1;32m     30\u001b[0m \u001b[39m# Avg Val Loss, F1_Score\u001b[39;00m\n\u001b[1;32m     31\u001b[0m val_loss, val_acc \u001b[39m=\u001b[39m evaluate(dev_loader, \n\u001b[1;32m     32\u001b[0m                              model, \n\u001b[1;32m     33\u001b[0m                              loss_fn, \n\u001b[1;32m     34\u001b[0m                              score_fn)\n",
      "Cell \u001b[0;32mIn[234], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(loader, model, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     12\u001b[0m \u001b[39m# Calculate y_pred\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m y_pred \u001b[39m=\u001b[39m model(x, x_lengths)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_pred, y\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m     16\u001b[0m pbar\u001b[39m.\u001b[39mset_postfix({\u001b[39m'\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m'\u001b[39m: loss\u001b[39m.\u001b[39mitem()})\n",
      "File \u001b[0;32m~/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[232], line 33\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, x, x_lengths)\u001b[0m\n\u001b[1;32m     31\u001b[0m embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[1;32m     32\u001b[0m \u001b[39m# Dropout Layer before LSTM Layer\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(embedded)\n\u001b[1;32m     34\u001b[0m \u001b[39m# Packed Sequence\u001b[39;00m\n\u001b[1;32m     35\u001b[0m packed_embedded \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mpack_padded_sequence(embedded, \n\u001b[1;32m     36\u001b[0m                                                     x_lengths, \n\u001b[1;32m     37\u001b[0m                                                     batch_first \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, \n\u001b[1;32m     38\u001b[0m                                                     enforce_sorted \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/conda/lib/python3.10/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/conda/lib/python3.10/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: philox_cuda_state for an unexpected CUDA generator used during capture. In regions captured by CUDA graphs, you may only use the default CUDA RNG generator on the device that's current when capture begins. If you need a non-default (user-supplied) generator, or a generator on another device, please file an issue."
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "if args.OPTIMIZER == 'AdamW':\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                                  lr = args.LEARNING_RATE)\n",
    "\n",
    "# Loss Function\n",
    "if args.LOSS_FN == 'BCELoss':\n",
    "    loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "# Initialize Best Validation Loss\n",
    "best_valid_loss = float('inf')\n",
    "    \n",
    "# Path to Save Best Model\n",
    "PATH = f'lstm-best-model.pt'\n",
    "\n",
    "# Score Function\n",
    "if args.SCORE_FN == 'F1_Score':\n",
    "    score_fn = f1_score\n",
    "\n",
    "for epoch in range(args.EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Avg Train Loss, Train Accuracy\n",
    "    train_loss = train(train_loader, \n",
    "                       model, \n",
    "                       optimizer, \n",
    "                       loss_fn)\n",
    "\n",
    "    # Avg Val Loss, F1_Score\n",
    "    val_loss, val_acc = evaluate(dev_loader, \n",
    "                                 model, \n",
    "                                 loss_fn, \n",
    "                                 score_fn)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'\\n\\tEpoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\tValidation Loss: {val_loss:.3f} | F1_Score: {val_acc*100:.2f}%\\n')\n",
    "\n",
    "    if val_loss < best_valid_loss:\n",
    "        best_valid_loss = val_loss\n",
    "        torch.save(model.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4920579d663edfdfd95f9f62c615bd0d63289cacc215aee642510734814eb369"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
